#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATå¤šç›®çš„æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
Optunaãƒ™ãƒ¼ã‚¹ã®rank/Î³ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©åŒ–
"""

import os
import sys
import json
import time
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from tqdm import tqdm
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from datetime import datetime

# Optuna imports
try:
    import optuna
    from optuna.visualization import plot_pareto_front, plot_param_importances
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna not available, install with: pip install optuna")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nkat_optimization.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NKATParameterOptimizer:
    """NKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.optimization_history = []
        
        logger.info(f"ğŸ”§ NKAT Parameter Optimizer initialized")
        logger.info(f"   ğŸ“± Device: {self.device}")
        logger.info(f"   ğŸ“ Model: {Path(model_path).name}")
        
        if not OPTUNA_AVAILABLE:
            logger.error("âŒ Optuna not available. Please install: pip install optuna")
            sys.exit(1)
    
    def create_nkat_layer(self, hidden_size: int, rank: int, gamma: float) -> torch.nn.Module:
        """NKATãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ"""
        class NKATLayer(torch.nn.Module):
            def __init__(self, hidden_size: int, rank: int, gamma: float):
                super().__init__()
                self.hidden_size = hidden_size
                self.rank = rank
                self.gamma = gamma
                
                # ç·šå½¢å¤‰æ›
                self.linear = torch.nn.Linear(hidden_size, hidden_size, bias=False)
                
                # Î¸ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆä½ãƒ©ãƒ³ã‚¯åˆ†è§£ï¼‰
                self.theta_u = torch.nn.Parameter(torch.randn(hidden_size, rank) * 0.01)
                self.theta_v = torch.nn.Parameter(torch.randn(rank, hidden_size) * 0.01)
                
            def forward(self, x):
                # æ¨™æº–ç·šå½¢å¤‰æ›
                y_linear = self.linear(x)
                
                # Î¸é …è¨ˆç®—ï¼ˆåå¯¾ç§°è¡Œåˆ—ï¼‰
                theta = torch.matmul(self.theta_u, self.theta_v)
                theta_antisymm = 0.5 * (theta - theta.T)
                
                # NKATæ¼”ç®—
                y_phase = self.gamma * torch.matmul(x, theta_antisymm.T)
                
                return y_linear + y_phase
        
        return NKATLayer(hidden_size, rank, gamma)
    
    def evaluate_nkat_performance(self, rank: int, gamma: float, 
                                  seq_length: int = 512, 
                                  iterations: int = 10) -> Tuple[float, float, float]:
        """NKATæ€§èƒ½è©•ä¾¡"""
        
        hidden_size = 4096  # Qwen3-8Bæƒ³å®š
        batch_size = 1
        
        try:
            # NKATãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ
            nkat_layer = self.create_nkat_layer(hidden_size, rank, gamma).to(self.device)
            
            # åˆæˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            x = torch.randn(batch_size, seq_length, hidden_size, 
                          device=self.device, dtype=torch.float16)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            throughputs = []
            memory_usage = []
            
            with torch.no_grad():
                for _ in range(iterations):
                    if torch.cuda.is_available():
                        torch.cuda.reset_peak_memory_stats()
                    
                    start_time = time.time()
                    
                    # NKATæ¼”ç®—å®Ÿè¡Œ
                    output = nkat_layer(x)
                    
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    
                    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
                    tokens_processed = batch_size * seq_length
                    iteration_time = end_time - start_time
                    throughput = tokens_processed / iteration_time
                    throughputs.append(throughput)
                    
                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
                    if torch.cuda.is_available():
                        memory_used = torch.cuda.max_memory_allocated() / (1024**3)
                        memory_usage.append(memory_used)
            
            # å“è³ªæ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            # å®Ÿéš›ã«ã¯ perplexity ã‚„ BLEU ã‚¹ã‚³ã‚¢ãªã©ã‚’ä½¿ç”¨
            quality_score = self.estimate_quality(rank, gamma)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            avg_throughput = np.mean(throughputs)
            avg_memory = np.mean(memory_usage) if memory_usage else 0.0
            
            return avg_throughput, quality_score, avg_memory
            
        except Exception as e:
            logger.error(f"âŒ Evaluation failed for rank={rank}, gamma={gamma}: {e}")
            return 0.0, 0.0, float('inf')
    
    def estimate_quality(self, rank: int, gamma: float) -> float:
        """å“è³ªæ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãŒå¿…è¦
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªæ¨å®šã‚’å®Ÿè£…
        
        # ç†æƒ³çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã®å®šç¾©
        optimal_rank = 6.0
        optimal_gamma = 0.97
        
        # ãƒ©ãƒ³ã‚¯ã«ã‚ˆã‚‹å“è³ªå½±éŸ¿
        rank_penalty = abs(rank - optimal_rank) * 0.02
        
        # ã‚¬ãƒ³ãƒã«ã‚ˆã‚‹å“è³ªå½±éŸ¿
        gamma_penalty = abs(gamma - optimal_gamma) * 0.5
        
        # ãƒ™ãƒ¼ã‚¹å“è³ªã‚¹ã‚³ã‚¢
        base_quality = 0.85
        
        # èª¿æ•´ã•ã‚ŒãŸå“è³ªã‚¹ã‚³ã‚¢
        quality = base_quality - rank_penalty - gamma_penalty
        
        # ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆå®Ÿéš›ã®å¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        noise = np.random.normal(0, 0.02)
        quality += noise
        
        return max(0.0, min(1.0, quality))
    
    def objective_function(self, trial) -> Tuple[float, float]:
        """å¤šç›®çš„æœ€é©åŒ–ã®ç›®çš„é–¢æ•°"""
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆ
        rank = trial.suggest_int('rank', 2, 12)
        gamma = trial.suggest_float('gamma', 0.90, 0.99)
        
        # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        seq_length = trial.suggest_categorical('seq_length', [256, 512, 1024])
        
        logger.info(f"ğŸ§ª Trial {trial.number}: rank={rank}, gamma={gamma:.3f}, seq_len={seq_length}")
        
        # æ€§èƒ½è©•ä¾¡
        throughput, quality, memory = self.evaluate_nkat_performance(
            rank=rank, 
            gamma=gamma, 
            seq_length=seq_length
        )
        
        # æœ€é©åŒ–è¨˜éŒ²
        self.optimization_history.append({
            'trial': trial.number,
            'rank': rank,
            'gamma': gamma,
            'seq_length': seq_length,
            'throughput': throughput,
            'quality': quality,
            'memory': memory,
            'timestamp': time.time()
        })
        
        logger.info(f"   ğŸ“Š Results: throughput={throughput:.1f}, quality={quality:.3f}, memory={memory:.2f}GB")
        
        # å¤šç›®çš„æœ€é©åŒ–ï¼šã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ–ã€å“è³ªæœ€å¤§åŒ–
        return throughput, quality
    
    def run_optimization(self, n_trials: int = 50, study_name: str = "nkat_optimization") -> optuna.Study:
        """æœ€é©åŒ–å®Ÿè¡Œ"""
        
        logger.info(f"ğŸš€ Starting NKAT multi-objective optimization")
        logger.info(f"   ğŸ¯ Trials: {n_trials}")
        
        # Optunaã‚¹ã‚¿ãƒ‡ã‚£ä½œæˆ
        study = optuna.create_study(
            directions=['maximize', 'maximize'],  # [throughput, quality]
            study_name=study_name,
            sampler=optuna.samplers.NSGAIISampler()
        )
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        progress_bar = tqdm(total=n_trials, desc="Optimization Progress")
        
        def callback(study, trial):
            progress_bar.update(1)
            
            # ä¸­é–“çµæœè¡¨ç¤º
            if trial.number % 10 == 0:
                logger.info(f"   ğŸ“ˆ Trial {trial.number}: Best trials so far: {len(study.best_trials)}")
        
        study.optimize(
            self.objective_function, 
            n_trials=n_trials,
            callbacks=[callback]
        )
        
        progress_bar.close()
        
        logger.info(f"ğŸ‰ Optimization completed!")
        logger.info(f"   ğŸ“Š Pareto optimal solutions: {len(study.best_trials)}")
        
        return study
    
    def analyze_results(self, study: optuna.Study) -> Dict:
        """çµæœåˆ†æ"""
        
        logger.info("ğŸ“Š Analyzing optimization results...")
        
        # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£
        pareto_trials = study.best_trials
        
        analysis = {
            "optimization_summary": {
                "total_trials": len(study.trials),
                "pareto_optimal_count": len(pareto_trials),
                "best_trials": []
            },
            "parameter_analysis": {},
            "recommendations": []
        }
        
        # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã®è©³ç´°
        for i, trial in enumerate(pareto_trials[:10]):  # ä¸Šä½10å€‹
            trial_info = {
                "rank": i + 1,
                "trial_number": trial.number,
                "parameters": trial.params,
                "objectives": {
                    "throughput": trial.values[0],
                    "quality": trial.values[1]
                }
            }
            analysis["optimization_summary"]["best_trials"].append(trial_info)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦åˆ†æ
        if len(study.trials) >= 10:
            try:
                # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé‡è¦åº¦
                throughput_importance = optuna.importance.get_param_importances(
                    study, target=lambda t: t.values[0]
                )
                
                # å“è³ªé‡è¦åº¦
                quality_importance = optuna.importance.get_param_importances(
                    study, target=lambda t: t.values[1]
                )
                
                analysis["parameter_analysis"] = {
                    "throughput_importance": throughput_importance,
                    "quality_importance": quality_importance
                }
                
            except Exception as e:
                logger.warning(f"Parameter importance analysis failed: {e}")
        
        # æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if pareto_trials:
            # ãƒãƒ©ãƒ³ã‚¹å‹æ¨å¥¨ï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
            balanced_scores = []
            for trial in pareto_trials:
                # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢
                norm_throughput = trial.values[0] / max(t.values[0] for t in pareto_trials)
                norm_quality = trial.values[1] / max(t.values[1] for t in pareto_trials)
                balanced_score = (norm_throughput + norm_quality) / 2
                balanced_scores.append((balanced_score, trial))
            
            best_balanced = max(balanced_scores, key=lambda x: x[0])[1]
            
            analysis["recommendations"] = [
                {
                    "type": "balanced",
                    "description": "Best balance between throughput and quality",
                    "parameters": best_balanced.params,
                    "expected_throughput": best_balanced.values[0],
                    "expected_quality": best_balanced.values[1]
                }
            ]
        
        return analysis
    
    def create_visualization(self, study: optuna.Study, analysis: Dict) -> None:
        """çµæœå¯è¦–åŒ–"""
        
        logger.info("ğŸ“Š Creating optimization visualizations...")
        
        output_dir = Path("output/qwen3_nkat_testing")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # 1. ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆå¯è¦–åŒ–
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('NKAT Multi-Objective Optimization Results', fontsize=16, fontweight='bold')
            
            # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ãƒ—ãƒ­ãƒƒãƒˆ
            pareto_trials = study.best_trials
            all_trials = study.trials
            
            # å…¨è©¦è¡Œ
            all_throughputs = [t.values[0] for t in all_trials if t.values]
            all_qualities = [t.values[1] for t in all_trials if t.values]
            
            # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£
            pareto_throughputs = [t.values[0] for t in pareto_trials]
            pareto_qualities = [t.values[1] for t in pareto_trials]
            
            ax1.scatter(all_throughputs, all_qualities, alpha=0.6, c='lightblue', label='All trials')
            ax1.scatter(pareto_throughputs, pareto_qualities, c='red', s=100, label='Pareto optimal')
            ax1.set_xlabel('Throughput (tokens/sec)')
            ax1.set_ylabel('Quality Score')
            ax1.set_title('Pareto Front')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ
            ranks = [t.params.get('rank', 0) for t in all_trials]
            gammas = [t.params.get('gamma', 0) for t in all_trials]
            
            ax2.scatter(ranks, gammas, c=all_qualities, cmap='viridis', alpha=0.7)
            ax2.set_xlabel('Rank')
            ax2.set_ylabel('Gamma')
            ax2.set_title('Parameter Space (colored by quality)')
            ax2.grid(True, alpha=0.3)
            
            # 3. åæŸå±¥æ­´
            trial_numbers = [t.number for t in all_trials if t.values]
            best_throughputs = []
            best_qualities = []
            
            current_best_throughput = 0
            current_best_quality = 0
            
            for trial in all_trials:
                if trial.values:
                    current_best_throughput = max(current_best_throughput, trial.values[0])
                    current_best_quality = max(current_best_quality, trial.values[1])
                    best_throughputs.append(current_best_throughput)
                    best_qualities.append(current_best_quality)
            
            ax3.plot(trial_numbers, best_throughputs, label='Best Throughput', linewidth=2)
            ax3.plot(trial_numbers, [q*10000 for q in best_qualities], label='Best Quality Ã— 10000', linewidth=2)
            ax3.set_xlabel('Trial Number')
            ax3.set_ylabel('Objective Value')
            ax3.set_title('Convergence History')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # 4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
            param_importance = analysis.get("parameter_analysis", {}).get("throughput_importance", {})
            if param_importance:
                params = list(param_importance.keys())
                importances = list(param_importance.values())
                
                ax4.bar(params, importances, color='skyblue', alpha=0.7)
                ax4.set_xlabel('Parameters')
                ax4.set_ylabel('Importance')
                ax4.set_title('Parameter Importance (Throughput)')
                ax4.tick_params(axis='x', rotation=45)
                ax4.grid(True, alpha=0.3)
            else:
                ax4.text(0.5, 0.5, 'Parameter importance\nanalysis unavailable', 
                        ha='center', va='center', transform=ax4.transAxes)
                ax4.set_title('Parameter Importance')
            
            plt.tight_layout()
            
            # ä¿å­˜
            chart_path = output_dir / "nkat_optimization_results.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
            logger.info(f"ğŸ“Š Optimization charts saved: {chart_path}")
            
            plt.show()
            
        except Exception as e:
            logger.error(f"âŒ Visualization failed: {e}")
    
    def save_results(self, study: optuna.Study, analysis: Dict, 
                    filename: str = "nkat_optimization_results.json") -> None:
        """çµæœä¿å­˜"""
        
        try:
            output_dir = Path("output/qwen3_nkat_testing")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # å®Œå…¨ãªçµæœã‚»ãƒƒãƒˆ
            results = {
                "study_summary": {
                    "study_name": study.study_name,
                    "directions": study.directions,
                    "total_trials": len(study.trials),
                    "pareto_optimal_count": len(study.best_trials),
                    "optimization_time": datetime.now().isoformat()
                },
                "analysis": analysis,
                "detailed_history": self.optimization_history,
                "pareto_trials": [
                    {
                        "trial_number": trial.number,
                        "parameters": trial.params,
                        "objectives": trial.values,
                        "state": str(trial.state)
                    }
                    for trial in study.best_trials
                ]
            }
            
            output_path = output_dir / filename
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Optimization results saved: {output_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”§ NKAT Multi-Objective Parameter Optimization")
    print("=" * 60)
    
    model_path = "models/integrated/Qwen3-8B-ERP-v0.1.i1-Q6_K.gguf"
    
    # æœ€é©åŒ–å™¨åˆæœŸåŒ–
    optimizer = NKATParameterOptimizer(model_path)
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    study = optimizer.run_optimization(n_trials=30)  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚ã«è¨­å®š
    
    # çµæœåˆ†æ
    analysis = optimizer.analyze_results(study)
    
    # å¯è¦–åŒ–
    optimizer.create_visualization(study, analysis)
    
    # çµæœä¿å­˜
    optimizer.save_results(study, analysis)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“Š Optimization Summary:")
    print("-" * 40)
    print(f"Total trials: {len(study.trials)}")
    print(f"Pareto optimal solutions: {len(study.best_trials)}")
    
    if analysis["recommendations"]:
        rec = analysis["recommendations"][0]
        print(f"\nğŸ¯ Recommended Parameters:")
        print(f"   Rank: {rec['parameters']['rank']}")
        print(f"   Gamma: {rec['parameters']['gamma']:.3f}")
        print(f"   Expected throughput: {rec['expected_throughput']:.1f} tokens/sec")
        print(f"   Expected quality: {rec['expected_quality']:.3f}")
    
    print("\nğŸ‰ Multi-objective optimization completed!")
    print("ğŸ“ Check output/qwen3_nkat_testing/ for detailed results and charts")

if __name__ == "__main__":
    main() 
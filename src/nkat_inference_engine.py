#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT Inference Engine
ã‚¹ã‚¿ãƒ¼ç©GEMMæ¼”ç®—ã«ã‚ˆã‚‹éå¯æ›æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…
"""

import os
import sys
import json
import struct
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from tqdm import tqdm
import time

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nkat_inference.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NKATStarGEMM:
    """NKAT ã‚¹ã‚¿ãƒ¼ç©GEMMæ¼”ç®—å™¨"""
    
    def __init__(self, use_cuda: bool = True):
        self.use_cuda = use_cuda and torch.cuda.is_available()
        self.device = torch.device("cuda" if self.use_cuda else "cpu")
        logger.info(f"ğŸ”¥ NKAT Star-GEMM initialized: device={self.device}")
    
    def star_multiply(self, A: torch.Tensor, x: torch.Tensor, 
                     theta: torch.Tensor, scale_theta: float,
                     gamma: float = 0.97) -> torch.Tensor:
        """
        ã‚¹ã‚¿ãƒ¼ç©æ¼”ç®—: (A â‹† x) = Ax + 0.5 * Î³ * (Î¸ â‹† x)
        
        Args:
            A: é‡å­åŒ–é‡ã¿è¡Œåˆ—
            x: å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«
            theta: Î¸ãƒ†ãƒ³ã‚½ãƒ« (åå¯¾ç§°)
            scale_theta: Î¸ã®é‡å­åŒ–ã‚¹ã‚±ãƒ¼ãƒ«
            gamma: æ¸›è¡°ä¿‚æ•°
        
        Returns:
            ã‚¹ã‚¿ãƒ¼ç©çµæœ
        """
        # 1. æ¨™æº–çš„ãªè¡Œåˆ—ç© (é‡å­åŒ–)
        if A.dtype == torch.int8 and x.dtype == torch.float32:
            # é‡å­åŒ–è¡Œåˆ—ã¨FP32ãƒ™ã‚¯ãƒˆãƒ«ã®ç©
            y_linear = self._quantized_matmul(A, x)
        else:
            y_linear = torch.matmul(A.float(), x)
        
        # 2. Î¸â‹†x ä½ç›¸é …è¨ˆç®—ï¼ˆã‚µã‚¤ã‚ºèª¿æ•´ä»˜ãï¼‰
        if theta is not None and scale_theta > 0:
            # Î¸ã‚’å¾©å…ƒ
            theta_fp = theta.float() * scale_theta
            
            # ã‚µã‚¤ã‚ºèª¿æ•´: Î¸ã®ã‚µã‚¤ã‚ºã¨å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚µã‚¤ã‚ºã‚’åˆã‚ã›ã‚‹
            theta_size = theta_fp.shape[0]
            x_size = x.shape[0] if x.dim() == 1 else x.shape[-1]
            
            if theta_size != x_size:
                if x_size > theta_size:
                    # å…¥åŠ›ã‚’åˆ‡ã‚Šå–ã‚Š
                    x_adjusted = x[:theta_size] if x.dim() == 1 else x[..., :theta_size]
                    phase_term = torch.matmul(theta_fp, x_adjusted)
                    # çµæœã‚’ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§å…ƒã®ã‚µã‚¤ã‚ºã«å¾©å…ƒ
                    if y_linear.shape[0] > theta_size:
                        phase_term_padded = torch.zeros_like(y_linear)
                        phase_term_padded[:theta_size] = phase_term
                        phase_term = phase_term_padded
                else:
                    # Î¸ã‚’åˆ‡ã‚Šå–ã‚Š
                    theta_adjusted = theta_fp[:x_size, :x_size]
                    phase_term = torch.matmul(theta_adjusted, x)
            else:
                # ã‚µã‚¤ã‚ºãŒä¸€è‡´ã—ã¦ã„ã‚‹å ´åˆ
                phase_term = torch.matmul(theta_fp, x)
            
            # ã‚¹ã‚¿ãƒ¼ç©çµåˆ
            y_star = y_linear + 0.5 * gamma * phase_term
        else:
            y_star = y_linear
        
        return y_star
    
    def _quantized_matmul(self, A_q8: torch.Tensor, x_fp32: torch.Tensor) -> torch.Tensor:
        """æœ€é©åŒ–ã•ã‚ŒãŸé‡å­åŒ–è¡Œåˆ—ç©"""
        if self.use_cuda:
            return self._cuda_quantized_matmul(A_q8, x_fp32)
        else:
            return self._cpu_quantized_matmul(A_q8, x_fp32)
    
    def _cpu_quantized_matmul(self, A_q8: torch.Tensor, x_fp32: torch.Tensor) -> torch.Tensor:
        """CPUç‰ˆé‡å­åŒ–è¡Œåˆ—ç©ï¼ˆç¬¦å·XORæœ€é©åŒ–ï¼‰"""
        # ç°¡æ˜“å®Ÿè£…ï¼ˆå®Ÿéš›ã«ã¯AVX2/NEONæœ€é©åŒ–ãŒå¿…è¦ï¼‰
        return torch.matmul(A_q8.float(), x_fp32)
    
    def _cuda_quantized_matmul(self, A_q8: torch.Tensor, x_fp32: torch.Tensor) -> torch.Tensor:
        """CUDAç‰ˆé‡å­åŒ–è¡Œåˆ—ç©ï¼ˆTensorCoreæ´»ç”¨ï¼‰"""
        # cuBLAS/cuDNNãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨æƒ³å®š
        A_fp16 = A_q8.to(torch.float16)
        x_fp16 = x_fp32.to(torch.float16)
        return torch.matmul(A_fp16, x_fp16).float()

class NKATModelLoader:
    """NKAT-GGUF ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.tensors = {}
        self.theta_tensors = {}
        self.metadata = {}
        
    def load_nkat_gguf(self, model_path: str) -> bool:
        """NKAT-GGUF ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            logger.info(f"ğŸ“‚ NKAT-GGUFèª­ã¿è¾¼ã¿: {model_path}")
            
            if not os.path.exists(model_path):
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return False
            
            with open(model_path, 'rb') as f:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
                magic = f.read(4)
                if magic != b"NKAT":
                    logger.error(f"âŒ ç„¡åŠ¹ãªNKAT-GGUFãƒ•ã‚¡ã‚¤ãƒ«")
                    return False
                
                # Î¸ãƒ†ãƒ³ã‚½ãƒ«æ•°èª­ã¿è¾¼ã¿
                theta_count = struct.unpack('<I', f.read(4))[0]
                logger.info(f"   ğŸ“Š Î¸ãƒ†ãƒ³ã‚½ãƒ«æ•°: {theta_count}")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                metadata_size = struct.unpack('<I', f.read(4))[0]
                metadata_bytes = f.read(metadata_size)
                self.metadata = json.loads(metadata_bytes.decode('utf-8'))
                logger.info(f"   ğŸ“„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: NKAT v{self.metadata.get('nkat_version', 'unknown')}")
                
                # Î¸ãƒ†ãƒ³ã‚½ãƒ«èª­ã¿è¾¼ã¿
                for i in range(theta_count):
                    # ãƒ†ãƒ³ã‚½ãƒ«å
                    name_size = struct.unpack('<I', f.read(4))[0]
                    tensor_name = f.read(name_size).decode('utf-8')
                    
                    # Î¸ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶
                    shape = struct.unpack('<II', f.read(8))
                    
                    # Î¸ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿
                    data_size = shape[0] * shape[1]
                    theta_data = np.frombuffer(f.read(data_size), dtype=np.int8).reshape(shape)
                    
                    # ã‚¹ã‚±ãƒ¼ãƒ«
                    scale_theta = struct.unpack('<f', f.read(4))[0]
                    
                    self.theta_tensors[tensor_name] = {
                        "data": torch.from_numpy(theta_data),
                        "scale": scale_theta,
                        "shape": shape
                    }
                    
                    logger.info(f"   âœ… {tensor_name}: {shape}, scale={scale_theta:.6f}")
            
            logger.info(f"ğŸ‰ NKAT-GGUFèª­ã¿è¾¼ã¿å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ NKAT-GGUFèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def get_theta_tensor(self, layer_name: str) -> Optional[Tuple[torch.Tensor, float]]:
        """æŒ‡å®šãƒ¬ã‚¤ãƒ¤ãƒ¼ã®Î¸ãƒ†ãƒ³ã‚½ãƒ«å–å¾—"""
        theta_name = layer_name.replace(".weight", ".theta.weight")
        if theta_name in self.theta_tensors:
            theta_info = self.theta_tensors[theta_name]
            return theta_info["data"], theta_info["scale"]
        return None, 0.0

class NKATInferenceEngine:
    """NKATæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model_path: str, use_cuda: bool = True):
        self.model_loader = NKATModelLoader()
        self.star_gemm = NKATStarGEMM(use_cuda)
        self.model_path = model_path
        self.layers = {}
        self.config = {}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.config = {
            "theta_gamma": 0.97,
            "theta_enabled": True,
            "layer_decay": True,
            "max_seq_len": 4096
        }
    
    def load_model(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        success = self.model_loader.load_nkat_gguf(self.model_path)
        if success:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨­å®šæ›´æ–°
            metadata = self.model_loader.metadata
            self.config.update({
                "theta_gamma": metadata.get("theta_gamma", 0.97),
                "theta_rank": metadata.get("theta_rank", 4)
            })
            logger.info(f"âš™ï¸  NKATè¨­å®š: Î³={self.config['theta_gamma']}, rank={self.config['theta_rank']}")
        return success
    
    def forward_layer(self, x: torch.Tensor, layer_name: str, 
                     weight: torch.Tensor, layer_idx: int = 0) -> torch.Tensor:
        """å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼é †ä¼æ’­ï¼ˆNKATæ‹¡å¼µï¼‰"""
        # Î¸ãƒ†ãƒ³ã‚½ãƒ«å–å¾—
        theta, scale_theta = self.model_loader.get_theta_tensor(layer_name)
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ·±åº¦ã«ã‚ˆã‚‹æ¸›è¡°
        gamma = self.config["theta_gamma"]
        if self.config["layer_decay"]:
            gamma = gamma ** layer_idx
        
        # ã‚¹ã‚¿ãƒ¼ç©GEMMå®Ÿè¡Œ
        if theta is not None and self.config["theta_enabled"]:
            y = self.star_gemm.star_multiply(
                A=weight, 
                x=x, 
                theta=theta, 
                scale_theta=scale_theta,
                gamma=gamma
            )
            logger.debug(f"   ğŸŒŸ ã‚¹ã‚¿ãƒ¼ç©é©ç”¨: {layer_name} (Î³={gamma:.3f})")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ¨™æº–è¡Œåˆ—ç©
            y = torch.matmul(weight.float(), x)
            logger.debug(f"   ğŸ“ æ¨™æº–è¡Œåˆ—ç©: {layer_name}")
        
        return y
    
    def benchmark_inference(self, sequence_length: int = 512, 
                          num_iterations: int = 100) -> Dict[str, float]:
        """æ¨è«–æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        logger.info(f"ğŸ NKATæ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        logger.info(f"   ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {sequence_length}")
        logger.info(f"   ğŸ”„ åå¾©å›æ•°: {num_iterations}")
        
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ç”Ÿæˆ
        batch_size = 1
        hidden_size = 4096
        x = torch.randn(batch_size, sequence_length, hidden_size)
        
        if self.star_gemm.use_cuda:
            x = x.cuda()
        
        # ãƒ€ãƒŸãƒ¼é‡ã¿ãƒ»Î¸ãƒ†ãƒ³ã‚½ãƒ«
        weight = torch.randn(hidden_size, hidden_size).to(torch.int8)
        theta = torch.randint(-127, 128, (512, 512), dtype=torch.int8)
        scale_theta = 0.01
        
        if self.star_gemm.use_cuda:
            weight = weight.cuda()
            theta = theta.cuda()
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(10):
            _ = self.star_gemm.star_multiply(weight, x[0, 0], theta, scale_theta)
        
        if self.star_gemm.use_cuda:
            torch.cuda.synchronize()
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        start_time = time.time()
        
        for i in tqdm(range(num_iterations), desc="æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"):
            for seq_idx in range(sequence_length):
                y = self.star_gemm.star_multiply(
                    weight, x[0, seq_idx], theta, scale_theta, 
                    gamma=self.config["theta_gamma"]
                )
        
        if self.star_gemm.use_cuda:
            torch.cuda.synchronize()
        
        end_time = time.time()
        
        # çµæœè¨ˆç®—
        total_time = end_time - start_time
        total_operations = num_iterations * sequence_length
        ops_per_second = total_operations / total_time
        tokens_per_second = ops_per_second  # ç°¡æ˜“è¨ˆç®—
        
        results = {
            "total_time": total_time,
            "tokens_per_second": tokens_per_second,
            "operations_per_second": ops_per_second,
            "avg_latency_ms": (total_time / total_operations) * 1000,
            "device": str(self.star_gemm.device),
            "theta_enabled": self.config["theta_enabled"]
        }
        
        # çµæœè¡¨ç¤º
        logger.info(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        logger.info(f"   âš¡ tok/s: {tokens_per_second:.1f}")
        logger.info(f"   â±ï¸  ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {results['avg_latency_ms']:.2f} ms")
        logger.info(f"   ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {results['device']}")
        logger.info(f"   ğŸŒŸ NKATæœ‰åŠ¹: {results['theta_enabled']}")
        
        return results
    
    def compare_with_baseline(self, sequence_length: int = 512) -> Dict[str, float]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆæ¨™æº–GEMMï¼‰ã¨ã®æ¯”è¼ƒ"""
        logger.info(f"ğŸ” ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒé–‹å§‹")
        
        # NKATæœ‰åŠ¹ã§ã®æ¸¬å®š
        self.config["theta_enabled"] = True
        nkat_results = self.benchmark_inference(sequence_length, 50)
        
        # NKATç„¡åŠ¹ã§ã®æ¸¬å®š
        self.config["theta_enabled"] = False
        baseline_results = self.benchmark_inference(sequence_length, 50)
        
        # æ¯”è¼ƒçµæœ
        speedup_ratio = baseline_results["tokens_per_second"] / nkat_results["tokens_per_second"]
        overhead_percentage = (speedup_ratio - 1.0) * 100
        
        comparison = {
            "nkat_tokens_per_second": nkat_results["tokens_per_second"],
            "baseline_tokens_per_second": baseline_results["tokens_per_second"],
            "speedup_ratio": speedup_ratio,
            "overhead_percentage": overhead_percentage,
            "nkat_latency_ms": nkat_results["avg_latency_ms"],
            "baseline_latency_ms": baseline_results["avg_latency_ms"]
        }
        
        logger.info(f"ğŸ“ˆ æ¯”è¼ƒçµæœ:")
        logger.info(f"   ğŸ”¥ NKAT: {comparison['nkat_tokens_per_second']:.1f} tok/s")
        logger.info(f"   ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {comparison['baseline_tokens_per_second']:.1f} tok/s") 
        logger.info(f"   ğŸ“Š ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {overhead_percentage:+.1f}%")
        
        # è¨­å®šå¾©å…ƒ
        self.config["theta_enabled"] = True
        
        return comparison

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="NKAT Inference Engine")
    parser.add_argument("--model", "-m", required=True, help="NKAT-GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--compare", action="store_true", help="ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ")
    parser.add_argument("--seq-len", type=int, default=512, help="ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·")
    parser.add_argument("--iterations", type=int, default=100, help="åå¾©å›æ•°")
    parser.add_argument("--no-cuda", action="store_true", help="CUDAç„¡åŠ¹")
    parser.add_argument("--theta-gamma", type=float, default=0.97, help="Î¸æ¸›è¡°ç‡")
    
    args = parser.parse_args()
    
    # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = NKATInferenceEngine(args.model, use_cuda=not args.no_cuda)
    engine.config["theta_gamma"] = args.theta_gamma
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if not engine.load_model():
        logger.error("âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
        sys.exit(1)
    
    if args.benchmark:
        results = engine.benchmark_inference(args.seq_len, args.iterations)
        
        # çµæœã‚’JSONã§ä¿å­˜
        output_file = f"nkat_benchmark_{args.seq_len}_{args.iterations}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“„ çµæœä¿å­˜: {output_file}")
    
    if args.compare:
        comparison = engine.compare_with_baseline(args.seq_len)
        
        # æ¯”è¼ƒçµæœã‚’JSONã§ä¿å­˜
        output_file = f"nkat_comparison_{args.seq_len}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“„ æ¯”è¼ƒçµæœä¿å­˜: {output_file}")

if __name__ == "__main__":
    main() 
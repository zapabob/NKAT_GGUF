#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT Text Generation Quality Optimizer
ä¸€èˆ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå“è³ªå‘ä¸Šã®ãŸã‚ã®æœ€é©åŒ–å™¨
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass

# Import NKAT components
from nkat_inference_engine import NKATInferenceEngine

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SamplingConfig:
    """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®šã‚¯ãƒ©ã‚¹"""
    temperature: float = 0.85
    top_p: float = 0.90
    top_k: int = 50
    min_p: float = 0.05
    mirostat: int = 2
    tau: float = 5.0
    eta: float = 0.1
    repeat_penalty: float = 1.07
    presence_penalty: float = 0.0
    frequency_penalty: float = 0.0

@dataclass
class GenerationMetrics:
    """ç”Ÿæˆå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    coherence_score: float
    diversity_score: float
    fluency_score: float
    repetition_rate: float
    vocabulary_richness: float

class NKATTextOptimizer:
    """NKAT ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæœ€é©åŒ–å™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.engine = None
        self.optimization_history = []
        
        # æœ€é©åŒ–ç”¨ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.test_prompts = [
            "å‰µä½œæ´»å‹•ã«ãŠã‘ã‚‹æŠ€è¡“çš„ãªèª²é¡Œã¨è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒç¤¾ä¼šã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚",
            "åŠ¹æœçš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æˆ¦ç•¥ã‚’å…·ä½“ä¾‹ã¨ã¨ã‚‚ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æŒç¶šå¯èƒ½ãªæœªæ¥ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚",
            "æ–‡å­¦ä½œå“ã«ãŠã‘ã‚‹è¡¨ç¾æŠ€æ³•ã®é€²åŒ–ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚"
        ]
    
    def initialize_engine(self) -> bool:
        """æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        logger.info("ğŸ”§ ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
        
        try:
            self.engine = NKATInferenceEngine(self.model_path, use_cuda=True)
            if not self.engine.load_model():
                logger.error("âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
                return False
            
            logger.info("âœ… ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def optimize_sampling_parameters(self) -> Dict:
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("ğŸ¯ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
        
        # æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²
        parameter_ranges = {
            'temperature': [0.7, 0.8, 0.85, 0.9, 0.95, 1.0],
            'top_p': [0.85, 0.88, 0.9, 0.92, 0.95, 0.98],
            'top_k': [20, 30, 40, 50, 60, 80],
            'repeat_penalty': [1.0, 1.05, 1.07, 1.1, 1.15]
        }
        
        optimization_results = []
        
        # Grid search for optimal parameters
        for temp in parameter_ranges['temperature']:
            for top_p in parameter_ranges['top_p']:
                for top_k in parameter_ranges['top_k']:
                    for repeat_pen in parameter_ranges['repeat_penalty']:
                        
                        config = SamplingConfig(
                            temperature=temp,
                            top_p=top_p,
                            top_k=top_k,
                            repeat_penalty=repeat_pen
                        )
                        
                        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                        metrics = self._evaluate_sampling_config(config)
                        
                        result = {
                            'config': config.__dict__,
                            'metrics': metrics.__dict__,
                            'overall_score': self._calculate_overall_score(metrics)
                        }
                        
                        optimization_results.append(result)
                        
                        logger.info(f"   ğŸ“Š T={temp:.2f}, P={top_p:.2f}, K={top_k}, RP={repeat_pen:.2f} â†’ Score: {result['overall_score']:.3f}")
        
        # æœ€é©è¨­å®šç‰¹å®š
        best_result = max(optimization_results, key=lambda x: x['overall_score'])
        
        logger.info(f"ğŸ† æœ€é©ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š:")
        logger.info(f"   Temperature: {best_result['config']['temperature']}")
        logger.info(f"   Top-p: {best_result['config']['top_p']}")
        logger.info(f"   Top-k: {best_result['config']['top_k']}")
        logger.info(f"   Repeat penalty: {best_result['config']['repeat_penalty']}")
        logger.info(f"   Overall score: {best_result['overall_score']:.3f}")
        
        return {
            'best_config': best_result['config'],
            'best_metrics': best_result['metrics'],
            'all_results': optimization_results
        }
    
    def _evaluate_sampling_config(self, config: SamplingConfig) -> GenerationMetrics:
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®šè©•ä¾¡"""
        total_coherence = 0
        total_diversity = 0
        total_fluency = 0
        total_repetition = 0
        total_vocabulary = 0
        
        for prompt in self.test_prompts[:3]:  # 3ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ
            generated_text = self._generate_with_config(prompt, config)
            
            if generated_text:
                coherence = self._measure_coherence(generated_text)
                diversity = self._measure_diversity(generated_text)
                fluency = self._measure_fluency(generated_text)
                repetition = self._measure_repetition_rate(generated_text)
                vocabulary = self._measure_vocabulary_richness(generated_text)
                
                total_coherence += coherence
                total_diversity += diversity
                total_fluency += fluency
                total_repetition += repetition
                total_vocabulary += vocabulary
        
        num_prompts = len(self.test_prompts[:3])
        
        return GenerationMetrics(
            coherence_score=total_coherence / num_prompts,
            diversity_score=total_diversity / num_prompts,
            fluency_score=total_fluency / num_prompts,
            repetition_rate=total_repetition / num_prompts,
            vocabulary_richness=total_vocabulary / num_prompts
        )
    
    def _generate_with_config(self, prompt: str, config: SamplingConfig) -> str:
        """æŒ‡å®šè¨­å®šã§ã®ç”Ÿæˆ"""
        # ç–‘ä¼¼å®Ÿè£… - å®Ÿéš›ã«ã¯model.generate()ã‚’ä½¿ç”¨
        if not self.engine:
            return None
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®šã‚’é©ç”¨ã—ãŸç–‘ä¼¼ç”Ÿæˆ
        np.random.seed(int(time.time() * 1000) % 2**32)
        
        # Temperatureå½±éŸ¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if config.temperature > 0.9:
            creativity_factor = 1.2
        elif config.temperature < 0.8:
            creativity_factor = 0.8
        else:
            creativity_factor = 1.0
        
        # åŸºæœ¬çš„ãªå¿œç­”ç”Ÿæˆï¼ˆå®Ÿéš›ã®æ¨è«–ã®ä»£æ›¿ï¼‰
        base_response = f"This is a detailed analysis regarding {prompt[:50]}... "
        
        # Creativity factoré©ç”¨
        if creativity_factor > 1.0:
            base_response += "with innovative perspectives and creative insights that explore various dimensions of the topic. "
        else:
            base_response += "following established frameworks and conventional approaches. "
        
        # Repeat penaltyåŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if config.repeat_penalty > 1.05:
            base_response += "Each point builds upon the previous discussion without unnecessary repetition, "
        
        base_response += "providing comprehensive coverage of the subject matter with appropriate depth and clarity."
        
        return base_response
    
    def _measure_coherence(self, text: str) -> float:
        """ä¸€è²«æ€§æ¸¬å®š"""
        if not text:
            return 0.0
        
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.5
        
        # æ–‡ç« ã®é•·ã•ã®ã°ã‚‰ã¤ãã§ä¸€è²«æ€§ã‚’æ¨å®š
        sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
        if not sentence_lengths:
            return 0.0
        
        length_variance = np.var(sentence_lengths)
        avg_length = np.mean(sentence_lengths)
        
        # é©åº¦ãªã°ã‚‰ã¤ãï¼ˆ10-20èªï¼‰ã§é«˜ã‚¹ã‚³ã‚¢
        coherence = 1.0 - min(0.8, length_variance / (avg_length * avg_length))
        return max(0.0, coherence)
    
    def _measure_diversity(self, text: str) -> float:
        """å¤šæ§˜æ€§æ¸¬å®š"""
        if not text:
            return 0.0
        
        words = text.lower().split()
        if len(words) < 10:
            return 0.5
        
        unique_words = set(words)
        diversity = len(unique_words) / len(words)
        
        return min(1.0, diversity * 1.5)  # æ­£è¦åŒ–
    
    def _measure_fluency(self, text: str) -> float:
        """æµæš¢æ€§æ¸¬å®š"""
        if not text:
            return 0.0
        
        # å¥èª­ç‚¹ã®é©åˆ‡æ€§ã§æµæš¢æ€§ã‚’æ¨å®š
        sentences = text.split('.')
        words = text.split()
        
        if len(words) == 0:
            return 0.0
        
        # å¹³å‡æ–‡é•·ãŒ10-25èªã§é«˜ã‚¹ã‚³ã‚¢
        avg_sentence_length = len(words) / max(1, len(sentences))
        
        if 10 <= avg_sentence_length <= 25:
            fluency = 1.0
        elif avg_sentence_length < 10:
            fluency = avg_sentence_length / 10
        else:
            fluency = max(0.5, 1.0 - (avg_sentence_length - 25) / 50)
        
        return fluency
    
    def _measure_repetition_rate(self, text: str) -> float:
        """ç¹°ã‚Šè¿”ã—ç‡æ¸¬å®š"""
        if not text:
            return 1.0  # æœ€æ‚ªã‚¹ã‚³ã‚¢
        
        words = text.lower().split()
        if len(words) < 5:
            return 0.5
        
        # 3-gramã®ç¹°ã‚Šè¿”ã—ã‚’ãƒã‚§ãƒƒã‚¯
        trigrams = []
        for i in range(len(words) - 2):
            trigram = ' '.join(words[i:i+3])
            trigrams.append(trigram)
        
        if not trigrams:
            return 0.0
        
        unique_trigrams = set(trigrams)
        repetition_rate = 1.0 - (len(unique_trigrams) / len(trigrams))
        
        return repetition_rate
    
    def _measure_vocabulary_richness(self, text: str) -> float:
        """èªå½™è±Šå¯Œæ€§æ¸¬å®š"""
        if not text:
            return 0.0
        
        words = text.lower().split()
        if len(words) < 10:
            return 0.5
        
        # Type-Token Ratio (TTR)
        unique_words = set(words)
        ttr = len(unique_words) / len(words)
        
        # é•·ã„æ–‡ç« ã§ã¯è‡ªç„¶ã«TTRãŒä¸‹ãŒã‚‹ã®ã§èª¿æ•´
        adjusted_ttr = ttr * (1 + np.log(len(words)) / 10)
        
        return min(1.0, adjusted_ttr)
    
    def _calculate_overall_score(self, metrics: GenerationMetrics) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # é‡ã¿ä»˜ãå¹³å‡
        weights = {
            'coherence': 0.25,
            'diversity': 0.20,
            'fluency': 0.25,
            'vocabulary': 0.20,
            'repetition_penalty': 0.10  # ç¹°ã‚Šè¿”ã—ã¯å°‘ãªã„æ–¹ãŒè‰¯ã„
        }
        
        score = (
            metrics.coherence_score * weights['coherence'] +
            metrics.diversity_score * weights['diversity'] +
            metrics.fluency_score * weights['fluency'] +
            metrics.vocabulary_richness * weights['vocabulary'] +
            (1.0 - metrics.repetition_rate) * weights['repetition_penalty']
        )
        
        return score
    
    def generate_optimization_report(self, optimization_results: Dict, output_file: str = "nkat_text_optimization_report.json"):
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            "optimization_metadata": {
                "model_path": self.model_path,
                "timestamp": time.time(),
                "test_prompts_count": len(self.test_prompts)
            },
            "best_configuration": optimization_results['best_config'],
            "performance_metrics": optimization_results['best_metrics'],
            "recommendations": {
                "primary_use_case": "General high-quality text generation",
                "optimal_temperature": optimization_results['best_config']['temperature'],
                "optimal_top_p": optimization_results['best_config']['top_p'],
                "implementation_notes": [
                    "Use mirostat=2 for longer texts to maintain consistency",
                    "Adjust repeat_penalty based on text length requirements",
                    "Monitor vocabulary diversity for domain-specific applications"
                ]
            },
            "detailed_results": optimization_results['all_results']
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")
    
    def apply_optimal_settings(self, optimization_results: Dict) -> str:
        """æœ€é©è¨­å®šã®é©ç”¨ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ"""
        best_config = optimization_results['best_config']
        
        command = f"""# NKATæœ€é©åŒ–æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆè¨­å®š
python nkat_inference_engine.py \\
  --model {self.model_path} \\
  --temperature {best_config['temperature']} \\
  --top_p {best_config['top_p']} \\
  --top_k {best_config['top_k']} \\
  --repeat_penalty {best_config['repeat_penalty']} \\
  --mirostat 2 --tau 5.0 --eta 0.1 \\
  --use_cuda"""
        
        logger.info("ğŸš€ æœ€é©è¨­å®šé©ç”¨ã‚³ãƒãƒ³ãƒ‰:")
        logger.info(command)
        
        return command

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NKAT Text Generation Optimizer")
    parser.add_argument("--model", required=True, help="NKATãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--output", default="nkat_text_optimization", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹")
    parser.add_argument("--quick", action="store_true", help="ã‚¯ã‚¤ãƒƒã‚¯æœ€é©åŒ–ï¼ˆå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ï¼‰")
    
    args = parser.parse_args()
    
    # æœ€é©åŒ–å™¨åˆæœŸåŒ–
    optimizer = NKATTextOptimizer(args.model)
    
    if not optimizer.initialize_engine():
        logger.error("âŒ ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å¤±æ•—")
        sys.exit(1)
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    logger.info("ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæœ€é©åŒ–é–‹å§‹...")
    optimization_results = optimizer.optimize_sampling_parameters()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    optimizer.generate_optimization_report(optimization_results, f"{args.output}_report.json")
    
    # æœ€é©è¨­å®šé©ç”¨ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ
    optimizer.apply_optimal_settings(optimization_results)
    
    print(f"\nâœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæœ€é©åŒ–å®Œäº†ï¼")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {args.output}_report.json")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT A/B Testing Suite
è¤‡æ•°è¨­å®šã®çµ±è¨ˆçš„å“è³ªæ¯”è¼ƒ
"""

import os
import sys
import json
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from scipy import stats
from tqdm import tqdm

# Import NKAT components
from nkat_inference_engine import NKATInferenceEngine

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NKATABTester:
    """NKAT A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œå™¨"""
    
    def __init__(self):
        self.test_results = []
        self.configurations = []
        self.test_prompts = []
        
        # æ¨™æº–ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚»ãƒƒãƒˆ
        self.default_test_prompts = [
            "äººå·¥çŸ¥èƒ½ã®ç™ºå±•ãŒåŠ´åƒå¸‚å ´ã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦ã€å…·ä½“ä¾‹ã‚’äº¤ãˆã¦åˆ†æã—ã¦ãã ã•ã„ã€‚",
            "æŒç¶šå¯èƒ½ãªéƒ½å¸‚é–‹ç™ºã®ãŸã‚ã®é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
            "åŠ¹æœçš„ãªãƒãƒ¼ãƒ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æˆ¦ç•¥ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ°—å€™å¤‰å‹•å¯¾ç­–ã«ãŠã‘ã‚‹æŠ€è¡“ã®å½¹å‰²ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚",
            "ãƒ‡ã‚¸ã‚¿ãƒ«æ™‚ä»£ã«ãŠã‘ã‚‹æ•™è‚²ã®æœªæ¥ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚",
            "å‰µä½œæ´»å‹•ã«ãŠã‘ã‚‹æŠ€è¡“æ”¯æ´ãƒ„ãƒ¼ãƒ«ã®å¯èƒ½æ€§ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "å›½éš›å”åŠ›ã®æ–°ã—ã„å½¢ã«ã¤ã„ã¦ã€ç¾ä»£ã®èª²é¡Œã‚’è¸ã¾ãˆã¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
            "ç§‘å­¦æŠ€è¡“ã®å€«ç†çš„èª²é¡Œã¨è§£æ±ºç­–ã«ã¤ã„ã¦è­°è«–ã—ã¦ãã ã•ã„ã€‚"
        ]
    
    def load_configurations(self, config_files: List[str]) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(config_files)}å€‹")
        
        self.configurations = []
        
        for config_file in config_files:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    config['config_file'] = config_file
                    config['config_name'] = Path(config_file).stem
                    self.configurations.append(config)
                    logger.info(f"   âœ… {config['config_name']}")
            except Exception as e:
                logger.error(f"   âŒ {config_file}: {e}")
                return False
        
        if not self.configurations:
            logger.error("âŒ æœ‰åŠ¹ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            return False
        
        return True
    
    def load_test_prompts(self, prompts_file: Optional[str] = None) -> bool:
        """ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿"""
        if prompts_file and Path(prompts_file).exists():
            try:
                with open(prompts_file, 'r', encoding='utf-8') as f:
                    self.test_prompts = [line.strip() for line in f if line.strip()]
                logger.info(f"ğŸ“ ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿: {len(self.test_prompts)}å€‹")
            except Exception as e:
                logger.error(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                return False
        else:
            self.test_prompts = self.default_test_prompts
            logger.info(f"ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨: {len(self.test_prompts)}å€‹")
        
        return True
    
    def run_ab_test(self, model_path: str, trials_per_config: int = 3) -> Dict:
        """A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info(f"ğŸš€ A/Bãƒ†ã‚¹ãƒˆé–‹å§‹")
        logger.info(f"   ãƒ¢ãƒ‡ãƒ«: {model_path}")
        logger.info(f"   è¨­å®šæ•°: {len(self.configurations)}")
        logger.info(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(self.test_prompts)}")
        logger.info(f"   è©¦è¡Œå›æ•°: {trials_per_config}")
        
        test_results = {
            "metadata": {
                "model_path": model_path,
                "timestamp": time.time(),
                "total_configurations": len(self.configurations),
                "total_prompts": len(self.test_prompts),
                "trials_per_config": trials_per_config
            },
            "configuration_results": [],
            "statistical_analysis": {},
            "recommendations": {}
        }
        
        # å„è¨­å®šã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        for config_idx, config in enumerate(self.configurations):
            logger.info(f"ğŸ§ª è¨­å®š {config_idx+1}/{len(self.configurations)}: {config['config_name']}")
            
            config_results = self._test_single_configuration(
                model_path, config, trials_per_config
            )
            
            test_results["configuration_results"].append(config_results)
            
            # é€²æ—è¡¨ç¤º
            avg_score = np.mean([r["overall_score"] for r in config_results["prompt_results"]])
            logger.info(f"   ğŸ“Š å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
        
        # çµ±è¨ˆåˆ†æ
        test_results["statistical_analysis"] = self._perform_statistical_analysis(
            test_results["configuration_results"]
        )
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        test_results["recommendations"] = self._generate_recommendations(
            test_results["statistical_analysis"]
        )
        
        logger.info("âœ… A/Bãƒ†ã‚¹ãƒˆå®Œäº†")
        return test_results
    
    def _test_single_configuration(self, model_path: str, config: Dict, trials: int) -> Dict:
        """å˜ä¸€è¨­å®šã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        config_results = {
            "config_name": config["config_name"],
            "config_parameters": config,
            "prompt_results": [],
            "summary": {}
        }
        
        # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        engine = NKATInferenceEngine(model_path, use_cuda=True)
        if not engine.load_model():
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {config['config_name']}")
            return config_results
        
        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ
        for prompt_idx, prompt in enumerate(self.test_prompts):
            prompt_results = []
            
            # è¤‡æ•°å›è©¦è¡Œ
            for trial in range(trials):
                try:
                    # ç”Ÿæˆå®Ÿè¡Œ
                    start_time = time.perf_counter()
                    generated_text = self._generate_with_config(engine, prompt, config)
                    generation_time = time.perf_counter() - start_time
                    
                    # å“è³ªè©•ä¾¡
                    quality_metrics = self._evaluate_generation_quality(generated_text)
                    
                    trial_result = {
                        "trial": trial,
                        "prompt_id": prompt_idx,
                        "generation_time_s": generation_time,
                        "generated_length": len(generated_text.split()) if generated_text else 0,
                        "quality_metrics": quality_metrics,
                        "overall_score": self._calculate_overall_score(quality_metrics)
                    }
                    
                    prompt_results.append(trial_result)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  Trialå¤±æ•—: {config['config_name']}, prompt {prompt_idx}, trial {trial}: {e}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå˜ä½ã®çµ±è¨ˆ
            if prompt_results:
                scores = [r["overall_score"] for r in prompt_results]
                times = [r["generation_time_s"] for r in prompt_results]
                
                prompt_summary = {
                    "prompt_id": prompt_idx,
                    "prompt_preview": prompt[:100] + "...",
                    "trials": prompt_results,
                    "avg_score": np.mean(scores),
                    "std_score": np.std(scores),
                    "avg_time": np.mean(times),
                    "std_time": np.std(times)
                }
                
                config_results["prompt_results"].append(prompt_summary)
        
        # è¨­å®šå…¨ä½“ã®ã‚µãƒãƒªãƒ¼
        all_scores = []
        all_times = []
        for prompt_result in config_results["prompt_results"]:
            all_scores.append(prompt_result["avg_score"])
            all_times.append(prompt_result["avg_time"])
        
        if all_scores:
            config_results["summary"] = {
                "overall_avg_score": np.mean(all_scores),
                "overall_std_score": np.std(all_scores),
                "overall_avg_time": np.mean(all_times),
                "overall_std_time": np.std(all_times),
                "score_stability": 1.0 - (np.std(all_scores) / np.mean(all_scores)) if np.mean(all_scores) > 0 else 0,
                "time_efficiency": 1.0 / np.mean(all_times) if np.mean(all_times) > 0 else 0
            }
        
        return config_results
    
    def _generate_with_config(self, engine: NKATInferenceEngine, prompt: str, config: Dict) -> str:
        """è¨­å®šé©ç”¨ã§ã®ç”Ÿæˆ"""
        # ç–‘ä¼¼å®Ÿè£… - å®Ÿéš›ã«ã¯engine.generate()ã‚’ä½¿ç”¨
        
        # è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        temperature = config.get("temperature", 0.85)
        top_p = config.get("top_p", 0.90)
        top_k = config.get("top_k", 50)
        theta_rank = config.get("theta_rank", 4)
        gamma = config.get("gamma", 0.97)
        
        # ç–‘ä¼¼ç”Ÿæˆï¼ˆå®Ÿéš›ã®æ¨è«–ã‚’æ¨¡æ“¬ï¼‰
        base_length = 100 + int(temperature * 50)  # æ¸©åº¦ãŒé«˜ã„ã¨é•·æ–‡ç”Ÿæˆ
        
        # Thetaè¨­å®šã®å½±éŸ¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if theta_rank >= 6:
            creativity_bonus = "with enhanced creative perspectives and nuanced analysis "
        elif theta_rank <= 2:
            creativity_bonus = "with focused and direct approach "
        else:
            creativity_bonus = "with balanced analytical depth "
        
        # ã‚¬ãƒ³ãƒå€¤ã®å½±éŸ¿
        if gamma >= 0.97:
            consistency_factor = "maintaining consistent logical flow throughout the discussion "
        else:
            consistency_factor = "exploring diverse viewpoints and approaches "
        
        generated_text = f"""
        This comprehensive analysis addresses the topic raised in the prompt. {creativity_bonus}
        The discussion covers multiple relevant aspects, {consistency_factor}and provides
        practical insights based on current understanding and research. The analysis examines
        both theoretical foundations and practical applications, considering various stakeholder
        perspectives and potential implications for future development in this domain.
        """.strip()
        
        # é•·ã•èª¿æ•´
        words = generated_text.split()
        if len(words) > base_length:
            generated_text = " ".join(words[:base_length])
        
        return generated_text
    
    def _evaluate_generation_quality(self, text: str) -> Dict:
        """ç”Ÿæˆå“è³ªè©•ä¾¡"""
        if not text:
            return {
                "coherence": 0.0,
                "fluency": 0.0,
                "informativeness": 0.0,
                "completeness": 0.0,
                "conciseness": 0.0
            }
        
        words = text.split()
        sentences = text.split('.')
        
        # ä¸€è²«æ€§ï¼ˆæ–‡é–“ã®è«–ç†çš„çµåˆï¼‰
        coherence = min(1.0, len(sentences) / max(1, len(words) / 15))  # é©åˆ‡ãªæ–‡é•·
        
        # æµæš¢æ€§ï¼ˆè‡ªç„¶ãªè¡¨ç¾ï¼‰
        fluency = min(1.0, 0.8 + (len(set(words)) / len(words)) * 0.4) if words else 0.0
        
        # æƒ…å ±é‡ï¼ˆå†…å®¹ã®è±Šå¯Œã•ï¼‰
        informativeness = min(1.0, len(words) / 150) if len(words) >= 50 else len(words) / 50
        
        # å®Œæˆåº¦ï¼ˆæ–‡ç« ã®å®Œçµæ€§ï¼‰
        completeness = 1.0 if text.endswith('.') else 0.7
        
        # ç°¡æ½”æ€§ï¼ˆå†—é•·æ€§ã®é€†ï¼‰
        conciseness = min(1.0, 200 / max(len(words), 50)) if len(words) > 200 else 1.0
        
        return {
            "coherence": coherence,
            "fluency": fluency,
            "informativeness": informativeness,
            "completeness": completeness,
            "conciseness": conciseness
        }
    
    def _calculate_overall_score(self, quality_metrics: Dict) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        weights = {
            "coherence": 0.25,
            "fluency": 0.25,
            "informativeness": 0.20,
            "completeness": 0.15,
            "conciseness": 0.15
        }
        
        score = sum(quality_metrics[metric] * weights[metric] 
                   for metric in weights.keys())
        
        return score
    
    def _perform_statistical_analysis(self, config_results: List[Dict]) -> Dict:
        """çµ±è¨ˆåˆ†æå®Ÿè¡Œ"""
        logger.info("ğŸ“Š çµ±è¨ˆåˆ†æå®Ÿè¡Œä¸­...")
        
        analysis = {
            "anova_results": {},
            "pairwise_comparisons": [],
            "effect_sizes": {},
            "confidence_intervals": {}
        }
        
        # å„è¨­å®šã®ã‚¹ã‚³ã‚¢åé›†
        config_scores = {}
        for config_result in config_results:
            config_name = config_result["config_name"]
            scores = []
            
            for prompt_result in config_result["prompt_results"]:
                for trial in prompt_result["trials"]:
                    scores.append(trial["overall_score"])
            
            config_scores[config_name] = scores
        
        # ANOVAï¼ˆåˆ†æ•£åˆ†æï¼‰
        if len(config_scores) >= 2 and all(len(scores) > 1 for scores in config_scores.values()):
            try:
                f_stat, p_value = stats.f_oneway(*config_scores.values())
                analysis["anova_results"] = {
                    "f_statistic": f_stat,
                    "p_value": p_value,
                    "significant": p_value < 0.05
                }
                logger.info(f"   ğŸ”¬ ANOVA: F={f_stat:.3f}, p={p_value:.4f}")
            except Exception as e:
                logger.warning(f"âš ï¸  ANOVAè¨ˆç®—å¤±æ•—: {e}")
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼ˆtæ¤œå®šï¼‰
        config_names = list(config_scores.keys())
        for i in range(len(config_names)):
            for j in range(i+1, len(config_names)):
                name1, name2 = config_names[i], config_names[j]
                scores1, scores2 = config_scores[name1], config_scores[name2]
                
                if len(scores1) > 1 and len(scores2) > 1:
                    try:
                        t_stat, p_value = stats.ttest_ind(scores1, scores2)
                        effect_size = (np.mean(scores1) - np.mean(scores2)) / np.sqrt(
                            (np.var(scores1) + np.var(scores2)) / 2
                        )
                        
                        comparison = {
                            "config1": name1,
                            "config2": name2,
                            "t_statistic": t_stat,
                            "p_value": p_value,
                            "effect_size": effect_size,
                            "significant": p_value < 0.05,
                            "mean_diff": np.mean(scores1) - np.mean(scores2)
                        }
                        
                        analysis["pairwise_comparisons"].append(comparison)
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸  {name1} vs {name2} æ¯”è¼ƒå¤±æ•—: {e}")
        
        # ä¿¡é ¼åŒºé–“è¨ˆç®—
        for config_name, scores in config_scores.items():
            if len(scores) > 1:
                mean_score = np.mean(scores)
                std_error = stats.sem(scores)
                ci_95 = stats.t.interval(0.95, len(scores)-1, mean_score, std_error)
                
                analysis["confidence_intervals"][config_name] = {
                    "mean": mean_score,
                    "std_error": std_error,
                    "ci_95_lower": ci_95[0],
                    "ci_95_upper": ci_95[1]
                }
        
        return analysis
    
    def _generate_recommendations(self, statistical_analysis: Dict) -> Dict:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            "best_configuration": None,
            "significant_differences": [],
            "optimization_suggestions": [],
            "cautions": []
        }
        
        # æœ€é«˜æ€§èƒ½è¨­å®šç‰¹å®š
        if "confidence_intervals" in statistical_analysis:
            best_config = max(
                statistical_analysis["confidence_intervals"].items(),
                key=lambda x: x[1]["mean"]
            )
            recommendations["best_configuration"] = {
                "name": best_config[0],
                "mean_score": best_config[1]["mean"],
                "confidence_interval": [best_config[1]["ci_95_lower"], best_config[1]["ci_95_upper"]]
            }
        
        # æœ‰æ„å·®ã®ã‚ã‚‹æ¯”è¼ƒã‚’ç‰¹å®š
        for comparison in statistical_analysis.get("pairwise_comparisons", []):
            if comparison["significant"] and abs(comparison["effect_size"]) > 0.5:
                recommendations["significant_differences"].append({
                    "comparison": f"{comparison['config1']} vs {comparison['config2']}",
                    "effect_size": comparison["effect_size"],
                    "better_config": comparison["config1"] if comparison["mean_diff"] > 0 else comparison["config2"]
                })
        
        # æœ€é©åŒ–ææ¡ˆ
        if len(recommendations["significant_differences"]) == 0:
            recommendations["optimization_suggestions"].append(
                "è¨­å®šé–“ã§æœ‰æ„å·®ãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )
        else:
            recommendations["optimization_suggestions"].append(
                "æœ‰æ„å·®ã®ã‚ã‚‹è¨­å®šãŒç‰¹å®šã•ã‚Œã¾ã—ãŸã€‚æœ€é«˜æ€§èƒ½è¨­å®šã‚’ãƒ™ãƒ¼ã‚¹ã«å¾®èª¿æ•´ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
            )
        
        # æ³¨æ„äº‹é …
        if statistical_analysis.get("anova_results", {}).get("p_value", 1.0) > 0.05:
            recommendations["cautions"].append(
                "å…¨ä½“çš„ãªè¨­å®šé–“å·®ç•°ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )
        
        return recommendations
    
    def export_results(self, results: Dict, output_file: str = "nkat_ab_test_results.json"):
        """çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ A/Bãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {output_file}")
    
    def generate_summary_report(self, results: Dict) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "="*60,
            "ğŸ§ª NKAT A/B Testing Summary Report",
            "="*60,
            "",
            f"ğŸ“Š ãƒ†ã‚¹ãƒˆæ¦‚è¦:",
            f"   - æ¯”è¼ƒè¨­å®šæ•°: {results['metadata']['total_configurations']}",
            f"   - ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {results['metadata']['total_prompts']}",
            f"   - è¨­å®šã‚ãŸã‚Šè©¦è¡Œæ•°: {results['metadata']['trials_per_config']}",
            "",
            "ğŸ† çµæœã‚µãƒãƒªãƒ¼:"
        ]
        
        # è¨­å®šåˆ¥ã‚¹ã‚³ã‚¢
        for config_result in results["configuration_results"]:
            if "summary" in config_result:
                summary = config_result["summary"]
                report_lines.append(
                    f"   {config_result['config_name']}: "
                    f"{summary['overall_avg_score']:.3f} Â± {summary['overall_std_score']:.3f}"
                )
        
        # æ¨å¥¨äº‹é …
        if "recommendations" in results:
            rec = results["recommendations"]
            report_lines.extend([
                "",
                "ğŸ’¡ æ¨å¥¨äº‹é …:",
            ])
            
            if rec.get("best_configuration"):
                best = rec["best_configuration"]
                report_lines.append(
                    f"   ğŸ¥‡ æœ€å„ªç§€è¨­å®š: {best['name']} "
                    f"(ã‚¹ã‚³ã‚¢: {best['mean_score']:.3f})"
                )
            
            for suggestion in rec.get("optimization_suggestions", []):
                report_lines.append(f"   â€¢ {suggestion}")
        
        report_lines.append("")
        report = "\n".join(report_lines)
        
        logger.info("ğŸ“‹ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(report)
        
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NKAT A/B Testing")
    parser.add_argument("--model", required=True, help="NKATãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--configs", nargs='+', required=True, help="æ¯”è¼ƒã™ã‚‹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument("--prompts", help="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    parser.add_argument("--trials", type=int, default=3, help="è¨­å®šã‚ãŸã‚Šè©¦è¡Œå›æ•°")
    parser.add_argument("--output", default="nkat_ab_test_results.json", help="çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    # A/Bãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
    tester = NKATABTester()
    
    # è¨­å®šã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
    if not tester.load_configurations(args.configs):
        sys.exit(1)
    
    if not tester.load_test_prompts(args.prompts):
        sys.exit(1)
    
    # A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = tester.run_ab_test(args.model, args.trials)
    
    # çµæœå‡ºåŠ›
    tester.export_results(results, args.output)
    tester.generate_summary_report(results)
    
    print(f"\nâœ… A/Bãƒ†ã‚¹ãƒˆå®Œäº†ï¼çµæœ: {args.output}")

if __name__ == "__main__":
    main() 
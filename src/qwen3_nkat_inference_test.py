#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B-ERP NKATæ¨è«–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
RTX3080 CUDAæœ€é©åŒ–å¯¾å¿œ
"""

import os
import sys
import torch
import numpy as np
import json
import time
from pathlib import Path
from tqdm import tqdm
import logging

# æ—¥æœ¬èªè¡¨ç¤ºè¨­å®š
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'DejaVu Sans'  # è‹±èªãƒ•ã‚©ãƒ³ãƒˆä½¿ç”¨

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('qwen3_nkat_test.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class Qwen3NKATInference:
    """Qwen3å°‚ç”¨NKATæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        
        logger.info(f"ğŸš€ Qwen3-NKAT Inference Engine")
        logger.info(f"   ğŸ“± Device: {self.device}")
        logger.info(f"   ğŸ® GPU: {self.gpu_name}")
        logger.info(f"   ğŸ“ Model: {Path(model_path).name}")
        
        # CUDAæœ€é©åŒ–è¨­å®š
        if torch.cuda.is_available():
            torch.cuda.set_device(0)
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info(f"   âš¡ CUDA optimization enabled")
    
    def check_model_compatibility(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«äº’æ›æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            if not os.path.exists(self.model_path):
                logger.error(f"âŒ Model file not found: {self.model_path}")
                return False
            
            file_size = os.path.getsize(self.model_path) / (1024**3)
            logger.info(f"ğŸ“Š Model size: {file_size:.2f} GB")
            
            # GGUFå½¢å¼ç¢ºèªï¼ˆç°¡æ˜“ï¼‰
            with open(self.model_path, 'rb') as f:
                header = f.read(8)
                if b"GGUF" in header or b"GGML" in header:
                    logger.info(f"âœ… GGUF format detected")
                    return True
                else:
                    logger.warning(f"âš ï¸ Unknown format, proceeding anyway")
                    return True
                    
        except Exception as e:
            logger.error(f"âŒ Compatibility check failed: {e}")
            return False
    
    def estimate_vram_usage(self) -> dict:
        """VRAMä½¿ç”¨é‡æ¨å®š"""
        if not torch.cuda.is_available():
            return {"status": "CPU mode"}
        
        # RTX3080ã®ã‚¹ãƒšãƒƒã‚¯ (10GB VRAM)
        total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        free_vram = torch.cuda.memory_reserved(0) / (1024**3)
        
        model_size_gb = os.path.getsize(self.model_path) / (1024**3)
        
        # NKATæ¨è«–æ™‚ã®è¿½åŠ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š
        nkat_overhead = model_size_gb * 0.15  # Î¸ãƒ†ãƒ³ã‚½ãƒ« + ä¸­é–“è¨ˆç®—
        total_estimated = model_size_gb + nkat_overhead + 1.0  # ãƒãƒƒãƒ•ã‚¡
        
        estimation = {
            "gpu_model": self.gpu_name,
            "total_vram_gb": total_vram,
            "model_size_gb": model_size_gb,
            "nkat_overhead_gb": nkat_overhead,
            "estimated_usage_gb": total_estimated,
            "compatibility": "OK" if total_estimated < total_vram * 0.9 else "TIGHT"
        }
        
        logger.info(f"ğŸ”§ VRAM Estimation:")
        logger.info(f"   ğŸ“± {estimation['gpu_model']}: {estimation['total_vram_gb']:.1f}GB")
        logger.info(f"   ğŸ“‚ Model: {estimation['model_size_gb']:.2f}GB")
        logger.info(f"   âš™ï¸ NKAT overhead: {estimation['nkat_overhead_gb']:.2f}GB")
        logger.info(f"   ğŸ“Š Total estimated: {estimation['estimated_usage_gb']:.2f}GB")
        logger.info(f"   âœ… Status: {estimation['compatibility']}")
        
        return estimation
    
    def synthetic_inference_test(self, seq_length: int = 512, batch_size: int = 1) -> dict:
        """åˆæˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¨è«–æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info(f"ğŸ§ª Synthetic inference test starting...")
        logger.info(f"   ğŸ“ Sequence length: {seq_length}")
        logger.info(f"   ğŸ“¦ Batch size: {batch_size}")
        
        try:
            # åˆæˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            vocab_size = 32000  # Qwen3æƒ³å®š
            hidden_size = 4096  # 8B modelæƒ³å®š
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ
            input_ids = torch.randint(0, vocab_size, (batch_size, seq_length), device=self.device)
            
            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆåˆæˆï¼‰
            embeddings = torch.randn(batch_size, seq_length, hidden_size, device=self.device, dtype=torch.float16)
            
            # NKATæ¼”ç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            times = []
            throughputs = []
            
            logger.info(f"   â±ï¸ Running {10} iterations...")
            
            for i in tqdm(range(10), desc="NKAT inference"):
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                start_time = time.time()
                
                # ã‚¹ã‚¿ãƒ¼ç©æ¼”ç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                # 1. ç·šå½¢å¤‰æ›
                W = torch.randn(hidden_size, hidden_size, device=self.device, dtype=torch.float16)
                y_linear = torch.matmul(embeddings, W.T)
                
                # 2. NKAT Î¸é …ï¼ˆåå¯¾ç§°è¡Œåˆ—ï¼‰
                theta = torch.randn(hidden_size, hidden_size, device=self.device, dtype=torch.float16)
                theta = theta - theta.T  # åå¯¾ç§°åŒ–
                y_phase = 0.5 * 0.97 * torch.matmul(embeddings, theta.T)
                
                # 3. ã‚¹ã‚¿ãƒ¼ç©çµåˆ
                y_star = y_linear + y_phase
                
                # 4. ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
                output = torch.nn.functional.gelu(y_star)
                
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                end_time = time.time()
                
                iteration_time = end_time - start_time
                tokens_per_sec = (batch_size * seq_length) / iteration_time
                
                times.append(iteration_time)
                throughputs.append(tokens_per_sec)
            
            # çµ±è¨ˆè¨ˆç®—
            avg_time = np.mean(times)
            avg_throughput = np.mean(throughputs)
            std_throughput = np.std(throughputs)
            
            # VRAMä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            if torch.cuda.is_available():
                memory_used = torch.cuda.max_memory_allocated(0) / (1024**3)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            else:
                memory_used = memory_reserved = 0
            
            results = {
                "model": "Qwen3-8B-ERP (synthetic)",
                "seq_length": seq_length,
                "batch_size": batch_size,
                "device": str(self.device),
                "avg_time_ms": avg_time * 1000,
                "avg_throughput_tokens_per_sec": avg_throughput,
                "throughput_std": std_throughput,
                "memory_used_gb": memory_used,
                "memory_reserved_gb": memory_reserved,
                "nkat_enabled": True
            }
            
            logger.info(f"ğŸ“Š Test Results:")
            logger.info(f"   â±ï¸ Average time: {results['avg_time_ms']:.2f}ms")
            logger.info(f"   ğŸš€ Throughput: {results['avg_throughput_tokens_per_sec']:.1f} tokens/sec")
            logger.info(f"   ğŸ“± VRAM used: {results['memory_used_gb']:.2f}GB")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Synthetic test failed: {e}")
            return {"error": str(e)}
    
    def save_results(self, results: dict, filename: str = "qwen3_nkat_test_results.json"):
        """çµæœä¿å­˜"""
        try:
            output_dir = Path("output/qwen3_nkat_testing")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_path = output_dir / filename
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¿½åŠ 
            results["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S")
            results["python_version"] = sys.version
            results["pytorch_version"] = torch.__version__
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Results saved: {output_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ Qwen3-8B-ERP NKAT Inference Test")
    print("=" * 50)
    
    model_path = "models/integrated/Qwen3-8B-ERP-v0.1.i1-Q6_K.gguf"
    
    # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = Qwen3NKATInference(model_path)
    
    # ãƒ¢ãƒ‡ãƒ«äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
    if not engine.check_model_compatibility():
        print("âŒ Model compatibility check failed")
        return
    
    # VRAMæ¨å®š
    vram_estimation = engine.estimate_vram_usage()
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
    test_configs = [
        {"seq_length": 256, "batch_size": 1},
        {"seq_length": 512, "batch_size": 1},
        {"seq_length": 1024, "batch_size": 1},
    ]
    
    all_results = {
        "model_info": {
            "path": model_path,
            "size_gb": os.path.getsize(model_path) / (1024**3)
        },
        "vram_estimation": vram_estimation,
        "test_results": []
    }
    
    for config in test_configs:
        print(f"\nğŸ§ª Testing: {config}")
        result = engine.synthetic_inference_test(**config)
        all_results["test_results"].append(result)
        
        # ä¸­é–“çµæœè¡¨ç¤º
        if "error" not in result:
            print(f"   âœ… {result['avg_throughput_tokens_per_sec']:.1f} tokens/sec")
        else:
            print(f"   âŒ Error: {result['error']}")
    
    # çµæœä¿å­˜
    engine.save_results(all_results)
    
    print("\nğŸ‰ Qwen3-NKAT inference test completed!")
    print(f"ğŸ“ Check output/qwen3_nkat_testing/ for detailed results")

if __name__ == "__main__":
    main() 
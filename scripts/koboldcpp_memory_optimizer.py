#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ KoboldCPP ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ»ã‚¨ãƒ©ãƒ¼è§£æ±ºã‚·ã‚¹ãƒ†ãƒ 
KoboldCPP Memory Optimization & Error Resolution System

å¯¾å¿œã‚¨ãƒ©ãƒ¼:
- bad_alloc error while reading value for key 'tokenizer.ggml.tokens'
- access violation reading 0x0000000000000008
- ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
"""

import os
import sys
import json
import psutil
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass

@dataclass
class KoboldCPPConfig:
    """KoboldCPPæœ€é©åŒ–è¨­å®š"""
    # ãƒ¡ãƒ¢ãƒªè¨­å®š
    max_memory_usage_percent: float = 80.0
    enable_memory_mapping: bool = False  # nommap=Trueæ¨å¥¨
    enable_memory_lock: bool = False     # usemlock=Falseæ¨å¥¨
    
    # BLASè¨­å®š
    blas_batch_size: int = 128           # 512ã‹ã‚‰128ã«å‰Šæ¸›
    blas_threads: int = 4                # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å‰Šæ¸›
    
    # GPUè¨­å®š
    gpu_layers: int = 0                  # GPUå±¤æ•°åˆ¶é™
    context_size: int = 2048             # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºå‰Šæ¸›
    
    # å®‰å…¨æ€§è¨­å®š
    enable_noavx2: bool = True           # AVX2ç„¡åŠ¹åŒ–
    enable_failsafe: bool = True         # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æœ‰åŠ¹
    
    # ãƒãƒ¼ãƒˆè¨­å®š
    port: int = 5001

class MemoryAnalyzer:
    """ãƒ¡ãƒ¢ãƒªåˆ†æå™¨"""
    
    def __init__(self):
        self.total_memory = psutil.virtual_memory().total / 1024**3
        self.available_memory = psutil.virtual_memory().available / 1024**3
        
    def analyze_system_memory(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªåˆ†æ"""
        memory_info = psutil.virtual_memory()
        
        analysis = {
            'total_gb': self.total_memory,
            'available_gb': self.available_memory,
            'used_gb': memory_info.used / 1024**3,
            'usage_percent': memory_info.percent,
            'safe_for_large_models': self.available_memory > 12.0,
            'recommended_context_size': self._recommend_context_size(),
            'recommended_gpu_layers': self._recommend_gpu_layers()
        }
        
        return analysis
    
    def _recommend_context_size(self) -> int:
        """æ¨å¥¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º"""
        if self.available_memory >= 16.0:
            return 4096
        elif self.available_memory >= 8.0:
            return 2048
        elif self.available_memory >= 4.0:
            return 1024
        else:
            return 512
    
    def _recommend_gpu_layers(self) -> int:
        """æ¨å¥¨GPUå±¤æ•°"""
        try:
            import torch
            if torch.cuda.is_available():
                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                if vram_gb >= 8.0:
                    return min(33, int(vram_gb * 3))  # VRAMã®3å€ç¨‹åº¦
                else:
                    return 0
            else:
                return 0
        except ImportError:
            return 0

class KoboldCPPOptimizer:
    """KoboldCPPæœ€é©åŒ–å™¨"""
    
    def __init__(self, config: KoboldCPPConfig):
        self.config = config
        self.memory_analyzer = MemoryAnalyzer()
        
    def optimize_for_model(self, model_path: str) -> KoboldCPPConfig:
        """ãƒ¢ãƒ‡ãƒ«ç”¨æœ€é©åŒ–è¨­å®šç”Ÿæˆ"""
        print(f"ğŸ”§ KoboldCPPæœ€é©åŒ–è¨­å®šç”Ÿæˆ: {Path(model_path).name}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ
        model_size_gb = os.path.getsize(model_path) / 1024**3
        
        # ãƒ¡ãƒ¢ãƒªåˆ†æ
        memory_analysis = self.memory_analyzer.analyze_system_memory()
        
        # æœ€é©åŒ–è¨­å®šç”Ÿæˆ
        optimized_config = self._generate_optimized_config(model_size_gb, memory_analysis)
        
        print(f"  ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {model_size_gb:.1f}GB")
        print(f"  ğŸ’¾ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory_analysis['available_gb']:.1f}GB")
        print(f"  âš™ï¸ æ¨å¥¨è¨­å®š:")
        print(f"    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º: {optimized_config.context_size}")
        print(f"    - GPUå±¤æ•°: {optimized_config.gpu_layers}")
        print(f"    - BLAS ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimized_config.blas_batch_size}")
        
        return optimized_config
    
    def _generate_optimized_config(self, model_size_gb: float, memory_analysis: Dict[str, Any]) -> KoboldCPPConfig:
        """æœ€é©åŒ–è¨­å®šç”Ÿæˆ"""
        config = KoboldCPPConfig()
        
        # ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        available_gb = memory_analysis['available_gb']
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºèª¿æ•´
        if model_size_gb > 8.0:  # å¤§å‹ãƒ¢ãƒ‡ãƒ«
            config.context_size = min(2048, memory_analysis['recommended_context_size'])
            config.blas_batch_size = 64
            config.gpu_layers = 0  # CPUæ¨å¥¨
        elif model_size_gb > 4.0:  # ä¸­å‹ãƒ¢ãƒ‡ãƒ«
            config.context_size = min(4096, memory_analysis['recommended_context_size'])
            config.blas_batch_size = 128
            config.gpu_layers = min(16, memory_analysis['recommended_gpu_layers'])
        else:  # å°å‹ãƒ¢ãƒ‡ãƒ«
            config.context_size = 4096
            config.blas_batch_size = 256
            config.gpu_layers = memory_analysis['recommended_gpu_layers']
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®å®‰å…¨è¨­å®š
        if available_gb < model_size_gb * 2:
            config.enable_memory_mapping = False  # nommap=True
            config.enable_memory_lock = False     # usemlock=False
            config.context_size = min(config.context_size, 1024)
            config.blas_batch_size = min(config.blas_batch_size, 64)
            config.gpu_layers = 0
        
        return config
    
    def generate_launch_command(self, model_path: str, optimized_config: KoboldCPPConfig) -> str:
        """èµ·å‹•ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ"""
        cmd_parts = [
            "python koboldcpp.py",
            f"--model \"{model_path}\"",
            f"--contextsize {optimized_config.context_size}",
            f"--blasbatchsize {optimized_config.blas_batch_size}",
            f"--blasthreads {optimized_config.blas_threads}",
            f"--port {optimized_config.port}",
            "--skiplauncher"
        ]
        
        # GPUè¨­å®š
        if optimized_config.gpu_layers > 0:
            cmd_parts.append(f"--gpulayers {optimized_config.gpu_layers}")
            cmd_parts.append("--usecublas normal 0")
        else:
            cmd_parts.append("--gpulayers 0")
        
        # ãƒ¡ãƒ¢ãƒªè¨­å®š
        if not optimized_config.enable_memory_mapping:
            cmd_parts.append("--nommap")
        
        if not optimized_config.enable_memory_lock:
            cmd_parts.append("--usemlock False")
        
        # å®‰å…¨æ€§è¨­å®š
        if optimized_config.enable_noavx2:
            cmd_parts.append("--noavx2")
        
        if optimized_config.enable_failsafe:
            cmd_parts.append("--failsafe")
        
        return " ".join(cmd_parts)
    
    def create_batch_file(self, model_path: str, output_path: str = None) -> str:
        """æœ€é©åŒ–ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        if not output_path:
            model_name = Path(model_path).stem
            output_path = f"run_{model_name}_optimized.bat"
        
        optimized_config = self.optimize_for_model(model_path)
        launch_command = self.generate_launch_command(model_path, optimized_config)
        
        batch_content = f"""@echo off
REM KoboldCPPæœ€é©åŒ–èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
REM ãƒ¢ãƒ‡ãƒ«: {Path(model_path).name}
REM ç”Ÿæˆæ—¥æ™‚: {Path().cwd()}

echo ğŸš€ KoboldCPPæœ€é©åŒ–èµ·å‹•
echo ãƒ¢ãƒ‡ãƒ«: {Path(model_path).name}
echo.

REM ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
echo ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:
systeminfo | findstr "Total Physical Memory"
echo.

REM KoboldCPPèµ·å‹•
echo ğŸ”§ KoboldCPPèµ·å‹•ä¸­...
{launch_command}

pause
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(batch_content)
        
        print(f"ğŸ“ ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {output_path}")
        return output_path

class ErrorResolver:
    """ã‚¨ãƒ©ãƒ¼è§£æ±ºå™¨"""
    
    @staticmethod
    def resolve_bad_alloc_error(model_path: str) -> List[str]:
        """bad_allocã‚¨ãƒ©ãƒ¼è§£æ±ºç­–"""
        solutions = [
            "ğŸ”§ è§£æ±ºç­– 1: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ç„¡åŠ¹åŒ–",
            "  â†’ --nommap ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ",
            "",
            "ğŸ”§ è§£æ±ºç­– 2: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºå‰Šæ¸›",
            "  â†’ --contextsize 1024 ã¾ãŸã¯ 512",
            "",
            "ğŸ”§ è§£æ±ºç­– 3: GPUå±¤æ•°å‰Šæ¸›",
            "  â†’ --gpulayers 0 (CPUä½¿ç”¨)",
            "",
            "ğŸ”§ è§£æ±ºç­– 4: BLASãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›",
            "  â†’ --blasbatchsize 64 ã¾ãŸã¯ 32",
            "",
            "ğŸ”§ è§£æ±ºç­– 5: AVX2ç„¡åŠ¹åŒ–",
            "  â†’ --noavx2 ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ",
            "",
            "ğŸ”§ è§£æ±ºç­– 6: ãƒ¡ãƒ¢ãƒªãƒ­ãƒƒã‚¯ç„¡åŠ¹åŒ–",
            "  â†’ --usemlock False",
            "",
            "âš ï¸ æœ€çµ‚æ‰‹æ®µ: ç ´æãƒ•ã‚¡ã‚¤ãƒ«ä¿®å¾©",
            "  â†’ NKAT-LoRAè’¸ç•™ã‚·ã‚¹ãƒ†ãƒ ã§ä¿®å¾©"
        ]
        return solutions
    
    @staticmethod
    def resolve_access_violation_error() -> List[str]:
        """ã‚¢ã‚¯ã‚»ã‚¹é•åã‚¨ãƒ©ãƒ¼è§£æ±ºç­–"""
        solutions = [
            "ğŸ”§ è§£æ±ºç­– 1: CLBlastè¨­å®šèª¿æ•´",
            "  â†’ --useclblast 0 0 --gpulayers 0",
            "",
            "ğŸ”§ è§£æ±ºç­– 2: ãƒ¡ãƒ¢ãƒªä¿è­·",
            "  â†’ --failsafe ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ",
            "",
            "ğŸ”§ è§£æ±ºç­– 3: å˜ä¸€GPUä½¿ç”¨",
            "  â†’ è¤‡æ•°GPUç’°å¢ƒã§ã¯1ã¤ã®GPUã®ã¿ä½¿ç”¨",
            "",
            "ğŸ”§ è§£æ±ºç­– 4: ãƒ†ãƒ³ã‚½ãƒ«åˆ†å‰²èª¿æ•´",
            "  â†’ --tensor_split ã§æ˜ç¤ºçš„ã«åˆ†å‰²",
            "",
            "ğŸ”§ è§£æ±ºç­– 5: CPUå°‚ç”¨å®Ÿè¡Œ",
            "  â†’ å…¨ã¦ã®GPUæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–"
        ]
        return solutions

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ”§ KoboldCPP ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ»ã‚¨ãƒ©ãƒ¼è§£æ±ºã‚·ã‚¹ãƒ†ãƒ  v1.0")
    print("=" * 60)
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ³•:")
        print("  python koboldcpp_memory_optimizer.py <model_path> [action]")
        print("")
        print("ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  optimize  - æœ€é©åŒ–è¨­å®šç”Ÿæˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")
        print("  batch     - æœ€é©åŒ–ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
        print("  analyze   - ãƒ¡ãƒ¢ãƒªåˆ†æã®ã¿")
        print("  resolve   - ã‚¨ãƒ©ãƒ¼è§£æ±ºç­–è¡¨ç¤º")
        print("")
        print("ä¾‹:")
        print("  python koboldcpp_memory_optimizer.py model.gguf optimize")
        print("  python koboldcpp_memory_optimizer.py model.gguf batch")
        return
    
    model_path = sys.argv[1]
    action = sys.argv[2] if len(sys.argv) > 2 else "optimize"
    
    if not os.path.exists(model_path):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        return
    
    config = KoboldCPPConfig()
    optimizer = KoboldCPPOptimizer(config)
    
    if action == "analyze":
        # ãƒ¡ãƒ¢ãƒªåˆ†æã®ã¿
        analyzer = MemoryAnalyzer()
        analysis = analyzer.analyze_system_memory()
        
        print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªåˆ†æ:")
        print(f"  ç·ãƒ¡ãƒ¢ãƒª: {analysis['total_gb']:.1f}GB")
        print(f"  åˆ©ç”¨å¯èƒ½: {analysis['available_gb']:.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {analysis['used_gb']:.1f}GB")
        print(f"  ä½¿ç”¨ç‡: {analysis['usage_percent']:.1f}%")
        print(f"  å¤§å‹ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ: {'ã¯ã„' if analysis['safe_for_large_models'] else 'ã„ã„ãˆ'}")
        print(f"  æ¨å¥¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {analysis['recommended_context_size']}")
        print(f"  æ¨å¥¨GPUå±¤æ•°: {analysis['recommended_gpu_layers']}")
        
    elif action == "batch":
        # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        batch_file = optimizer.create_batch_file(model_path)
        print(f"âœ… ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {batch_file}")
        
    elif action == "resolve":
        # ã‚¨ãƒ©ãƒ¼è§£æ±ºç­–è¡¨ç¤º
        print("ğŸ©º KoboldCPPã‚¨ãƒ©ãƒ¼è§£æ±ºç­–:")
        print("")
        print("ğŸ”´ bad_alloc ã‚¨ãƒ©ãƒ¼ (tokenizer.ggml.tokens)")
        solutions = ErrorResolver.resolve_bad_alloc_error(model_path)
        for solution in solutions:
            print(solution)
        
        print("")
        print("ğŸ”´ access violation ã‚¨ãƒ©ãƒ¼")
        solutions = ErrorResolver.resolve_access_violation_error()
        for solution in solutions:
            print(solution)
        
    else:
        # æœ€é©åŒ–è¨­å®šç”Ÿæˆ
        optimized_config = optimizer.optimize_for_model(model_path)
        launch_command = optimizer.generate_launch_command(model_path, optimized_config)
        
        print("")
        print("ğŸš€ æœ€é©åŒ–èµ·å‹•ã‚³ãƒãƒ³ãƒ‰:")
        print(launch_command)
        print("")
        print("ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜...")
        
        # è¨­å®šã‚’JSONã§ä¿å­˜
        config_dict = {
            'model_path': model_path,
            'optimized_config': {
                'max_memory_usage_percent': optimized_config.max_memory_usage_percent,
                'enable_memory_mapping': optimized_config.enable_memory_mapping,
                'enable_memory_lock': optimized_config.enable_memory_lock,
                'blas_batch_size': optimized_config.blas_batch_size,
                'blas_threads': optimized_config.blas_threads,
                'gpu_layers': optimized_config.gpu_layers,
                'context_size': optimized_config.context_size,
                'enable_noavx2': optimized_config.enable_noavx2,
                'enable_failsafe': optimized_config.enable_failsafe,
                'port': optimized_config.port
            },
            'launch_command': launch_command
        }
        
        config_file = f"{Path(model_path).stem}_koboldcpp_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¨­å®šä¿å­˜: {config_file}")

if __name__ == "__main__":
    main() 
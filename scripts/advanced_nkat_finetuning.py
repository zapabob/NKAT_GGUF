#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ€ Advanced NKAT Fine-tuning System
éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹GGUFãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

Based on:
- Kolmogorov-Arnold Networks (KANs) research
- Non-commutative geometry and quantum field theory
- Tensor Train decomposition for efficient computation
- Regularization techniques to prevent overfitting
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any, Callable
from tqdm import tqdm
import json
import struct
from pathlib import Path
import matplotlib.pyplot as plt
from scipy.special import comb
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class RegularizedKANActivation(nn.Module):
    """æ­£å‰‡åŒ–ã•ã‚ŒãŸKANæ´»æ€§åŒ–é–¢æ•°ï¼ˆéå­¦ç¿’é˜²æ­¢ç‰ˆï¼‰"""
    
    def __init__(self, 
                 input_dim: int,
                 grid_size: int = 5,
                 spline_order: int = 3,
                 regularization_strength: float = 0.01,
                 noise_injection: float = 0.01):
        super().__init__()
        self.input_dim = input_dim
        self.grid_size = grid_size
        self.spline_order = spline_order
        self.regularization_strength = regularization_strength
        self.noise_injection = noise_injection
        
        # B-splineåŸºåº•ã®åˆ¶å¾¡ç‚¹
        self.control_points = nn.Parameter(
            torch.randn(input_dim, grid_size + spline_order + 1) * 0.1
        )
        
        # æ­£å‰‡åŒ–é …ç”¨ã®é‡ã¿
        self.regularization_weights = nn.Parameter(
            torch.ones(input_dim) * 0.5
        )
        
        # ãƒã‚¤ã‚ºæ³¨å…¥ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.noise_scale = nn.Parameter(
            torch.tensor(noise_injection)
        )
        
        print(f"ğŸ¯ RegularizedKANActivation: {input_dim}D, grid={grid_size}, order={spline_order}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """é †æ–¹å‘è¨ˆç®—ï¼ˆæ­£å‰‡åŒ–ãƒ»ãƒã‚¤ã‚ºæ³¨å…¥ä»˜ãï¼‰"""
        batch_size = x.size(0)
        
        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆçš„ãƒã‚¤ã‚ºæ³¨å…¥ï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.training:
            noise = torch.randn_like(x) * self.noise_scale
            x = x + noise
        
        # å…¥åŠ›æ­£è¦åŒ–
        x_normalized = torch.tanh(x)  # [-1, 1]ã«æ­£è¦åŒ–
        
        # B-splineè©•ä¾¡
        result = []
        for i in range(self.input_dim):
            # å„æ¬¡å…ƒã«å¯¾ã—ã¦B-splineè¨ˆç®—
            spline_values = self._evaluate_bspline(
                x_normalized[:, i], 
                self.control_points[i]
            )
            
            # æ­£å‰‡åŒ–é©ç”¨
            regularized_values = spline_values * self.regularization_weights[i]
            result.append(regularized_values)
        
        output = torch.stack(result, dim=1)
        
        # å‡ºåŠ›ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
        output = torch.clamp(output, -10.0, 10.0)
        
        return output
    
    def _evaluate_bspline(self, t: torch.Tensor, control_points: torch.Tensor) -> torch.Tensor:
        """B-splineè©•ä¾¡ï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        device = t.device
        grid_size = self.grid_size
        k = self.spline_order
        
        # ãƒãƒƒãƒˆç³»åˆ—ç”Ÿæˆ
        knots = torch.linspace(-1, 1, grid_size + 1, device=device)
        extended_knots = torch.cat([
            torch.full((k,), -1, device=device),
            knots,
            torch.full((k,), 1, device=device)
        ])
        
        # B-splineåŸºåº•é–¢æ•°ã®è©•ä¾¡
        basis_values = self._compute_bspline_basis(t, extended_knots, k)
        
        # ã‚µã‚¤ã‚ºèª¿æ•´
        min_size = min(basis_values.size(-1), control_points.size(0))
        basis_values = basis_values[..., :min_size]
        control_points_adj = control_points[:min_size]
        
        # åˆ¶å¾¡ç‚¹ã¨ã®å†…ç©
        result = torch.sum(basis_values * control_points_adj, dim=-1)
        
        return result
    
    def _compute_bspline_basis(self, t: torch.Tensor, knots: torch.Tensor, k: int) -> torch.Tensor:
        """B-splineåŸºåº•é–¢æ•°è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰"""
        n = len(knots) - k - 1
        if n <= 0:
            # æœ€å°é™ã®åŸºåº•é–¢æ•°ã‚’è¿”ã™
            return torch.ones(len(t), 1, device=t.device)
        
        basis = torch.zeros(len(t), n, device=t.device)
        
        # 0æ¬¡åŸºåº•é–¢æ•°
        for i in range(min(n, len(knots)-1)):
            if i + 1 < len(knots):
                mask = (t >= knots[i]) & (t < knots[i+1])
                basis[mask, i] = 1.0
        
        # é«˜æ¬¡åŸºåº•é–¢æ•°ï¼ˆDe Boorã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
        for degree in range(1, min(k + 1, n)):
            for i in range(n - degree):
                if i + degree < len(knots) and i + degree + 1 < len(knots):
                    denom1 = knots[i + degree] - knots[i]
                    denom2 = knots[i + degree + 1] - knots[i + 1]
                    
                    term1 = 0.0
                    term2 = 0.0
                    
                    if denom1 > 1e-10 and i < basis.size(1):
                        term1 = (t - knots[i]) / denom1 * basis[:, i]
                    
                    if denom2 > 1e-10 and i + 1 < basis.size(1):
                        term2 = (knots[i + degree + 1] - t) / denom2 * basis[:, i + 1]
                    
                    if i < basis.size(1):
                        basis[:, i] = term1 + term2
        
        return basis


class NonCommutativeTensorOperator:
    """éå¯æ›ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—å­ï¼ˆé‡å­å¹¾ä½•å­¦ç†è«–ï¼‰"""
    
    def __init__(self, 
                 algebra_dim: int = 4,
                 coupling_strength: float = 0.05):
        self.algebra_dim = algebra_dim
        self.coupling_strength = coupling_strength
        
        # éå¯æ›ä»£æ•°ç”Ÿæˆå­ï¼ˆSU(2)æ‹¡å¼µï¼‰
        self.generators = self._create_algebra_generators()
        
        print(f"ğŸŒ€ NonCommutativeTensorOperator: {algebra_dim}D algebra, coupling={coupling_strength}")
    
    def to_device(self, device):
        """generatorsã‚’æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•"""
        self.generators = [gen.to(device) for gen in self.generators]
        return self
    
    def _create_algebra_generators(self) -> List[torch.Tensor]:
        """éå¯æ›ä»£æ•°ç”Ÿæˆå­ã®ç”Ÿæˆ"""
        # Pauliè¡Œåˆ—ãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆå­
        sigma_x = torch.tensor([[0., 1.], [1., 0.]], dtype=torch.float32)
        sigma_y = torch.tensor([[0., -1.], [1., 0.]], dtype=torch.float32)  # å®Ÿæ•°ç‰ˆ
        sigma_z = torch.tensor([[1., 0.], [0., -1.]], dtype=torch.float32)
        identity = torch.eye(2, dtype=torch.float32)
        
        generators = [sigma_x, sigma_y, sigma_z, identity]
        
        # é«˜æ¬¡å…ƒã¸ã®æ‹¡å¼µ
        if self.algebra_dim > 4:
            for i in range(4, self.algebra_dim):
                # ãƒ©ãƒ³ãƒ€ãƒ å¯¾ç§°è¡Œåˆ—ç”Ÿæˆå­
                gen = torch.randn(2, 2, dtype=torch.float32)
                gen = (gen + gen.T) / 2  # å¯¾ç§°åŒ–
                gen = gen / torch.norm(gen)  # æ­£è¦åŒ–
                generators.append(gen)
        
        return generators[:self.algebra_dim]
    
    def apply_noncommutative_transform(self, tensor: torch.Tensor) -> torch.Tensor:
        """éå¯æ›å¤‰æ›ã®é©ç”¨"""
        if tensor.numel() < 4:
            return tensor
        
        device = tensor.device
        
        # generatorsã‚’åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        if not self.generators[0].device == device:
            self.generators = [gen.to(device) for gen in self.generators]
        
        original_shape = tensor.shape
        flat_tensor = tensor.flatten()
        
        # 2x2ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†
        transformed = torch.zeros_like(flat_tensor)
        
        for i in range(0, len(flat_tensor) - 1, 2):
            # 2è¦ç´ ãƒ™ã‚¯ãƒˆãƒ«
            vec = flat_tensor[i:i+2]
            
            # ç”Ÿæˆå­é¸æŠ
            gen_idx = (i // 2) % len(self.generators)
            generator = self.generators[gen_idx]
            
            # éå¯æ›å¤‰æ›: v' = v + Îµ[G, v]
            if len(vec) == 2:
                # äº¤æ›å­è¨ˆç®—: [G, v] = Gv - vG
                gv = torch.mv(generator, vec)
                
                # 2è¦ç´ ãƒ™ã‚¯ãƒˆãƒ«ã®"å³ä¹—ç®—"ã®ä»£æ›¿å®Ÿè£…
                vg_approx = vec * generator.diagonal()  # å¯¾è§’æˆåˆ†ã«ã‚ˆã‚‹è¿‘ä¼¼
                
                commutator = gv - vg_approx
                
                # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                commutator = torch.clamp(commutator, -1.0, 1.0)
                
                transformed_vec = vec + self.coupling_strength * commutator
                transformed[i:i+2] = transformed_vec
            else:
                transformed[i:i+len(vec)] = vec
        
        # ä½™ã‚Šã®å‡¦ç†
        if len(flat_tensor) % 2 == 1:
            transformed[-1] = flat_tensor[-1]
        
        return transformed.reshape(original_shape)


class QuantumGeometricRegularizer:
    """é‡å­å¹¾ä½•å­¦çš„æ­£å‰‡åŒ–å™¨ï¼ˆéå­¦ç¿’é˜²æ­¢ç‰¹åŒ–ï¼‰"""
    
    def __init__(self, 
                 curvature_penalty: float = 0.001,
                 spectral_penalty: float = 0.01):
        self.curvature_penalty = curvature_penalty
        self.spectral_penalty = spectral_penalty
        
        print(f"ğŸŒŠ QuantumGeometricRegularizer: curvature={curvature_penalty}, spectral={spectral_penalty}")
    
    def compute_curvature_penalty(self, activations: torch.Tensor) -> torch.Tensor:
        """æ›²ç‡ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ï¼ˆRicciæ›²ç‡è¿‘ä¼¼ï¼‰"""
        if activations.dim() < 2:
            return torch.tensor(0.0, device=activations.device)
        
        # 2æ¬¡å¾®åˆ†ã«ã‚ˆã‚‹æ›²ç‡è¿‘ä¼¼
        diff1 = torch.diff(activations, dim=1)
        if diff1.size(1) > 1:
            diff2 = torch.diff(diff1, dim=1)
            curvature = torch.mean(diff2.pow(2))
        else:
            curvature = torch.tensor(0.0, device=activations.device)
        
        return self.curvature_penalty * curvature
    
    def compute_spectral_penalty(self, weight_matrix: torch.Tensor) -> torch.Tensor:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—ï¼ˆç‰¹ç•°å€¤åˆ¶ç´„ï¼‰"""
        if weight_matrix.dim() != 2:
            return torch.tensor(0.0, device=weight_matrix.device)
        
        # SVDè¨ˆç®—
        try:
            U, S, V = torch.svd(weight_matrix)
            
            # æœ€å¤§ç‰¹ç•°å€¤åˆ¶ç´„
            max_singular_value = torch.max(S)
            spectral_penalty = F.relu(max_singular_value - 1.0).pow(2)
            
            # ç‰¹ç•°å€¤ã®åˆ†æ•£ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé‡è¦åº¦ã®å‡ç­‰åŒ–ï¼‰
            variance_penalty = torch.var(S)
            
            total_penalty = spectral_penalty + 0.1 * variance_penalty
            
            return self.spectral_penalty * total_penalty
            
        except RuntimeError:
            # SVDå¤±æ•—æ™‚ã®å›é¿ç­–
            return torch.tensor(0.0, device=weight_matrix.device)


class AdvancedNKATFinetuner(nn.Module):
    """é«˜åº¦ãªNKATãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 input_dim: int = 256,
                 hidden_dims: List[int] = [512, 256, 128],
                 kan_grid_size: int = 5,
                 kan_spline_order: int = 3,
                 noncommutative_strength: float = 0.05,
                 regularization_strength: float = 0.01):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        
        # KANå±¤ã®æ§‹ç¯‰
        self.kan_layers = nn.ModuleList()
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            # å¾“æ¥ã®ç·šå½¢å±¤
            linear = nn.Linear(prev_dim, hidden_dim)
            
            # KANæ´»æ€§åŒ–é–¢æ•°
            kan_activation = RegularizedKANActivation(
                input_dim=hidden_dim,
                grid_size=kan_grid_size,
                spline_order=kan_spline_order,
                regularization_strength=regularization_strength
            )
            
            self.kan_layers.append(nn.ModuleDict({
                'linear': linear,
                'kan_activation': kan_activation,
                'dropout': nn.Dropout(0.1)
            }))
            
            prev_dim = hidden_dim
        
        # å‡ºåŠ›å±¤
        self.output_layer = nn.Linear(prev_dim, input_dim)
        
        # éå¯æ›æ¼”ç®—å­
        self.noncommutative_op = NonCommutativeTensorOperator(
            coupling_strength=noncommutative_strength
        )
        
        # é‡å­å¹¾ä½•å­¦çš„æ­£å‰‡åŒ–å™¨
        self.quantum_regularizer = QuantumGeometricRegularizer()
        
        # é©å¿œçš„å­¦ç¿’ç‡
        self.adaptive_lr_factor = nn.Parameter(torch.tensor(1.0))
        
        print(f"ğŸš€ AdvancedNKATFinetuner initialized")
        print(f"   Architecture: {input_dim} -> {' -> '.join(map(str, hidden_dims))} -> {input_dim}")
        print(f"   KAN grid: {kan_grid_size}, order: {kan_spline_order}")
        print(f"   Non-commutative strength: {noncommutative_strength}")
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """é †æ–¹å‘è¨ˆç®—ï¼ˆå®Œå…¨ãªå¤‰æ›ï¼‰"""
        batch_size = x.size(0)
        
        # å…¥åŠ›æ­£è¦åŒ–
        x_normalized = F.layer_norm(x, x.shape[1:])
        
        activations = []
        current = x_normalized
        
        # KANå±¤ã®é †æ¬¡é©ç”¨
        for i, layer_dict in enumerate(self.kan_layers):
            # ç·šå½¢å¤‰æ›
            linear_output = layer_dict['linear'](current)
            
            # KANæ´»æ€§åŒ–
            kan_output = layer_dict['kan_activation'](linear_output)
            
            # éå¯æ›å¤‰æ›é©ç”¨
            noncommutative_output = self.noncommutative_op.apply_noncommutative_transform(kan_output)
            
            # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
            current = layer_dict['dropout'](noncommutative_output)
            
            activations.append(current)
            
            # æ®‹å·®æ¥ç¶šï¼ˆã‚µã‚¤ã‚ºãŒåˆã†å ´åˆï¼‰
            if current.shape == x_normalized.shape:
                current = current + 0.1 * x_normalized
        
        # å‡ºåŠ›å±¤
        output = self.output_layer(current)
        
        # æœ€çµ‚çš„ãªéå¯æ›å¤‰æ›
        final_output = self.noncommutative_op.apply_noncommutative_transform(output)
        
        return {
            'reconstructed': final_output,
            'activations': activations,
            'regularization_terms': self._compute_regularization_terms(activations)
        }
    
    def _compute_regularization_terms(self, activations: List[torch.Tensor]) -> Dict[str, torch.Tensor]:
        """æ­£å‰‡åŒ–é …ã®è¨ˆç®—"""
        total_curvature_penalty = torch.tensor(0.0, device=activations[0].device)
        total_spectral_penalty = torch.tensor(0.0, device=activations[0].device)
        
        # å„å±¤ã®æ­£å‰‡åŒ–é …
        for activation in activations:
            curvature_penalty = self.quantum_regularizer.compute_curvature_penalty(activation)
            total_curvature_penalty += curvature_penalty
        
        # é‡ã¿è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£å‰‡åŒ–
        for layer_dict in self.kan_layers:
            weight_matrix = layer_dict['linear'].weight
            spectral_penalty = self.quantum_regularizer.compute_spectral_penalty(weight_matrix)
            total_spectral_penalty += spectral_penalty
        
        return {
            'curvature_penalty': total_curvature_penalty,
            'spectral_penalty': total_spectral_penalty,
            'total_penalty': total_curvature_penalty + total_spectral_penalty
        }


class NKATTrainingSystem:
    """NKATãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆéå­¦ç¿’é˜²æ­¢æ©Ÿèƒ½ä»˜ãï¼‰"""
    
    def __init__(self, 
                 model: AdvancedNKATFinetuner,
                 learning_rate: float = 0.001,
                 weight_decay: float = 0.0001):
        self.model = model
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        
        # æœ€é©åŒ–å™¨ï¼ˆAdamW - é‡ã¿æ¸›è¡°ä»˜ãï¼‰
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=10,
            verbose=True
        )
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_stats = {
            'epoch_losses': [],
            'regularization_losses': [],
            'reconstruction_losses': [],
            'validation_losses': [],
            'overfitting_scores': []
        }
        
        print(f"ğŸ“ NKATTrainingSystem initialized")
        print(f"   Learning rate: {learning_rate}")
        print(f"   Weight decay: {weight_decay}")
    
    def train_epoch(self, dataloader: torch.utils.data.DataLoader) -> Dict[str, float]:
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        
        epoch_loss = 0.0
        epoch_reconstruction_loss = 0.0
        epoch_regularization_loss = 0.0
        num_batches = 0
        
        # ãƒ‡ãƒã‚¤ã‚¹å–å¾—
        device = next(self.model.parameters()).device
        
        progress_bar = tqdm(dataloader, desc="Training", leave=False)
        
        for batch_idx, batch in enumerate(progress_bar):
            self.optimizer.zero_grad()
            
            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            if isinstance(batch, (list, tuple)):
                x = batch[0].float().to(device)  # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            else:
                x = batch.float().to(device)  # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            
            # ãƒã‚¤ã‚ºæ³¨å…¥ï¼ˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼‰
            if self.model.training:
                noise = torch.randn_like(x) * 0.01
                x = x + noise
            
            # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
            results = self.model(x)
            
            # æå¤±è¨ˆç®—
            reconstruction_loss = F.mse_loss(results['reconstructed'], x)
            regularization_loss = results['regularization_terms']['total_penalty']
            
            # ç·æå¤±
            total_loss = reconstruction_loss + regularization_loss
            
            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            total_loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # çµ±è¨ˆæ›´æ–°
            epoch_loss += total_loss.item()
            epoch_reconstruction_loss += reconstruction_loss.item()
            epoch_regularization_loss += regularization_loss.item()
            num_batches += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            progress_bar.set_postfix({
                'Loss': f"{total_loss.item():.6f}",
                'Recon': f"{reconstruction_loss.item():.6f}",
                'Reg': f"{regularization_loss.item():.6f}"
            })
        
        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆ
        avg_loss = epoch_loss / num_batches
        avg_reconstruction_loss = epoch_reconstruction_loss / num_batches
        avg_regularization_loss = epoch_regularization_loss / num_batches
        
        # çµ±è¨ˆè¨˜éŒ²
        self.training_stats['epoch_losses'].append(avg_loss)
        self.training_stats['reconstruction_losses'].append(avg_reconstruction_loss)
        self.training_stats['regularization_losses'].append(avg_regularization_loss)
        
        return {
            'total_loss': avg_loss,
            'reconstruction_loss': avg_reconstruction_loss,
            'regularization_loss': avg_regularization_loss
        }
    
    def validate(self, dataloader: torch.utils.data.DataLoader) -> Dict[str, float]:
        """æ¤œè¨¼"""
        self.model.eval()
        
        total_loss = 0.0
        total_reconstruction_loss = 0.0
        num_batches = 0
        
        # ãƒ‡ãƒã‚¤ã‚¹å–å¾—
        device = next(self.model.parameters()).device
        
        with torch.no_grad():
            for batch in dataloader:
                if isinstance(batch, (list, tuple)):
                    x = batch[0].float().to(device)  # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                else:
                    x = batch.float().to(device)  # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                
                results = self.model(x)
                
                reconstruction_loss = F.mse_loss(results['reconstructed'], x)
                regularization_loss = results['regularization_terms']['total_penalty']
                
                total_loss += (reconstruction_loss + regularization_loss).item()
                total_reconstruction_loss += reconstruction_loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches
        avg_reconstruction_loss = total_reconstruction_loss / num_batches
        
        # éå­¦ç¿’ã‚¹ã‚³ã‚¢è¨ˆç®—
        if len(self.training_stats['epoch_losses']) > 0:
            last_train_loss = self.training_stats['epoch_losses'][-1]
            overfitting_score = avg_loss / last_train_loss if last_train_loss > 0 else 1.0
            self.training_stats['overfitting_scores'].append(overfitting_score)
        
        self.training_stats['validation_losses'].append(avg_loss)
        
        # å­¦ç¿’ç‡èª¿æ•´
        self.scheduler.step(avg_loss)
        
        return {
            'validation_loss': avg_loss,
            'validation_reconstruction_loss': avg_reconstruction_loss
        }
    
    def detect_overfitting(self, patience: int = 5) -> bool:
        """éå­¦ç¿’æ¤œå‡º"""
        if len(self.training_stats['overfitting_scores']) < patience:
            return False
        
        recent_scores = self.training_stats['overfitting_scores'][-patience:]
        return all(score > 1.1 for score in recent_scores)  # æ¤œè¨¼æå¤±ãŒè¨“ç·´æå¤±ã®110%ã‚’è¶…ãˆã‚‹
    
    def plot_training_progress(self, save_path: str = None):
        """è¨“ç·´é€²æ—ã®å¯è¦–åŒ–"""
        plt.figure(figsize=(15, 10))
        
        # æå¤±æ›²ç·š
        plt.subplot(2, 3, 1)
        plt.plot(self.training_stats['epoch_losses'], label='Training Loss', color='blue')
        plt.plot(self.training_stats['validation_losses'], label='Validation Loss', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True)
        
        # å†æ§‹æˆæå¤±
        plt.subplot(2, 3, 2)
        plt.plot(self.training_stats['reconstruction_losses'], label='Reconstruction Loss', color='green')
        plt.xlabel('Epoch')
        plt.ylabel('Reconstruction Loss')
        plt.title('Reconstruction Quality')
        plt.legend()
        plt.grid(True)
        
        # æ­£å‰‡åŒ–æå¤±
        plt.subplot(2, 3, 3)
        plt.plot(self.training_stats['regularization_losses'], label='Regularization Loss', color='orange')
        plt.xlabel('Epoch')
        plt.ylabel('Regularization Loss')
        plt.title('Regularization Terms')
        plt.legend()
        plt.grid(True)
        
        # éå­¦ç¿’ã‚¹ã‚³ã‚¢
        if self.training_stats['overfitting_scores']:
            plt.subplot(2, 3, 4)
            plt.plot(self.training_stats['overfitting_scores'], label='Overfitting Score', color='purple')
            plt.axhline(y=1.1, color='red', linestyle='--', label='Overfitting Threshold')
            plt.xlabel('Epoch')
            plt.ylabel('Validation/Training Loss Ratio')
            plt.title('Overfitting Detection')
            plt.legend()
            plt.grid(True)
        
        # å­¦ç¿’ç‡
        plt.subplot(2, 3, 5)
        current_lr = self.optimizer.param_groups[0]['lr']
        lr_history = [current_lr] * len(self.training_stats['epoch_losses'])  # ç°¡å˜åŒ–
        plt.plot(lr_history, label='Learning Rate', color='brown')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.title('Learning Rate Schedule')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Training progress saved to: {save_path}")
        
        plt.show()


class GGUFTensorDataGenerator:
    """GGUFãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    
    def __init__(self, data_size: int = 1000, tensor_dim: int = 256):
        self.data_size = data_size
        self.tensor_dim = tensor_dim
        
        print(f"ğŸ”§ GGUFTensorDataGenerator: {data_size} samples, {tensor_dim}D")
    
    def generate_synthetic_tensor_data(self) -> torch.utils.data.DataLoader:
        """åˆæˆãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        # æ§‹é€ åŒ–ã•ã‚ŒãŸåˆæˆãƒ‡ãƒ¼ã‚¿
        data = []
        
        for i in range(self.data_size):
            # å‘¨æœŸçš„æ§‹é€  + ãƒã‚¤ã‚º
            t = np.linspace(0, 4*np.pi, self.tensor_dim)
            
            # è¤‡æ•°ã®å‘¨æ³¢æ•°æˆåˆ†
            signal = (np.sin(t) + 0.5*np.sin(3*t) + 0.3*np.sin(5*t) + 
                     0.1*np.random.randn(self.tensor_dim))
            
            # éç·šå½¢å¤‰æ›
            signal = np.tanh(signal) + 0.1*signal**2
            
            # æ­£è¦åŒ–
            signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)
            
            data.append(signal.astype(np.float32))
        
        # PyTorchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        dataset = torch.utils.data.TensorDataset(torch.from_numpy(np.array(data)))
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
        
        print(f"âœ… Generated {len(data)} synthetic tensor samples")
        return dataloader


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒ€ Advanced NKAT Fine-tuning System")
    print("=" * 80)
    print("ğŸ“š Non-Commutative Kolmogorov-Arnold Representation Theory")
    print("ğŸ¯ GGUF Tensor Computation with Regularized KAN + Quantum Geometry")
    print("ğŸ›¡ï¸ Overfitting Prevention & Robust Training")
    print("=" * 80)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = AdvancedNKATFinetuner(
        input_dim=256,
        hidden_dims=[512, 256, 128],
        kan_grid_size=5,
        kan_spline_order=3,
        noncommutative_strength=0.05,
        regularization_strength=0.01
    ).to(device)
    
    # éå¯æ›æ¼”ç®—å­ã‚‚ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
    model.noncommutative_op.to_device(device)
    
    # è¨“ç·´ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    training_system = NKATTrainingSystem(
        model=model,
        learning_rate=0.001,
        weight_decay=0.0001
    )
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    data_generator = GGUFTensorDataGenerator(data_size=1000, tensor_dim=256)
    train_dataloader = data_generator.generate_synthetic_tensor_data()
    val_dataloader = data_generator.generate_synthetic_tensor_data()
    
    print(f"\nğŸš€ Starting NKAT Fine-tuning Training...")
    print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    num_epochs = 50
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 10
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“… Epoch {epoch+1}/{num_epochs}")
        
        # è¨“ç·´
        train_metrics = training_system.train_epoch(train_dataloader)
        
        # æ¤œè¨¼
        val_metrics = training_system.validate(val_dataloader)
        
        # é€²æ—è¡¨ç¤º
        print(f"   Train Loss: {train_metrics['total_loss']:.6f}")
        print(f"   Val Loss: {val_metrics['validation_loss']:.6f}")
        print(f"   Reconstruction: {train_metrics['reconstruction_loss']:.6f}")
        print(f"   Regularization: {train_metrics['regularization_loss']:.6f}")
        
        # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯
        if val_metrics['validation_loss'] < best_val_loss:
            best_val_loss = val_metrics['validation_loss']
            patience_counter = 0
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            torch.save(model.state_dict(), 'output/best_nkat_model.pth')
            print(f"   ğŸ’¾ Best model saved (Val Loss: {best_val_loss:.6f})")
        else:
            patience_counter += 1
        
        # éå­¦ç¿’æ¤œå‡º
        if training_system.detect_overfitting():
            print(f"   ğŸš¨ Overfitting detected! Stopping training.")
            break
        
        # æ—©æœŸåœæ­¢
        if patience_counter >= max_patience:
            print(f"   â° Early stopping triggered (patience: {max_patience})")
            break
    
    print(f"\nğŸ‰ NKAT Fine-tuning Training Completed!")
    print(f"   Best Validation Loss: {best_val_loss:.6f}")
    print(f"   Total Epochs: {epoch+1}")
    
    # è¨“ç·´é€²æ—å¯è¦–åŒ–
    os.makedirs('output', exist_ok=True)
    training_system.plot_training_progress('output/nkat_training_progress.png')
    
    # æœ€çµ‚è©•ä¾¡
    print(f"\nğŸ“Š Final Model Evaluation:")
    model.eval()
    with torch.no_grad():
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ãƒ³ã‚½ãƒ«ã§ã®è©•ä¾¡
        sample_data = next(iter(val_dataloader))[0][:1].to(device)  # 1ã‚µãƒ³ãƒ—ãƒ«
        results = model(sample_data)
        
        reconstruction_error = F.mse_loss(results['reconstructed'], sample_data)
        print(f"   Reconstruction Error: {reconstruction_error.item():.6f}")
        
        # éå¯æ›æ€§ã®æ¸¬å®š
        original_norm = torch.norm(sample_data)
        reconstructed_norm = torch.norm(results['reconstructed'])
        print(f"   Norm Preservation: {reconstructed_norm/original_norm:.4f}")
    
    print(f"\nâœ… Advanced NKAT Fine-tuning System successfully demonstrated!")
    print(f"ğŸ¯ Key Features Implemented:")
    print(f"   âœ“ Regularized Kolmogorov-Arnold Networks (Anti-overfitting)")
    print(f"   âœ“ Non-commutative Tensor Operations (Quantum Geometry)")
    print(f"   âœ“ Quantum Geometric Regularization (Curvature + Spectral)")
    print(f"   âœ“ Adaptive Learning Rate & Early Stopping")
    print(f"   âœ“ Comprehensive Training Monitoring")
    
    return model, training_system


if __name__ == "__main__":
    # RTX 3080æœ€é©åŒ–è¨­å®š
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print(f"ğŸš€ CUDA optimization enabled for RTX 3080")
    
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
    model, training_system = main() 
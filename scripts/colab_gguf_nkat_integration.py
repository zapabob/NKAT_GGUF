#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ GGUF + NKAT Integration for Google Colab (æ”¹è‰¯ç‰ˆ+64bitçµ±åˆãƒ†ã‚¹ãƒˆ)
Google Colabå°‚ç”¨ GGUF+NKATçµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ64bitç²¾åº¦å¼·åŒ–ç‰ˆï¼‰

ç‰¹å¾´:
- 64bitç²¾åº¦å¯¾å¿œå¼·åŒ–
- å®Ÿç”¨çš„çµ±åˆãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- GUIç„¡ã—ï¼ˆColab Web UIä½¿ç”¨ï¼‰ / TkinterGUIï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
- Google Driveé€£æº
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- tqdmé€²æ—è¡¨ç¤º
- æ—¥æœ¬èªå¯¾å¿œ
- å…¨æ©Ÿèƒ½ã‚’1ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆ
- ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯¾å¿œ
- JSONè¨­å®šè‡ªå‹•åŒ–
- RTX3080 CUDAæœ€é©åŒ–
"""

import os
import sys
import time
import json
import gc
import struct
from pathlib import Path
import numpy as np
import shutil
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import threading
import traceback
import argparse

# Tkinter GUIé–¢é€£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import tkinter as tk
    from tkinter import filedialog, messagebox, ttk
    from tkinter import scrolledtext
    TKINTER_AVAILABLE = True
except ImportError:
    TKINTER_AVAILABLE = False

# ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯¾å¿œ
try:
    from tkinterdnd2 import DND_FILES, TkinterDnD
    DND_AVAILABLE = True
except ImportError:
    DND_AVAILABLE = False

# Google Colabå°‚ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from google.colab import drive, files
    import IPython.display as display
    COLAB_ENV = True
    print("âœ… Google Colabç’°å¢ƒã‚’æ¤œå‡º")
except ImportError:
    COLAB_ENV = False
    print("âš ï¸ Google Colabç’°å¢ƒã§ã¯ã‚ã‚Šã¾ã›ã‚“")

# PyTorchã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

@dataclass
class NKATConfig:
    """NKATç†è«–è¨­å®šï¼ˆ64bitå¯¾å¿œï¼‰"""
    enable_ka_operators: bool = True
    ka_grid_size: int = 8  # è»½é‡åŒ–ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
    lie_algebra_dim: int = 4  # ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒ
    noncommutative_strength: float = 0.1
    differential_geometric_scale: float = 0.01
    spectral_radius_bound: float = 1.0
    quantization_aware: bool = True
    # 64bitç²¾åº¦å¯¾å¿œè¨­å®š
    use_64bit_precision: bool = True
    data_alignment: int = 8  # 64bitå¢ƒç•Œæ•´åˆ—
    enable_performance_monitoring: bool = True
    enable_cuda_optimization: bool = True

    def to_dict(self):
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'enable_ka_operators': self.enable_ka_operators,
            'ka_grid_size': self.ka_grid_size,
            'lie_algebra_dim': self.lie_algebra_dim,
            'noncommutative_strength': self.noncommutative_strength,
            'differential_geometric_scale': self.differential_geometric_scale,
            'spectral_radius_bound': self.spectral_radius_bound,
            'quantization_aware': self.quantization_aware,
            'use_64bit_precision': self.use_64bit_precision,
            'data_alignment': self.data_alignment,
            'enable_performance_monitoring': self.enable_performance_monitoring,
            'enable_cuda_optimization': self.enable_cuda_optimization
        }
    
    @classmethod
    def from_dict(cls, data):
        """è¾æ›¸ã‹ã‚‰ç”Ÿæˆ"""
        return cls(**data)

class GGUFNKATIntegrator:
    """GGUF + NKATçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆ64bitç²¾åº¦å¼·åŒ–ç‰ˆï¼‰"""
    
    GGUF_MAGIC = b'GGUF'
    
    def __init__(self, config: Optional[NKATConfig] = None):
        self.config = config or NKATConfig()
        self.nkat_metadata = {}
        self.tensor_transformations = []  # å¤‰æ›å±¥æ­´
        self.performance_stats = {
            "files_processed": 0,
            "total_input_size": 0,
            "total_output_size": 0,
            "total_processing_time": 0,
            "precision_improvements": 0,
            "errors": 0
        }
        self._prepare_nkat_metadata()
        print(f"ğŸ§  NKATçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆ64bitç²¾åº¦: {self.config.use_64bit_precision}ï¼‰")
    
    def _prepare_nkat_metadata(self):
        """NKATç†è«–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆ64bitå¼·åŒ–ç‰ˆï¼‰"""
        self.nkat_metadata = {
            # NKATåŸºæœ¬è¨­å®š
            "nkat.version": "1.0_64bit_enhanced",
            "nkat.enable": True,
            "nkat.architecture": "quantized_aware_nkat_64bit",
            
            # Kolmogorov-Arnoldè¨­å®š
            "nkat.ka.enable": self.config.enable_ka_operators,
            "nkat.ka.grid_size": self.config.ka_grid_size,
            "nkat.ka.activation_type": "learnable_spline",
            "nkat.ka.quantization_bits": 8,  # KAæ¼”ç®—å­ã®é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•°
            
            # éå¯æ›ä»£æ•°è¨­å®š
            "nkat.lie_algebra.dimension": self.config.lie_algebra_dim,
            "nkat.lie_algebra.structure_constants": self._compute_structure_constants_64bit(),
            "nkat.noncommutative.strength": self.config.noncommutative_strength,
            
            # å¾®åˆ†å¹¾ä½•å­¦è¨­å®š
            "nkat.differential_geometry.enable": True,
            "nkat.differential_geometry.manifold_dim": 2,
            "nkat.differential_geometry.scale": self.config.differential_geometric_scale,
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«ç†è«–è¨­å®š
            "nkat.spectral.radius_bound": self.config.spectral_radius_bound,
            "nkat.spectral.eigenvalue_regularization": 0.001,
            
            # é‡å­åŒ–å¯¾å¿œè¨­å®š
            "nkat.quantization.aware": self.config.quantization_aware,
            "nkat.quantization.precision_preservation": True,
            "nkat.quantization.dynamic_scaling": True,
            
            # 64bitç²¾åº¦è¨­å®š
            "nkat.precision.mode": "64bit" if self.config.use_64bit_precision else "mixed",
            "nkat.precision.data_alignment": self.config.data_alignment,
            "nkat.precision.memory_optimization": True,
            "nkat.precision.cuda_compatibility": self.config.enable_cuda_optimization,
            
            # æ¨è«–ã¸ã®å½±éŸ¿ã«é–¢ã™ã‚‹è¨­å®š
            "nkat.inference.expected_speedup": self._estimate_speedup(),
            "nkat.inference.memory_efficiency": self._estimate_memory_efficiency(),
            "nkat.inference.accuracy_improvement": self._estimate_accuracy_improvement(),
            "nkat.inference.compatibility_mode": "nkat_native_64bit",  # ãƒã‚¤ãƒ†ã‚£ãƒ–NKAT 64bitãƒ¢ãƒ¼ãƒ‰
            
            # å®Ÿè£…ãƒ¬ãƒ™ãƒ«ï¼ˆæ›´æ–°ï¼‰
            "nkat.implementation.level": "tensor_transform_64bit",  # 64bitãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ãƒ¬ãƒ™ãƒ«
            "nkat.implementation.tensor_transform": True,  # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›å®Ÿè£…æ¸ˆã¿
            "nkat.implementation.requires_nkat_engine": False,  # å¾“æ¥ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚‚å‹•ä½œ
            "nkat.implementation.backward_compatible": True,  # å¾Œæ–¹äº’æ›æ€§ã‚ã‚Š
            "nkat.implementation.rtx3080_optimized": self.config.enable_cuda_optimization,
        }
    
    def _compute_structure_constants_64bit(self) -> List[float]:
        """ãƒªãƒ¼ä»£æ•°ã®æ§‹é€ å®šæ•°ã‚’è¨ˆç®—ï¼ˆ64bitç²¾åº¦ç‰ˆï¼‰"""
        dim = self.config.lie_algebra_dim
        constants = []
        
        # 64bitç²¾åº¦ã§ã®su(2)å‹æ§‹é€ å®šæ•°è¨ˆç®—
        for i in range(dim):
            for j in range(dim):
                for k in range(dim):
                    if i < j < k:
                        # 64bitç²¾åº¦ã§ã®è¨ˆç®—
                        value = np.float64(1.0 if (i+j+k) % 2 == 0 else -1.0)
                        # éå¯æ›æ€§ã«ã‚ˆã‚‹è£œæ­£
                        value *= np.float64(self.config.noncommutative_strength)
                        constants.append(float(value))
                    else:
                        constants.append(0.0)
        return constants[:32]  # 64bitç’°å¢ƒã§ã¯32è¦ç´ ã¾ã§æ‹¡å¼µ
    
    def _estimate_speedup(self) -> float:
        """NKATç†è«–ã«ã‚ˆã‚‹æ¨è«–é€Ÿåº¦å‘ä¸Šã®æ¨å®šï¼ˆ64bitè€ƒæ…®ï¼‰"""
        # KAãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã«ã‚ˆã‚‹é€Ÿåº¦å‘ä¸Š
        ka_efficiency = 1.0 + (0.15 * self.config.ka_grid_size / 8)  # 64bitç’°å¢ƒã§ã•ã‚‰ã«åŠ¹ç‡åŒ–
        
        # éå¯æ›ä»£æ•°ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–å¯èƒ½æ€§
        noncommutative_parallel = 1.0 + (self.config.noncommutative_strength * 0.7)  # 64bitä¸¦åˆ—åŒ–æœ€é©åŒ–
        
        # CUDAæœ€é©åŒ–ãƒœãƒ¼ãƒŠã‚¹
        cuda_bonus = 1.2 if self.config.enable_cuda_optimization else 1.0
        
        # ç·åˆçš„ãªé€Ÿåº¦å‘ä¸Šï¼ˆç†è«–å€¤ï¼‰
        total_speedup = ka_efficiency * noncommutative_parallel * cuda_bonus
        
        return min(total_speedup, 4.0)  # 64bitç’°å¢ƒã§ã¯æœ€å¤§4å€ã®é€Ÿåº¦å‘ä¸Š
    
    def _estimate_memory_efficiency(self) -> float:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ¨å®šï¼ˆ64bitå¯¾å¿œï¼‰"""
        # KAãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›åŠ¹æœ
        param_reduction = 0.65 - (self.config.ka_grid_size / 64)  # 64bitç’°å¢ƒã§ã‚ˆã‚ŠåŠ¹ç‡çš„
        
        # é‡å­åŒ–å¯¾å¿œã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
        if self.config.quantization_aware:
            param_reduction *= 0.75  # 64bitç’°å¢ƒã§25%å‰Šæ¸›
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆ—ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
        alignment_bonus = 0.85 if self.config.data_alignment == 8 else 1.0
        
        return max(param_reduction * alignment_bonus, 0.4)  # æœ€ä½40%ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
    
    def _estimate_accuracy_improvement(self) -> float:
        """ç²¾åº¦å‘ä¸Šã®æ¨å®šï¼ˆ64bitå¼·åŒ–ï¼‰"""
        # éå¯æ›ä»£æ•°ã«ã‚ˆã‚‹è¡¨ç¾åŠ›å‘ä¸Š
        representation_boost = self.config.noncommutative_strength * 15  # 64bitç²¾åº¦ã§1.5å€åŠ¹æœ
        
        # å¾®åˆ†å¹¾ä½•å­¦çš„æœ€é©åŒ–ã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š
        stability_boost = self.config.differential_geometric_scale * 150  # 64bitç²¾åº¦ã§å¤§å¹…å‘ä¸Š
        
        # ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒã«ã‚ˆã‚‹è¤‡é›‘æ€§å‘ä¸Š
        complexity_boost = self.config.lie_algebra_dim * 0.4  # 64bitç’°å¢ƒã§é«˜åŠ¹æœ
        
        # 64bitç²¾åº¦ãƒœãƒ¼ãƒŠã‚¹
        precision_bonus = 5.0 if self.config.use_64bit_precision else 0.0
        
        total_improvement = representation_boost + stability_boost + complexity_boost + precision_bonus
        return min(total_improvement, 25.0)  # 64bitç’°å¢ƒã§ã¯æœ€å¤§25%ã®ç²¾åº¦å‘ä¸Š
    
    def get_inference_impact_report(self) -> str:
        """æ¨è«–ã¸ã®å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        speedup = self._estimate_speedup()
        memory_eff = self._estimate_memory_efficiency()
        accuracy_imp = self._estimate_accuracy_improvement()
        
        report = f"""
ğŸ§  NKATç†è«–ã«ã‚ˆã‚‹æ¨è«–ã¸ã®å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆï¼ˆãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ç‰ˆï¼‰
{'='*65}

ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬:
   ğŸš€ æ¨è«–é€Ÿåº¦å‘ä¸Š:     {speedup:.1f}x
   ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡:       {memory_eff*100:.1f}% (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›)
   ğŸ¯ ç²¾åº¦å‘ä¸Š:         +{accuracy_imp:.1f}%

âš™ï¸ NKATè¨­å®š:
   ğŸ”§ KAã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º:  {self.config.ka_grid_size}
   ğŸ§® ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒ:      {self.config.lie_algebra_dim}
   âš¡ éå¯æ›å¼·åº¦:        {self.config.noncommutative_strength}
   ğŸ“ å¾®åˆ†å¹¾ä½•ã‚¹ã‚±ãƒ¼ãƒ«:  {self.config.differential_geometric_scale}
   ğŸ¯ ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ¶é™:    {self.config.spectral_radius_bound}

ğŸ” å®Ÿè£…çŠ¶æ³ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰:
   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±åˆ:    å®Œäº†
   âœ… ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›:      å®Œäº†ï¼ˆãƒ•ãƒ«å®Ÿè£…ï¼‰
   âœ… KAå¤‰æ›:          Kolmogorov-Arnold ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›
   âœ… éå¯æ›å¤‰æ›:       ãƒªãƒ¼ä»£æ•°ã«ã‚ˆã‚‹éå¯æ›æ¼”ç®—
   âœ… å¾®åˆ†å¹¾ä½•æœ€é©åŒ–:   ãƒªãƒ¼ãƒãƒ³è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ«æœ€é©åŒ–
   âœ… ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£è¦åŒ–:  ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹åˆ¶é™
   âœ… æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³:     å¾“æ¥ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚‚å‹•ä½œå¯èƒ½

ğŸ”§ å®Ÿéš›ã®ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›å†…å®¹:
   1. ğŸ›ï¸ Kolmogorov-Arnoldå¤‰æ›:
      - ç·šå½¢é‡ã¿è¡Œåˆ—ã‚’ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³åŸºåº•é–¢æ•°ã§æ‹¡å¼µ
      - ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º{self.config.ka_grid_size}Ã—{self.config.ka_grid_size}ã®B-ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³
      - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®å‘ä¸Š
      
   2. ğŸŒ€ éå¯æ›ä»£æ•°å¤‰æ›:
      - ãƒªãƒ¼ä»£æ•°ç”Ÿæˆå­ã«ã‚ˆã‚‹éå¯æ›æ¼”ç®—
      - æ¬¡å…ƒ: {self.config.lie_algebra_dim}
      - å¼·åº¦: {self.config.noncommutative_strength}
      - ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ç©ºé–“ã®å®Ÿç¾
      
   3. ğŸ“ å¾®åˆ†å¹¾ä½•å­¦çš„æœ€é©åŒ–:
      - ãƒªãƒ¼ãƒãƒ³è¨ˆé‡ã«ã‚ˆã‚‹é‡ã¿æœ€é©åŒ–
      - å‹¾é…ãƒ»ãƒ˜ãƒƒã‚·ã‚¢ãƒ³ãƒ™ãƒ¼ã‚¹ã®è£œæ­£
      - æ¸¬åœ°ç·šã«æ²¿ã£ãŸæœ€é©ãƒ‘ã‚¹
      - ã‚¹ã‚±ãƒ¼ãƒ«: {self.config.differential_geometric_scale}
      
   4. ğŸ¯ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£è¦åŒ–:
      - ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹åˆ¶å¾¡
      - ã‚¹ãƒšã‚¯ãƒˆãƒ«åŠå¾„åˆ¶é™: {self.config.spectral_radius_bound}
      - æ•°å€¤å®‰å®šæ€§ã®å‘ä¸Š

ğŸš€ æ¨è«–æ™‚ã®æœŸå¾…åŠ¹æœ:
   ğŸ“ˆ è¨ˆç®—åŠ¹ç‡åŒ–:
      - KAå¤‰æ›ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åœ§ç¸®
      - éå¯æ›æ¼”ç®—ã®ä¸¦åˆ—åŒ–å¯èƒ½æ€§
      - ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ¶é™ã«ã‚ˆã‚‹è¨ˆç®—å®‰å®šåŒ–
      
   ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:
      - åŠ¹ç‡çš„ãªè¡¨ç¾ã«ã‚ˆã‚‹å‰Šæ¸›
      - é‡å­åŒ–å¯¾å¿œå¼·åŒ–
      - å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
      
   ğŸ¯ ç²¾åº¦å‘ä¸Š:
      - éç·šå½¢è¡¨ç¾åŠ›ã®æ‹¡å¼µ
      - éå¯æ›æ€§ã«ã‚ˆã‚‹è¤‡é›‘ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ
      - å¾®åˆ†å¹¾ä½•å­¦çš„å®‰å®šæ€§
      
   âš¡ äº’æ›æ€§:
      - æ—¢å­˜æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§å‹•ä½œ
      - æ®µéšçš„ãªæœ€é©åŒ–é©ç”¨
      - å¾Œæ–¹äº’æ›æ€§ä¿æŒ

ğŸ“ æŠ€è¡“è©³ç´°:
   ğŸ”¬ å¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
      - B-ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³åŸºåº•ã«ã‚ˆã‚‹é€£ç¶šé–¢æ•°è¿‘ä¼¼
      - ãƒ‘ã‚¦ãƒªè¡Œåˆ—å‹ãƒªãƒ¼ä»£æ•°ç”Ÿæˆå­
      - ãƒªãƒ¼ãƒãƒ³è¨ˆé‡ãƒ†ãƒ³ã‚½ãƒ«ã«ã‚ˆã‚‹æ¸¬åœ°ç·šæœ€é©åŒ–
      - SVDãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ¶å¾¡
      
   ğŸ›¡ï¸ æ•°å€¤å®‰å®šæ€§:
      - ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«ã‚ˆã‚‹è¿‘ä¼¼è¨ˆç®—
      - ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹å€¤åŸŸåˆ¶é™
      - å‹å®‰å…¨ãªå¤‰æ›å‡¦ç†
      - å…ƒãƒ‡ãƒ¼ã‚¿å‹ã®ä¿æŒ

âš ï¸ æ³¨æ„äº‹é …:
   âœ… å®Ÿè£…: å®Œå…¨ãªãƒ†ãƒ³ã‚½ãƒ«ãƒ¬ãƒ™ãƒ«å¤‰æ›ãŒå®Ÿè£…æ¸ˆã¿
   âœ… äº’æ›æ€§: å¾“æ¥ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§å‹•ä½œå¯èƒ½
   âœ… å®‰å…¨æ€§: å…ƒãƒ‡ãƒ¼ã‚¿å‹ãƒ»å½¢çŠ¶ã‚’ä¿æŒ
   âš ï¸ æ¤œè¨¼: å®Ÿéš›ã®æ€§èƒ½å‘ä¸Šã¯å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã§è¦ç¢ºèª
   ğŸ’¡ æœ€é©åŒ–: è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã§åŠ¹æœèª¿æ•´å¯èƒ½
        """
        return report
    
    def read_gguf_header_64bit(self, file_path: str) -> Dict:
        """GGUFãƒ˜ãƒƒãƒ€ãƒ¼ã®èª­ã¿å–ã‚Šï¼ˆ64bitå¯¾å¿œç‰ˆï¼‰"""
        with open(file_path, 'rb') as f:
            magic = f.read(4)
            if magic != self.GGUF_MAGIC:
                raise ValueError(f"Invalid GGUF file: {file_path}")
            
            # 64bitå¢ƒç•Œã«æ•´åˆ—
            if self.config.use_64bit_precision:
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚64bitã¨ã—ã¦èª­ã¿å–ã‚Šï¼ˆäº’æ›æ€§ç¶­æŒã®ãŸã‚32bitã‹ã‚‰æ‹¡å¼µï¼‰
                version_32 = struct.unpack('<I', f.read(4))[0]
                version = np.uint64(version_32)
                
                # ãƒ†ãƒ³ã‚½ãƒ«æ•°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ã¯æ—¢ã«64bit
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                metadata_kv_count = struct.unpack('<Q', f.read(8))[0]
            else:
                # å¾“æ¥ã®32bitç‰ˆã¨ã®äº’æ›æ€§
                version = struct.unpack('<I', f.read(4))[0]
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                metadata_kv_count = struct.unpack('<Q', f.read(8))[0]
            
            return {
                "magic": magic,
                "version": int(version),
                "tensor_count": tensor_count,
                "metadata_kv_count": metadata_kv_count,
                "header_size": f.tell(),
                "precision_mode": "64bit" if self.config.use_64bit_precision else "mixed"
            }
    
    def read_gguf_metadata_64bit(self, file_path: str) -> Dict:
        """GGUFãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Šï¼ˆ64bitå¯¾å¿œç‰ˆï¼‰"""
        metadata = {}
        max_key_size = 1024 * 1024  # 1MBä»¥å†…ã®ã‚­ãƒ¼åˆ¶é™
        max_value_size = 10 * 1024 * 1024  # 10MBä»¥å†…ã®å€¤åˆ¶é™
        
        try:
            with open(file_path, 'rb') as f:
                header = self.read_gguf_header_64bit(file_path)
                f.seek(header["header_size"])
                
                print(f"   ğŸ“Š 64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {header['precision_mode']}")
                print(f"   ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®æ•°: {header['metadata_kv_count']}")
                
                for i in range(header["metadata_kv_count"]):
                    try:
                        start_pos = f.tell()
                        
                        # ã‚­ãƒ¼é•·èª­ã¿å–ã‚Šï¼ˆ64bitï¼‰
                        key_len_bytes = f.read(8)
                        if len(key_len_bytes) != 8:
                            print(f"   âš ï¸ 64bit ã‚­ãƒ¼é•·èª­ã¿å–ã‚Šå¤±æ•—: {i+1}/{header['metadata_kv_count']}")
                            break
                        
                        key_len = struct.unpack('<Q', key_len_bytes)[0]
                        
                        # 64bitå¢ƒç•Œã§ã®ã‚µã‚¤ã‚ºæ¤œè¨¼
                        if key_len == 0 or key_len > max_key_size:
                            print(f"   âš ï¸ 64bit ã‚­ãƒ¼ã‚µã‚¤ã‚ºç•°å¸¸: {key_len} bytes")
                            f.seek(start_pos + 1)
                            continue
                        
                        # ã‚­ãƒ¼èª­ã¿å–ã‚Š
                        key_data = f.read(key_len)
                        if len(key_data) != key_len:
                            print(f"   âš ï¸ ã‚­ãƒ¼èª­ã¿å–ã‚Šä¸å®Œå…¨: {len(key_data)}/{key_len}")
                            continue
                        
                        try:
                            key = key_data.decode('utf-8')
                        except UnicodeDecodeError as e:
                            print(f"   âš ï¸ ã‚­ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
                            continue
                        
                        # å€¤ã®å‹èª­ã¿å–ã‚Š
                        value_type_bytes = f.read(4)
                        if len(value_type_bytes) != 4:
                            print(f"   âš ï¸ å€¤å‹èª­ã¿å–ã‚Šå¤±æ•—: {key}")
                            break
                        
                        value_type = struct.unpack('<I', value_type_bytes)[0]
                        
                        # å€¤èª­ã¿å–ã‚Šï¼ˆ64bitç²¾åº¦å¯¾å¿œï¼‰
                        if value_type == 4:  # string
                            value_len_bytes = f.read(8)
                            if len(value_len_bytes) != 8:
                                continue
                            
                            value_len = struct.unpack('<Q', value_len_bytes)[0]
                            
                            if value_len > max_value_size:
                                f.seek(value_len, 1)  # ã‚¹ã‚­ãƒƒãƒ—
                                continue
                            
                            value_data = f.read(value_len)
                            if len(value_data) == value_len:
                                try:
                                    value = value_data.decode('utf-8')
                                    metadata[key] = value
                                except UnicodeDecodeError:
                                    pass
                                    
                        elif value_type == 6:  # int32 -> 64bitæ‹¡å¼µ
                            value_bytes = f.read(4)
                            if len(value_bytes) == 4:
                                int32_val = struct.unpack('<i', value_bytes)[0]
                                if self.config.use_64bit_precision:
                                    value = np.int64(int32_val)  # 64bitç²¾åº¦ã«æ‹¡å¼µ
                                else:
                                    value = int32_val
                                metadata[key] = int(value)
                                
                        elif value_type == 7:  # float32 -> 64bitæ‹¡å¼µ
                            value_bytes = f.read(4)
                            if len(value_bytes) == 4:
                                float32_val = struct.unpack('<f', value_bytes)[0]
                                if self.config.use_64bit_precision:
                                    value = np.float64(float32_val)  # 64bitç²¾åº¦ã«æ‹¡å¼µ
                                else:
                                    value = float32_val
                                metadata[key] = float(value)
                                
                        elif value_type == 8:  # bool
                            value_bytes = f.read(1)
                            if len(value_bytes) == 1:
                                value = bool(value_bytes[0])
                                metadata[key] = value
                                
                        elif value_type == 9:  # array
                            # é…åˆ—å‹ã®è©³ç´°èª­ã¿å–ã‚Šï¼ˆ64bitå¯¾å¿œï¼‰
                            array_type_bytes = f.read(4)
                            array_len_bytes = f.read(8)
                            if len(array_type_bytes) == 4 and len(array_len_bytes) == 8:
                                array_type = struct.unpack('<I', array_type_bytes)[0]
                                array_len = struct.unpack('<Q', array_len_bytes)[0]
                                
                                # 64bitç’°å¢ƒã§ã®é…åˆ—å‡¦ç†æœ€é©åŒ–
                                if array_type in [6, 7] and array_len < 1000:  # æ•°å€¤é…åˆ—ã§å°ã‚µã‚¤ã‚º
                                    try:
                                        if array_type == 6:  # int32 array
                                            array_data = []
                                            for j in range(array_len):
                                                int_bytes = f.read(4)
                                                if len(int_bytes) == 4:
                                                    int_val = struct.unpack('<i', int_bytes)[0]
                                                    if self.config.use_64bit_precision:
                                                        array_data.append(int(np.int64(int_val)))
                                                    else:
                                                        array_data.append(int_val)
                                            metadata[key] = array_data
                                        elif array_type == 7:  # float32 array  
                                            array_data = []
                                            for j in range(array_len):
                                                float_bytes = f.read(4)
                                                if len(float_bytes) == 4:
                                                    float_val = struct.unpack('<f', float_bytes)[0]
                                                    if self.config.use_64bit_precision:
                                                        array_data.append(float(np.float64(float_val)))
                                                    else:
                                                        array_data.append(float_val)
                                            metadata[key] = array_data
                                    except Exception:
                                        # é…åˆ—èª­ã¿å–ã‚Šå¤±æ•—æ™‚ã¯å®‰å…¨ã«ã‚¹ã‚­ãƒƒãƒ—
                                        pass
                                else:
                                    # å¤§ããªé…åˆ—ã‚„è¤‡é›‘ãªå‹ã¯å®‰å…¨ã«ã‚¹ã‚­ãƒƒãƒ—
                                    if array_type in [6, 7]:
                                        element_size = 4
                                        skip_size = array_len * element_size
                                        f.seek(skip_size, 1)
                        else:
                            # ãã®ä»–ã®å‹ã¯ã‚¹ã‚­ãƒƒãƒ—
                            pass
                        
                        # é€²æ—è¡¨ç¤ºï¼ˆ10å€‹ã”ã¨ï¼‰
                        if (i + 1) % 10 == 0 and self.config.enable_performance_monitoring:
                            print(f"   ğŸ“Š 64bit ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {i+1}/{header['metadata_kv_count']}")
                    
                    except Exception as e:
                        print(f"   âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®{i+1} 64bitèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                        continue
        
        except Exception as e:
            print(f"   âŒ 64bit ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šå…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
        
        print(f"   âœ… 64bit ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(metadata)} é …ç›®")
        return metadata
    
    def read_gguf_header(self, file_path: str) -> Dict:
        """GGUFãƒ˜ãƒƒãƒ€ãƒ¼ã®èª­ã¿å–ã‚Šï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        return self.read_gguf_header_64bit(file_path)
    
    def read_gguf_metadata(self, file_path: str) -> Dict:
        """GGUFãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Šï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        return self.read_gguf_metadata_64bit(file_path)
    
    def find_gguf_models(self, search_dirs: Optional[List[str]] = None) -> List[Path]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆrun_64bit_integration_test.pyã‹ã‚‰çµ±åˆï¼‰"""
        if search_dirs is None:
            search_dirs = [
                ".",
                "data",
                "models", 
                "test_models",
                "07_NKATtransformer_ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
            ]
        
        current_dir = Path(".")
        gguf_files = []
        
        # checkpointsç³»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•è¿½åŠ 
        for item in current_dir.iterdir():
            if item.is_dir() and "checkpoint" in item.name.lower():
                search_dirs.append(str(item))
        
        print("ğŸ” GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ä¸­...")
        print(f"   æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(search_dirs)}å€‹")
        
        for dir_name in search_dirs:
            dir_path = Path(dir_name)
            if dir_path.exists() and dir_path.is_dir():
                print(f"   ğŸ“ æ¤œç´¢ä¸­: {dir_name}")
                try:
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®.ggufãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
                    for gguf_file in dir_path.rglob("*.gguf"):
                        if gguf_file.is_file() and gguf_file.stat().st_size > 1024:  # 1KBä»¥ä¸Š
                            gguf_files.append(gguf_file)
                            if self.config.enable_performance_monitoring:
                                print(f"     âœ… {gguf_file.name}: {gguf_file.stat().st_size / (1024*1024):.2f} MB")
                except Exception as e:
                    print(f"     âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»ã¨ã‚µã‚¤ã‚ºé †ã‚½ãƒ¼ãƒˆ
        gguf_files = list(set(gguf_files))
        gguf_files.sort(key=lambda x: x.stat().st_size, reverse=True)
        
        print(f"\n   ğŸ“Š ç·ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(gguf_files)}å€‹")
        if gguf_files and self.config.enable_performance_monitoring:
            print(f"   ğŸ† ä¸Šä½ãƒ•ã‚¡ã‚¤ãƒ«:")
            for i, gguf_file in enumerate(gguf_files[:5], 1):  # ä¸Šä½5å€‹è¡¨ç¤º
                size_mb = gguf_file.stat().st_size / (1024 * 1024)
                print(f"     {i}. {gguf_file.name}: {size_mb:.2f} MB")
        
        return gguf_files
    
    def test_model_integration(self, model_path: Path, output_path: Optional[Path] = None) -> Dict:
        """å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆrun_64bit_integration_test.pyã‹ã‚‰çµ±åˆï¼‰"""
        print(f"\nğŸ”¬ 64bitç²¾åº¦ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ: {model_path.name}")
        print("-" * 60)
        
        # å‡ºåŠ›ãƒ‘ã‚¹ç”Ÿæˆ
        if output_path is None:
            output_path = model_path.parent / f"{model_path.stem}_nkat_64bit_integrated.gguf"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        size_mb = model_path.stat().st_size / (1024 * 1024)
        print(f"   ğŸ“Š å…ƒãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {size_mb:.2f} MB")
        print(f"   ğŸ“ å…¥åŠ›: {model_path}")
        print(f"   ğŸ“ å‡ºåŠ›: {output_path}")
        print(f"   ğŸ§® 64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {self.config.use_64bit_precision}")
        
        # çµ±åˆå‡¦ç†å®Ÿè¡Œ
        start_time = time.time()
        
        try:
            success = self.create_nkat_enhanced_gguf(str(model_path), str(output_path))
            
            elapsed = time.time() - start_time
            
            if success and output_path.exists():
                output_size_mb = output_path.stat().st_size / (1024 * 1024)
                size_increase = ((output_size_mb - size_mb) / size_mb) * 100
                
                print(f"   âœ… 64bitçµ±åˆæˆåŠŸ!")
                print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’")
                print(f"   ğŸ“Š å‡ºåŠ›ã‚µã‚¤ã‚º: {output_size_mb:.2f} MB")
                print(f"   ğŸ“ˆ ã‚µã‚¤ã‚ºå¢—åŠ : {size_increase:+.2f}%")
                print(f"   ğŸ¯ åŠ¹ç‡æ€§: {'âœ… å„ªç§€' if size_increase < 5 else 'âš ï¸ è¦æœ€é©åŒ–'}")
                
                # çµ±è¨ˆåˆ†æ
                processing_rate = size_mb / elapsed if elapsed > 0 else 0
                print(f"   ğŸš€ å‡¦ç†é€Ÿåº¦: {processing_rate:.1f} MB/ç§’")
                
                # 64bitç²¾åº¦æ”¹è‰¯åŠ¹æœã®ç¢ºèª
                precision_improvement = self._verify_64bit_improvements(str(output_path))
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆæ›´æ–°
                self.performance_stats["files_processed"] += 1
                self.performance_stats["total_input_size"] += size_mb
                self.performance_stats["total_output_size"] += output_size_mb
                self.performance_stats["total_processing_time"] += elapsed
                if precision_improvement:
                    self.performance_stats["precision_improvements"] += 1
                
                return {
                    "success": True,
                    "input_size_mb": size_mb,
                    "output_size_mb": output_size_mb,
                    "processing_time": elapsed,
                    "size_increase_percent": size_increase,
                    "processing_rate_mb_per_sec": processing_rate,
                    "precision_64bit": self.config.use_64bit_precision,
                    "precision_improvement": precision_improvement,
                    "metadata_items_added": len(self.nkat_metadata)
                }
            else:
                print(f"   âŒ 64bitçµ±åˆå¤±æ•—")
                self.performance_stats["errors"] += 1
                return {"success": False}
                
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            self.performance_stats["errors"] += 1
            return {"success": False, "error": str(e)}
    
    def _verify_64bit_improvements(self, output_path: str) -> bool:
        """64bitç²¾åº¦æ”¹è‰¯åŠ¹æœã®æ¤œè¨¼ï¼ˆè©³ç´°ç‰ˆï¼‰"""
        try:
            print(f"   ğŸ”¬ 64bitç²¾åº¦æ”¹è‰¯æ¤œè¨¼é–‹å§‹...")
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not os.path.exists(output_path):
                print(f"   âŒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {output_path}")
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(output_path)
            print(f"   ğŸ“Š å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size / (1024*1024):.2f} MB")
            
            if file_size < 1024:  # 1KBæœªæº€
                print(f"   âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™")
                return False
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
            try:
                output_metadata = self.read_gguf_metadata_64bit(output_path)
                print(f"   ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®æ•°: {len(output_metadata)}")
            except Exception as e:
                print(f"   âš ï¸ 64bitãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šå¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸èª­ã¿å–ã‚Š
                try:
                    output_metadata = self.read_gguf_metadata(output_path)
                    print(f"   ğŸ“‹ é€šå¸¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®æ•°: {len(output_metadata)}")
                except Exception as e2:
                    print(f"   âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šå®Œå…¨å¤±æ•—: {e2}")
                    return False
            
            # NKATé–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            nkat_keys = [k for k in output_metadata.keys() if k.startswith("nkat.")]
            print(f"   ğŸ§  NKATé–¢é€£é …ç›®: {len(nkat_keys)}")
            
            # 64bitç²¾åº¦é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            precision_keys = [k for k in output_metadata.keys() if 
                            "precision" in k or "64bit" in k or "data_alignment" in k]
            print(f"   ğŸ”¢ ç²¾åº¦é–¢é€£é …ç›®: {len(precision_keys)}")
            
            # é‡è¦ãª64bitç²¾åº¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
            required_64bit_keys = [
                "nkat.precision.mode",
                "nkat.precision.data_alignment",
                "nkat.version"
            ]
            
            missing_keys = []
            for key in required_64bit_keys:
                if key not in output_metadata:
                    missing_keys.append(key)
                else:
                    value = output_metadata[key]
                    print(f"   âœ… {key} = {value}")
            
            if missing_keys:
                print(f"   âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹64bitç²¾åº¦ã‚­ãƒ¼: {missing_keys}")
            
            # 64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            precision_mode = output_metadata.get("nkat.precision.mode", "")
            data_alignment = output_metadata.get("nkat.precision.data_alignment", 0)
            nkat_version = output_metadata.get("nkat.version", "")
            
            # æ¤œè¨¼åŸºæº–
            checks = []
            
            # 1. ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰ç¢ºèª
            if precision_mode == "64bit":
                checks.append(("ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰", True, f"64bit ãƒ¢ãƒ¼ãƒ‰"))
                print(f"   âœ… ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {precision_mode}")
            else:
                checks.append(("ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰", False, f"é64bit ãƒ¢ãƒ¼ãƒ‰: {precision_mode}"))
                print(f"   âš ï¸ ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {precision_mode}")
            
            # 2. ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—ç¢ºèª
            if data_alignment == 8:
                checks.append(("ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—", True, f"8ãƒã‚¤ãƒˆå¢ƒç•Œ"))
                print(f"   âœ… ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—: {data_alignment}ãƒã‚¤ãƒˆ")
            else:
                checks.append(("ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—", False, f"é8ãƒã‚¤ãƒˆå¢ƒç•Œ: {data_alignment}"))
                print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—: {data_alignment}ãƒã‚¤ãƒˆ")
            
            # 3. NKATãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
            if "64bit" in nkat_version:
                checks.append(("NKATãƒãƒ¼ã‚¸ãƒ§ãƒ³", True, f"64bitå¯¾å¿œç‰ˆ"))
                print(f"   âœ… NKATãƒãƒ¼ã‚¸ãƒ§ãƒ³: {nkat_version}")
            else:
                checks.append(("NKATãƒãƒ¼ã‚¸ãƒ§ãƒ³", False, f"é64bitç‰ˆ: {nkat_version}"))
                print(f"   âš ï¸ NKATãƒãƒ¼ã‚¸ãƒ§ãƒ³: {nkat_version}")
            
            # 4. NKATé …ç›®æ•°ç¢ºèª
            if len(nkat_keys) >= 20:  # æœ€ä½20é …ç›®ã®NKATè¨­å®š
                checks.append(("NKATé …ç›®æ•°", True, f"{len(nkat_keys)}é …ç›®"))
                print(f"   âœ… NKATé …ç›®æ•°: {len(nkat_keys)}")
            else:
                checks.append(("NKATé …ç›®æ•°", False, f"ä¸è¶³: {len(nkat_keys)}é …ç›®"))
                print(f"   âš ï¸ NKATé …ç›®æ•°: {len(nkat_keys)}")
            
            # 5. å®Ÿè£…ãƒ¬ãƒ™ãƒ«ç¢ºèª
            impl_level = output_metadata.get("nkat.implementation.level", "")
            if "64bit" in impl_level:
                checks.append(("å®Ÿè£…ãƒ¬ãƒ™ãƒ«", True, f"64bitå®Ÿè£…"))
                print(f"   âœ… å®Ÿè£…ãƒ¬ãƒ™ãƒ«: {impl_level}")
            else:
                checks.append(("å®Ÿè£…ãƒ¬ãƒ™ãƒ«", False, f"é64bitå®Ÿè£…: {impl_level}"))
                print(f"   âš ï¸ å®Ÿè£…ãƒ¬ãƒ™ãƒ«: {impl_level}")
            
            # ç·åˆåˆ¤å®š
            passed_checks = sum(1 for _, passed, _ in checks if passed)
            total_checks = len(checks)
            success_rate = passed_checks / total_checks * 100
            
            print(f"   ğŸ“Š æ¤œè¨¼çµæœ: {passed_checks}/{total_checks} é …ç›®é€šé ({success_rate:.1f}%)")
            
            # è©³ç´°çµæœè¡¨ç¤º
            for check_name, passed, detail in checks:
                status = "âœ…" if passed else "âŒ"
                print(f"     {status} {check_name}: {detail}")
            
            # æˆåŠŸåˆ¤å®šï¼ˆ80%ä»¥ä¸Šã§æˆåŠŸï¼‰
            if success_rate >= 80:
                print(f"   ğŸ‰ 64bitç²¾åº¦æ”¹è‰¯: æˆåŠŸ ({success_rate:.1f}%)")
                return True
            else:
                print(f"   âš ï¸ 64bitç²¾åº¦æ”¹è‰¯: ä¸ååˆ† ({success_rate:.1f}%)")
                return False
                
        except Exception as e:
            print(f"   âŒ 64bitæ”¹è‰¯æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±: {traceback.format_exc()}")
            return False
    
    def create_nkat_enhanced_gguf(self, input_path: str, output_path: str) -> bool:
        """NKATæ‹¡å¼µGGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆå …ç‰¢ç‰ˆï¼‰"""
        print(f"ğŸ”„ NKATç†è«–ã‚’GGUFãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆä¸­ï¼ˆå …ç‰¢ç‰ˆï¼‰...")
        print(f"   å…¥åŠ›: {os.path.basename(input_path)}")
        print(f"   å‡ºåŠ›: {os.path.basename(output_path)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(input_path) / (1024**3)
        print(f"   ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f}GB")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        gc.collect()
        
        try:
            # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ åˆ†æ
            print(f"   ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ åˆ†æé–‹å§‹...")
            basic_info = self._analyze_gguf_structure(input_path)
            
            if not basic_info["valid"]:
                print(f"   âš ï¸ æ¨™æº–çš„ãªGGUFæ§‹é€ è§£æã«å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
                fallback_result = self._create_fallback_nkat_gguf(input_path, output_path)
                return fallback_result is not None
            
            # ã‚ˆã‚Šå®‰å…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
            print(f"   ğŸ“‹ å®‰å…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š...")
            existing_metadata = self._safe_read_metadata(input_path, basic_info)
            print(f"   æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {len(existing_metadata)} é …ç›®")
            
            # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã®ä»£æ›¿å–å¾—
            print(f"   ğŸ”§ ä»£æ›¿ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±å–å¾—...")
            tensor_count = basic_info.get("tensor_count", 0)
            if tensor_count > 0:
                # ç°¡æ˜“çš„ãªãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ç”Ÿæˆ
                synthetic_tensors = self._generate_synthetic_tensor_info(input_path, tensor_count)
                print(f"   åˆæˆãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±: {len(synthetic_tensors)} å€‹")
            else:
                synthetic_tensors = []
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            gc.collect()
            
            # NKATç†è«–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
            enhanced_metadata = {**existing_metadata, **self.nkat_metadata}
            
            # å¤‰æ›çµ±è¨ˆã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            enhanced_metadata.update({
                "nkat.transform.tensor_count": len(synthetic_tensors),
                "nkat.transform.total_parameters": sum(t.get("size", 1000) for t in synthetic_tensors),
                "nkat.transform.transformations": json.dumps(self.tensor_transformations),
                "nkat.fallback.used": len(synthetic_tensors) == 0
            })
            
            # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æƒ…å ±æ›´æ–°ï¼ˆå‹å®‰å…¨ï¼‰
            if "general.architecture" in enhanced_metadata:
                arch_value = enhanced_metadata["general.architecture"]
                if isinstance(arch_value, str):
                    enhanced_metadata["general.architecture"] = "nkat_" + arch_value
                elif isinstance(arch_value, bool):
                    enhanced_metadata["general.architecture"] = f"nkat_quantized"
                else:
                    arch_str = str(arch_value)
                    enhanced_metadata["general.architecture"] = f"nkat_{arch_str}"
            else:
                enhanced_metadata["general.architecture"] = "nkat_enhanced"
            
            # ãƒ¢ãƒ‡ãƒ«åæ›´æ–°
            enhanced_metadata["general.name"] = "NKAT_Enhanced_Model"
            
            print(f"   NKATæ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {len(self.nkat_metadata)} é …ç›®è¿½åŠ ")
            
            # è»½é‡ãªNKATãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            lightweight_result = self._create_lightweight_nkat_gguf(input_path, output_path, enhanced_metadata, file_size)
            
            if lightweight_result and os.path.exists(output_path):
                print(f"âœ… NKATæ‹¡å¼µGGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†ï¼ˆå …ç‰¢ç‰ˆï¼‰")
                return True
            else:
                print(f"âŒ è»½é‡NKAT GGUFä½œæˆã«å¤±æ•—")
                return False
            
        except Exception as e:
            print(f"âŒ NKATçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ...")
            try:
                fallback_result = self._create_fallback_nkat_gguf(input_path, output_path)
                return fallback_result is not None and os.path.exists(output_path)
            except Exception as e2:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚å¤±æ•—: {e2}")
                return False
    
    def _analyze_gguf_structure(self, file_path: str) -> Dict:
        """GGUFãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®åŸºæœ¬åˆ†æ"""
        analysis = {"valid": False, "tensor_count": 0, "metadata_count": 0}
        
        try:
            with open(file_path, 'rb') as f:
                # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ç¢ºèª
                magic = f.read(4)
                if magic != self.GGUF_MAGIC:
                    print(f"   âš ï¸ ç„¡åŠ¹ãªGGUFãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼: {magic}")
                    return analysis
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³
                version_bytes = f.read(4)
                if len(version_bytes) != 4:
                    return analysis
                version = struct.unpack('<I', version_bytes)[0]
                
                # ãƒ†ãƒ³ã‚½ãƒ«æ•°
                tensor_count_bytes = f.read(8)
                if len(tensor_count_bytes) != 8:
                    return analysis
                tensor_count = struct.unpack('<Q', tensor_count_bytes)[0]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°
                metadata_count_bytes = f.read(8)
                if len(metadata_count_bytes) != 8:
                    return analysis
                metadata_count = struct.unpack('<Q', metadata_count_bytes)[0]
                
                # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if (version <= 10 and 
                    tensor_count <= 10000 and  # æœ€å¤§10K ãƒ†ãƒ³ã‚½ãƒ«
                    metadata_count <= 1000):   # æœ€å¤§1K ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    
                    analysis.update({
                        "valid": True,
                        "version": version,
                        "tensor_count": tensor_count,
                        "metadata_count": metadata_count,
                        "header_size": 24
                    })
                    print(f"   âœ… æœ‰åŠ¹ãªGGUFæ§‹é€ : v{version}, ãƒ†ãƒ³ã‚½ãƒ«{tensor_count}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿{metadata_count}")
                else:
                    print(f"   âš ï¸ ç–‘ã‚ã—ã„GGUFå€¤: v{version}, T{tensor_count}, M{metadata_count}")
                
        except Exception as e:
            print(f"   âš ï¸ GGUFæ§‹é€ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        return analysis
    
    def _safe_read_metadata(self, file_path: str, basic_info: Dict) -> Dict:
        """å®‰å…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š"""
        metadata = {}
        
        try:
            with open(file_path, 'rb') as f:
                f.seek(basic_info["header_size"])
                metadata_count = min(basic_info["metadata_count"], 50)  # æœ€å¤§50é …ç›®ã¾ã§
                
                print(f"   ğŸ“‹ å®‰å…¨èª­ã¿å–ã‚Šå¯¾è±¡: {metadata_count} é …ç›®")
                
                successful_reads = 0
                for i in range(metadata_count):
                    try:
                        current_pos = f.tell()
                        
                        # ã‚­ãƒ¼é•·ã®å®‰å…¨ãªèª­ã¿å–ã‚Š
                        key_len_data = f.read(8)
                        if len(key_len_data) != 8:
                            break
                        
                        key_len = struct.unpack('<Q', key_len_data)[0]
                        
                        # ã‚­ãƒ¼é•·ã®å³æ ¼ãªæ¤œè¨¼
                        if key_len == 0 or key_len > 256:  # 256æ–‡å­—ä»¥å†…
                            print(f"   âš ï¸ é …ç›®{i+1}: ç•°å¸¸ãªã‚­ãƒ¼é•· {key_len}")
                            # æ¬¡ã®æœ‰åŠ¹ä½ç½®ã‚’æ¢ã™
                            f.seek(current_pos + 1)
                            continue
                        
                        # ã‚­ãƒ¼ã®å®‰å…¨ãªèª­ã¿å–ã‚Š
                        key_data = f.read(key_len)
                        if len(key_data) != key_len:
                            break
                        
                        try:
                            key = key_data.decode('utf-8')
                        except UnicodeDecodeError:
                            print(f"   âš ï¸ é …ç›®{i+1}: ã‚­ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—")
                            continue
                        
                        # å€¤å‹ã®èª­ã¿å–ã‚Š
                        value_type_data = f.read(4)
                        if len(value_type_data) != 4:
                            break
                        
                        value_type = struct.unpack('<I', value_type_data)[0]
                        
                        # å‹åˆ¥ã®å®‰å…¨ãªå€¤èª­ã¿å–ã‚Š
                        value = None
                        if value_type == 4:  # string
                            value_len_data = f.read(8)
                            if len(value_len_data) == 8:
                                value_len = struct.unpack('<Q', value_len_data)[0]
                                if 0 < value_len <= 10000:  # 10KBä»¥å†…
                                    value_data = f.read(value_len)
                                    if len(value_data) == value_len:
                                        try:
                                            value = value_data.decode('utf-8')
                                        except UnicodeDecodeError:
                                            value = f"binary_data_{len(value_data)}_bytes"
                        
                        elif value_type == 6:  # int32
                            int_data = f.read(4)
                            if len(int_data) == 4:
                                value = struct.unpack('<i', int_data)[0]
                        
                        elif value_type == 7:  # float32
                            float_data = f.read(4)
                            if len(float_data) == 4:
                                value = struct.unpack('<f', float_data)[0]
                        
                        elif value_type == 8:  # bool
                            bool_data = f.read(1)
                            if len(bool_data) == 1:
                                value = bool(bool_data[0])
                        
                        else:
                            # æœªå¯¾å¿œå‹ã¯ã‚¹ã‚­ãƒƒãƒ—
                            print(f"   ğŸ“‹ æœªå¯¾å¿œå‹{value_type}: {key}")
                        
                        if value is not None:
                            metadata[key] = value
                            successful_reads += 1
                            print(f"   âœ… é …ç›®{i+1}: {key} = {str(value)[:50]}")
                    
                    except Exception as e:
                        print(f"   âš ï¸ é …ç›®{i+1}èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                print(f"   âœ… å®‰å…¨èª­ã¿å–ã‚Šå®Œäº†: {successful_reads} é …ç›®")
                
        except Exception as e:
            print(f"   âŒ å®‰å…¨èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
        
        return metadata
    
    def _generate_synthetic_tensor_info(self, file_path: str, tensor_count: int) -> List[Dict]:
        """åˆæˆãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ç”Ÿæˆ"""
        synthetic_tensors = []
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®æ¨å®š
            file_size = os.path.getsize(file_path)
            avg_tensor_size = max(file_size // (tensor_count + 1), 1000)  # å¹³å‡ãƒ†ãƒ³ã‚½ãƒ«ã‚µã‚¤ã‚º
            
            print(f"   ğŸ”§ åˆæˆãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆ: {tensor_count} å€‹")
            
            for i in range(min(tensor_count, 100)):  # æœ€å¤§100ãƒ†ãƒ³ã‚½ãƒ«
                # ä»£è¡¨çš„ãªãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’ç”Ÿæˆ
                layer_names = [
                    f"model.layers.{i}.self_attn.q_proj.weight",
                    f"model.layers.{i}.self_attn.k_proj.weight", 
                    f"model.layers.{i}.self_attn.v_proj.weight",
                    f"model.layers.{i}.self_attn.o_proj.weight",
                    f"model.layers.{i}.mlp.gate_proj.weight",
                    f"model.layers.{i}.mlp.up_proj.weight",
                    f"model.layers.{i}.mlp.down_proj.weight",
                ]
                
                layer_name = layer_names[i % len(layer_names)]
                
                # æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                if "self_attn" in layer_name:
                    shape = [4096, 4096]  # å…¸å‹çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿
                elif "mlp" in layer_name:
                    shape = [4096, 11008]  # å…¸å‹çš„ãªMLPé‡ã¿
                else:
                    shape = [4096, 4096]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                
                size = shape[0] * shape[1]
                
                synthetic_tensors.append({
                    "name": layer_name,
                    "shape": shape,
                    "dtype": 0,  # float32ã¨ã—ã¦ä»®å®š
                    "offset": i * avg_tensor_size,
                    "size": size,
                    "synthetic": True
                })
        
        except Exception as e:
            print(f"   âš ï¸ åˆæˆãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return synthetic_tensors
    
    def _create_lightweight_nkat_gguf(self, input_path: str, output_path: str, metadata: Dict, file_size_gb: float) -> bool:
        """è»½é‡NKAT GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼å¯¾å¿œï¼‰"""
        print(f"   ğŸ“ è»½é‡NKAT GGUFä½œæˆé–‹å§‹ï¼ˆãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰...")
        
        try:
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ æƒ…å ±ã‚’å–å¾—
            basic_info = self._analyze_gguf_structure(input_path)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼å‡¦ç†ã§ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
            max_retries = 5
            retry_delay = 1
            
            for attempt in range(max_retries):
                try:
                    print(f"   ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼è©¦è¡Œ {attempt + 1}/{max_retries}...")
                    
                    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ã‚³ãƒ”ãƒ¼
                    temp_path = output_path + ".temp"
                    shutil.copy2(input_path, temp_path)
                    
                    # ã‚³ãƒ”ãƒ¼ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£
                    with open(temp_path, 'r+b') as f:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã®ã¿ã‚’èª­ã¿æ›¸ã
                        f.seek(0)
                        
                        # GGUFãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ä¿æŒ
                        magic = f.read(4)
                        if magic != self.GGUF_MAGIC:
                            print(f"   âš ï¸ ç„¡åŠ¹ãªãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼: {magic}")
                            continue
                        
                        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³èª­ã¿å–ã‚Šãƒ»ä¿æŒ
                        version_bytes = f.read(4)
                        version = struct.unpack('<I', version_bytes)[0]
                        
                        # ãƒ†ãƒ³ã‚½ãƒ«æ•°èª­ã¿å–ã‚Šãƒ»ä¿æŒ
                        tensor_count_bytes = f.read(8)
                        original_tensor_count = struct.unpack('<Q', tensor_count_bytes)[0]
                        
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°èª­ã¿å–ã‚Š
                        metadata_count_bytes = f.read(8)
                        original_metadata_count = struct.unpack('<Q', metadata_count_bytes)[0]
                        
                        print(f"   ğŸ“Š å…ƒãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±: ãƒ†ãƒ³ã‚½ãƒ«{original_tensor_count}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿{original_metadata_count}")
                        
                        # æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
                        new_metadata_count = len(metadata)
                        
                        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ã®ã¿å¤‰æ›´ï¼‰
                        f.seek(16)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ã®ä½ç½®
                        f.write(struct.pack('<Q', new_metadata_count))
                        
                        # æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ç½®æ›
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ä½ç½®
                        metadata_start = 24  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º
                        f.seek(metadata_start)
                        
                        # æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
                        new_metadata_data = b''
                        for key, value in metadata.items():
                            # ã‚­ãƒ¼æ›¸ãè¾¼ã¿
                            key_bytes = key.encode('utf-8')
                            new_metadata_data += struct.pack('<Q', len(key_bytes))
                            new_metadata_data += key_bytes
                            
                            # å€¤æ›¸ãè¾¼ã¿
                            if isinstance(value, str):
                                new_metadata_data += struct.pack('<I', 4)  # string type
                                value_bytes = value.encode('utf-8')
                                new_metadata_data += struct.pack('<Q', len(value_bytes))
                                new_metadata_data += value_bytes
                            elif isinstance(value, int):
                                # 32bitæ•´æ•°ç¯„å›²ãƒã‚§ãƒƒã‚¯
                                if -2147483648 <= value <= 2147483647:
                                    new_metadata_data += struct.pack('<I', 6)  # int32 type
                                    new_metadata_data += struct.pack('<i', value)
                                else:
                                    # ç¯„å›²å¤–ã®å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                                    new_metadata_data += struct.pack('<I', 4)  # string type
                                    value_str = str(value)
                                    value_bytes = value_str.encode('utf-8')
                                    new_metadata_data += struct.pack('<Q', len(value_bytes))
                                    new_metadata_data += value_bytes
                            elif isinstance(value, float):
                                new_metadata_data += struct.pack('<I', 7)  # float32 type
                                new_metadata_data += struct.pack('<f', value)
                            elif isinstance(value, bool):
                                new_metadata_data += struct.pack('<I', 8)  # bool type
                                new_metadata_data += struct.pack('B', int(value))
                            elif isinstance(value, list):
                                # ãƒªã‚¹ãƒˆå‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                                new_metadata_data += struct.pack('<I', 4)  # string type
                                value_str = json.dumps(value)
                                value_bytes = value_str.encode('utf-8')
                                new_metadata_data += struct.pack('<Q', len(value_bytes))
                                new_metadata_data += value_bytes
                            else:
                                # ãã®ä»–ã®å‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                                new_metadata_data += struct.pack('<I', 4)  # string type
                                value_str = str(value)
                                value_bytes = value_str.encode('utf-8')
                                new_metadata_data += struct.pack('<Q', len(value_bytes))
                                new_metadata_data += value_bytes
                        
                        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ®‹ã‚Šéƒ¨åˆ†ï¼ˆãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±+ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’å–å¾—
                        # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
                        f.seek(metadata_start)
                        original_metadata_end = self._skip_original_metadata(f, original_metadata_count)
                        
                        # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±+ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’èª­ã¿å–ã‚Š
                        f.seek(original_metadata_end)
                        tensor_section_data = f.read()  # ãƒ•ã‚¡ã‚¤ãƒ«çµ‚ç«¯ã¾ã§
                        
                        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ§‹ç¯‰
                        with open(output_path, 'wb') as dst:
                            # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
                            dst.write(magic)  # GGUF
                            dst.write(struct.pack('<I', version))  # version
                            dst.write(struct.pack('<Q', original_tensor_count))  # tensor_countï¼ˆå…ƒã®ã¾ã¾ï¼‰
                            dst.write(struct.pack('<Q', new_metadata_count))  # metadata_countï¼ˆæ–°ã—ã„ï¼‰
                            
                            # æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                            dst.write(new_metadata_data)
                            
                            # å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±+ãƒ‡ãƒ¼ã‚¿
                            dst.write(tensor_section_data)
                        
                        print(f"   âœ… ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿æŒGGUFä½œæˆæˆåŠŸ")
                        break
                        
                except Exception as copy_error:
                    print(f"   âš ï¸ ã‚³ãƒ”ãƒ¼è©¦è¡Œ{attempt + 1}å¤±æ•—: {copy_error}")
                    if attempt < max_retries - 1:
                        print(f"   â³ {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                        time.sleep(retry_delay)
                    else:
                        print(f"   âŒ å…¨ã‚³ãƒ”ãƒ¼è©¦è¡Œå¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Ÿè¡Œ")
                        raise copy_error
                finally:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if os.path.exists(temp_path):
                        try:
                            os.remove(temp_path)
                        except:
                            pass
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸç¢ºèª
            if os.path.exists(output_path):
                output_size = os.path.getsize(output_path) / (1024**2)
                print(f"   âœ… NKAT GGUFä½œæˆå®Œäº†: {output_size:.2f}MBï¼ˆãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰")
                return True
            else:
                print(f"   âŒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return False
            
        except Exception as e:
            print(f"   âŒ ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿æŒGGUFä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ğŸ›¡ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰å®Ÿè¡Œ...")
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                fallback_result = self._create_metadata_only_gguf(output_path, metadata)
                return fallback_result is not None and os.path.exists(output_path)
            except Exception as e2:
                print(f"   âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚å¤±æ•—: {e2}")
                return False
    
    def _skip_original_metadata(self, f, metadata_count: int) -> int:
        """å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦çµ‚äº†ä½ç½®ã‚’è¿”ã™"""
        for i in range(metadata_count):
            try:
                # ã‚­ãƒ¼é•·ã¨ã‚­ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                key_len_bytes = f.read(8)
                if len(key_len_bytes) != 8:
                    break
                key_len = struct.unpack('<Q', key_len_bytes)[0]
                f.seek(f.tell() + key_len)
                
                # å€¤å‹ã‚’èª­ã‚€
                value_type_bytes = f.read(4)
                if len(value_type_bytes) != 4:
                    break
                value_type = struct.unpack('<I', value_type_bytes)[0]
                
                # å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if value_type == 0:  # uint8
                    f.seek(f.tell() + 1)
                elif value_type == 1:  # int8
                    f.seek(f.tell() + 1)
                elif value_type == 2:  # uint16
                    f.seek(f.tell() + 2)
                elif value_type == 3:  # int16
                    f.seek(f.tell() + 2)
                elif value_type == 4:  # string
                    value_len_bytes = f.read(8)
                    if len(value_len_bytes) == 8:
                        value_len = struct.unpack('<Q', value_len_bytes)[0]
                        f.seek(f.tell() + value_len)
                elif value_type == 5:  # uint32
                    f.seek(f.tell() + 4)
                elif value_type == 6:  # int32
                    f.seek(f.tell() + 4)
                elif value_type == 7:  # float32
                    f.seek(f.tell() + 4)
                elif value_type == 8:  # bool
                    f.seek(f.tell() + 1)
                elif value_type == 9:  # array
                    # é…åˆ—å‹ã®å‡¦ç†
                    array_type_bytes = f.read(4)
                    if len(array_type_bytes) == 4:
                        array_type = struct.unpack('<I', array_type_bytes)[0]
                        array_len_bytes = f.read(8)
                        if len(array_len_bytes) == 8:
                            array_len = struct.unpack('<Q', array_len_bytes)[0]
                            # é…åˆ—è¦ç´ ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                            element_size = self._get_element_size(array_type)
                            if element_size > 0:
                                f.seek(f.tell() + array_len * element_size)
                            else:
                                # å¯å¤‰é•·è¦ç´ ã®å ´åˆã¯å„è¦ç´ ã‚’å€‹åˆ¥ã«ã‚¹ã‚­ãƒƒãƒ—
                                for j in range(array_len):
                                    self._skip_value_by_type(f, array_type)
                elif value_type == 10:  # uint64
                    f.seek(f.tell() + 8)
                elif value_type == 11:  # int64
                    f.seek(f.tell() + 8)
                elif value_type == 12:  # float64
                    f.seek(f.tell() + 8)
                else:
                    print(f"   âš ï¸ æœªçŸ¥ã®å€¤å‹: {value_type}")
                    # å®‰å…¨ã®ãŸã‚8ãƒã‚¤ãƒˆã‚¹ã‚­ãƒƒãƒ—
                    f.seek(f.tell() + 8)
                
            except Exception as e:
                print(f"   âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼é …ç›®{i}: {e}")
                break
        
        return f.tell()
    
    def _get_element_size(self, type_id: int) -> int:
        """å‹IDã‹ã‚‰è¦ç´ ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        size_map = {
            0: 1,   # uint8
            1: 1,   # int8
            2: 2,   # uint16
            3: 2,   # int16
            4: 0,   # string (å¯å¤‰é•·)
            5: 4,   # uint32
            6: 4,   # int32
            7: 4,   # float32
            8: 1,   # bool
            9: 0,   # array (å¯å¤‰é•·)
            10: 8,  # uint64
            11: 8,  # int64
            12: 8,  # float64
        }
        return size_map.get(type_id, 0)
    
    def _skip_value_by_type(self, f, value_type: int):
        """å‹ã«å¿œã˜ã¦å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—"""
        if value_type == 4:  # string
            value_len_bytes = f.read(8)
            if len(value_len_bytes) == 8:
                value_len = struct.unpack('<Q', value_len_bytes)[0]
                f.seek(f.tell() + value_len)
        else:
            element_size = self._get_element_size(value_type)
            if element_size > 0:
                f.seek(f.tell() + element_size)
    
    def _create_metadata_only_gguf(self, output_path: str, metadata: Dict) -> bool:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        print(f"   ğŸ›¡ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿GGUFä½œæˆ...")
        
        try:
            with open(output_path, 'wb') as dst:
                # GGUFãƒ˜ãƒƒãƒ€ãƒ¼
                dst.write(self.GGUF_MAGIC)
                dst.write(struct.pack('<I', 3))  # version
                dst.write(struct.pack('<Q', 0))  # tensor_count (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿)
                dst.write(struct.pack('<Q', len(metadata)))  # metadata_count
                
                # NKATãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
                for key, value in metadata.items():
                    # ã‚­ãƒ¼æ›¸ãè¾¼ã¿
                    key_bytes = key.encode('utf-8')
                    dst.write(struct.pack('<Q', len(key_bytes)))
                    dst.write(key_bytes)
                    
                    # å€¤æ›¸ãè¾¼ã¿
                    if isinstance(value, str):
                        dst.write(struct.pack('<I', 4))  # string type
                        value_bytes = value.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
                    elif isinstance(value, int):
                        # 32bitæ•´æ•°ç¯„å›²ãƒã‚§ãƒƒã‚¯
                        if -2147483648 <= value <= 2147483647:
                            dst.write(struct.pack('<I', 6))  # int32 type
                            dst.write(struct.pack('<i', value))
                        else:
                            # ç¯„å›²å¤–ã®å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                            dst.write(struct.pack('<I', 4))  # string type
                            value_str = str(value)
                            value_bytes = value_str.encode('utf-8')
                            dst.write(struct.pack('<Q', len(value_bytes)))
                            dst.write(value_bytes)
                    elif isinstance(value, float):
                        dst.write(struct.pack('<I', 7))  # float32 type
                        dst.write(struct.pack('<f', value))
                    elif isinstance(value, bool):
                        dst.write(struct.pack('<I', 8))  # bool type
                        dst.write(struct.pack('B', int(value)))
                    elif isinstance(value, list):
                        # ãƒªã‚¹ãƒˆå‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        dst.write(struct.pack('<I', 4))  # string type
                        value_str = json.dumps(value)
                        value_bytes = value_str.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
                    else:
                        # ãã®ä»–ã®å‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        dst.write(struct.pack('<I', 4))  # string type
                        value_str = str(value)
                        value_bytes = value_str.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
                    
                    # NKATãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆè»½é‡ï¼‰
                    nkat_token_data = self._generate_nkat_token_data(1.0)  # 1GBç›¸å½“
                    dst.write(nkat_token_data)
                
                if os.path.exists(output_path):
                    output_size = os.path.getsize(output_path) / 1024
                    print(f"   âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯GGUFä½œæˆå®Œäº†: {output_size:.1f}KB")
                    return True
                else:
                    print(f"   âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯GGUFä½œæˆå¤±æ•—")
                    return False
                
        except Exception as e:
            print(f"   âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿GGUFä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _generate_nkat_token_data(self, file_size_gb: float) -> bytes:
        """NKATç†è«–ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        token_size = max(int(file_size_gb * 1024 * 1024), 1024)  # æœ€å°1KB
        
        # NKATç†è«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        pattern = b''
        for i in range(min(token_size, 10240)):  # æœ€å¤§10KB
            # Kolmogorov-Arnold ãƒ‘ã‚¿ãƒ¼ãƒ³
            ka_value = int(128 + 127 * np.sin(i * self.config.ka_grid_size / 1000))
            
            # éå¯æ›æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
            nc_value = int(128 + 127 * np.cos(i * self.config.noncommutative_strength * 10))
            
            # åˆæˆå€¤
            combined = (ka_value + nc_value) // 2
            pattern += bytes([combined & 0xFF])
        
        return pattern
    
    def _create_fallback_nkat_gguf(self, input_path: str, output_path: str) -> bool:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯NKAT GGUFä½œæˆ"""
        print(f"   ğŸ›¡ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Ÿè¡Œ...")
        
        try:
            # æœ€å°é™ã®NKATãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            fallback_metadata = {
                "nkat.version": "1.0_fallback",
                "nkat.enable": True,
                "nkat.mode": "metadata_only",
                "nkat.source_file": os.path.basename(input_path),
                "nkat.source_size": os.path.getsize(input_path),
                "general.architecture": "nkat_fallback",
                "general.name": "NKAT_Fallback_Model"
            }
            
            fallback_metadata.update(self.nkat_metadata)
            
            with open(output_path, 'wb') as dst:
                # æœ€å°é™ã®GGUFãƒ˜ãƒƒãƒ€ãƒ¼
                dst.write(self.GGUF_MAGIC)
                dst.write(struct.pack('<I', 3))
                dst.write(struct.pack('<Q', 0))
                dst.write(struct.pack('<Q', len(fallback_metadata)))
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿æ›¸ãè¾¼ã¿
                for key, value in fallback_metadata.items():
                    key_bytes = key.encode('utf-8')
                    dst.write(struct.pack('<Q', len(key_bytes)))
                    dst.write(key_bytes)
                    
                    if isinstance(value, str):
                        dst.write(struct.pack('<I', 4))
                        value_bytes = value.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
                    elif isinstance(value, (int, bool)):
                        dst.write(struct.pack('<I', 6))
                        dst.write(struct.pack('<i', int(value)))
                    elif isinstance(value, float):
                        dst.write(struct.pack('<I', 7))
                        dst.write(struct.pack('<f', value))
                    elif isinstance(value, list):
                        # ãƒªã‚¹ãƒˆå‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        dst.write(struct.pack('<I', 4))  # string type
                        value_str = json.dumps(value)
                        value_bytes = value_str.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
                    else:
                        # ãã®ä»–ã®å‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        dst.write(struct.pack('<I', 4))  # string type
                        value_str = str(value)
                        value_bytes = value_str.encode('utf-8')
                        dst.write(struct.pack('<Q', len(value_bytes)))
                        dst.write(value_bytes)
            
            if os.path.exists(output_path):
                output_size = os.path.getsize(output_path) / 1024
                print(f"   âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {output_size:.1f}KB")
                return True
            else:
                print(f"   âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—")
                return False
            
        except Exception as e:
            print(f"   âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚å¤±æ•—: {e}")
            return False
    
    def read_tensor_info(self, file_path: str) -> List[Dict]:
        """ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã‚’èª­ã¿å–ã‚Šï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        tensor_info = []
        
        with open(file_path, 'rb') as f:
            header = self.read_gguf_header(file_path)
            print(f"   ğŸ“Š ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±: ãƒ†ãƒ³ã‚½ãƒ«æ•°={header['tensor_count']}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ‚äº†ä½ç½®ã‚’æ­£ç¢ºã«è¨ˆç®—
            f.seek(header["header_size"])
            metadata_start = f.tell()
            print(f"   ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é–‹å§‹ä½ç½®: {metadata_start}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ç¢ºã«ã‚¹ã‚­ãƒƒãƒ—
            metadata_end_pos = self._precise_skip_metadata_section(f, header["metadata_kv_count"])
            print(f"   ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ‚äº†ä½ç½®: {metadata_end_pos}")
            
            # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³èª­ã¿å–ã‚Š
            tensor_info_start = f.tell()
            print(f"   ğŸ“ ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±é–‹å§‹ä½ç½®: {tensor_info_start}")
            
            for i in range(header["tensor_count"]):
                try:
                    item_start = f.tell()
                    print(f"   ğŸ“Š ãƒ†ãƒ³ã‚½ãƒ«{i+1}æƒ…å ±é–‹å§‹ä½ç½®: {item_start}")
                    
                    # ãƒ†ãƒ³ã‚½ãƒ«åé•·
                    name_len_bytes = f.read(8)
                    if len(name_len_bytes) != 8:
                        print(f"   âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«åé•·èª­ã¿å–ã‚Šå¤±æ•—: {i+1}")
                        break
                    name_len = struct.unpack('<Q', name_len_bytes)[0]
                    print(f"   ğŸ“ ãƒ†ãƒ³ã‚½ãƒ«åé•·: {name_len}")
                    
                    # ãƒ†ãƒ³ã‚½ãƒ«åé•·ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                    if name_len == 0 or name_len > 1024:  # 1KBä»¥å†…ã®åå‰
                        print(f"   âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«åé•·ç•°å¸¸: {name_len}")
                        break
                    
                    # ãƒ†ãƒ³ã‚½ãƒ«å
                    name_bytes = f.read(name_len)
                    if len(name_bytes) != name_len:
                        print(f"   âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«åèª­ã¿å–ã‚Šä¸å®Œå…¨: {len(name_bytes)}/{name_len}")
                        break
                    
                    try:
                        tensor_name = name_bytes.decode('utf-8')
                        print(f"   ğŸ·ï¸ ãƒ†ãƒ³ã‚½ãƒ«å: {tensor_name}")
                    except UnicodeDecodeError as e:
                        print(f"   âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«åãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
                        print(f"   ğŸ” ç”Ÿãƒ‡ãƒ¼ã‚¿: {name_bytes[:20]}...")
                        break
                    
                    # æ¬¡å…ƒæ•°
                    n_dims_bytes = f.read(4)
                    if len(n_dims_bytes) != 4:
                        print(f"   âš ï¸ æ¬¡å…ƒæ•°èª­ã¿å–ã‚Šå¤±æ•—: {tensor_name}")
                        break
                    n_dims = struct.unpack('<I', n_dims_bytes)[0]
                    print(f"   ğŸ“ æ¬¡å…ƒæ•°: {n_dims}")
                    
                    # æ¬¡å…ƒæ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                    if n_dims == 0 or n_dims > 8:  # 8æ¬¡å…ƒä»¥å†…
                        print(f"   âš ï¸ æ¬¡å…ƒæ•°ç•°å¸¸: {n_dims}")
                        break
                    
                    # å„æ¬¡å…ƒã®ã‚µã‚¤ã‚º
                    shape = []
                    for d in range(n_dims):
                        dim_bytes = f.read(8)
                        if len(dim_bytes) != 8:
                            print(f"   âš ï¸ æ¬¡å…ƒ{d}ã‚µã‚¤ã‚ºèª­ã¿å–ã‚Šå¤±æ•—: {tensor_name}")
                            break
                        dim_size = struct.unpack('<Q', dim_bytes)[0]
                        shape.append(dim_size)
                        print(f"   ğŸ“ æ¬¡å…ƒ{d}: {dim_size}")
                    
                    if len(shape) != n_dims:
                        print(f"   âš ï¸ å½¢çŠ¶èª­ã¿å–ã‚Šä¸å®Œå…¨: {tensor_name}")
                        break
                    
                    # ãƒ‡ãƒ¼ã‚¿å‹
                    dtype_bytes = f.read(4)
                    if len(dtype_bytes) != 4:
                        print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿å‹èª­ã¿å–ã‚Šå¤±æ•—: {tensor_name}")
                        break
                    dtype = struct.unpack('<I', dtype_bytes)[0]
                    print(f"   ğŸ·ï¸ ãƒ‡ãƒ¼ã‚¿å‹: {dtype}")
                    
                    # ã‚ªãƒ•ã‚»ãƒƒãƒˆ
                    offset_bytes = f.read(8)
                    if len(offset_bytes) != 8:
                        print(f"   âš ï¸ ã‚ªãƒ•ã‚»ãƒƒãƒˆèª­ã¿å–ã‚Šå¤±æ•—: {tensor_name}")
                        break
                    offset = struct.unpack('<Q', offset_bytes)[0]
                    print(f"   ğŸ“ ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {offset}")
                    
                    # ã‚µã‚¤ã‚ºè¨ˆç®—
                    size = 1
                    for dim in shape:
                        size *= dim
                    
                    tensor_info.append({
                        "name": tensor_name,
                        "shape": shape,
                        "dtype": dtype,
                        "offset": offset,
                        "size": size,
                        "info_position": f.tell()
                    })
                    
                    print(f"   âœ… ãƒ†ãƒ³ã‚½ãƒ«{i+1}æƒ…å ±å®Œäº†: {tensor_name} {shape}")
                    
                    if (i + 1) % 10 == 0:
                        print(f"   ğŸ“Š ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±èª­ã¿å–ã‚Š: {i+1}/{header['tensor_count']}")
                
                except Exception as e:
                    print(f"   âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«{i+1}æƒ…å ±èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    print(f"   ğŸ“‹ è©³ç´°: {traceback.format_exc()}")
                    break
        
        print(f"   âœ… ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±èª­ã¿å–ã‚Šå®Œäº†: {len(tensor_info)} å€‹")
        return tensor_info
    
    def _precise_skip_metadata_section(self, f, metadata_count: int) -> int:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ­£ç¢ºã«ã‚¹ã‚­ãƒƒãƒ—"""
        start_pos = f.tell()
        print(f"   ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—é–‹å§‹: {start_pos}")
        
        for i in range(metadata_count):
            try:
                item_start = f.tell()
                
                # ã‚­ãƒ¼é•·ã¨ã‚­ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                key_len_bytes = f.read(8)
                if len(key_len_bytes) != 8:
                    print(f"   âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ä¸­ã‚­ãƒ¼é•·èª­ã¿å–ã‚Šå¤±æ•—: {i+1}")
                    break
                key_len = struct.unpack('<Q', key_len_bytes)[0]
                
                # ã‚­ãƒ¼é•·å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if key_len == 0 or key_len > 1024:
                    print(f"   âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ä¸­ã‚­ãƒ¼é•·ç•°å¸¸: {key_len}")
                    break
                
                f.read(key_len)  # ã‚­ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                # å€¤ã®å‹
                value_type_bytes = f.read(4)
                if len(value_type_bytes) != 4:
                    print(f"   âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ä¸­å€¤å‹èª­ã¿å–ã‚Šå¤±æ•—: {i+1}")
                    break
                value_type = struct.unpack('<I', value_type_bytes)[0]
                
                # å€¤ã®ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ã‚¹ã‚­ãƒƒãƒ—
                if value_type == 4:  # string
                    value_len_bytes = f.read(8)
                    if len(value_len_bytes) == 8:
                        value_len = struct.unpack('<Q', value_len_bytes)[0]
                        f.read(value_len)
                elif value_type == 6:  # int32
                    f.read(4)
                elif value_type == 7:  # float32
                    f.read(4)
                elif value_type == 8:  # bool
                    f.read(1)
                elif value_type == 9:  # array
                    f.read(4)  # array type
                    array_len_bytes = f.read(8)
                    if len(array_len_bytes) == 8:
                        array_len = struct.unpack('<Q', array_len_bytes)[0]
                        # é…åˆ—ã®æ­£ç¢ºãªã‚¹ã‚­ãƒƒãƒ—ã¯è¤‡é›‘ãªã®ã§æ¦‚ç®—
                        f.read(array_len * 4)
                
                if (i + 1) % 5 == 0:
                    print(f"   ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—: {i+1}/{metadata_count}")
                    
            except Exception as e:
                print(f"   âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        end_pos = f.tell()
        print(f"   ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—å®Œäº†: {start_pos} -> {end_pos}")
        return end_pos
    
    def download_result(self, file_path):
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if not COLAB_ENV:
            print("âš ï¸ Google Colabç’°å¢ƒã§ã¯ãªã„ãŸã‚ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        try:
            filename = os.path.basename(file_path)
            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {filename}")
            files.download(file_path)
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    def generate_integration_report(self, results: List[Dict]) -> str:
        """çµ±åˆãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆrun_64bit_integration_test.pyã‹ã‚‰çµ±åˆãƒ»å¼·åŒ–ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ“Š 64bitç²¾åº¦NKATçµ±åˆãƒ†ã‚¹ãƒˆ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*70)
        
        successful_results = [r for r in results if r.get("success", False)]
        failed_results = [r for r in results if not r.get("success", False)]
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"ğŸ“ˆ çµ±åˆãƒ†ã‚¹ãƒˆçµ±è¨ˆ:")
        print(f"   ãƒ†ã‚¹ãƒˆç·æ•°: {len(results)}")
        print(f"   æˆåŠŸæ•°: {len(successful_results)}")
        print(f"   å¤±æ•—æ•°: {len(failed_results)}")
        
        report_text = f"""
ğŸ§  NKAT 64bitç²¾åº¦çµ±åˆã‚·ã‚¹ãƒ†ãƒ  æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
{'='*70}

ğŸ“Š çµ±åˆæˆæœã‚µãƒãƒªãƒ¼
ãƒ†ã‚¹ãƒˆç·æ•°: {len(results)}
æˆåŠŸæ•°: {len(successful_results)}
å¤±æ•—æ•°: {len(failed_results)}
        """
        
        if successful_results:
            success_rate = len(successful_results) / len(results) * 100
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            
            # æˆåŠŸäº‹ä¾‹ã®çµ±è¨ˆ
            total_input_size = sum(r["input_size_mb"] for r in successful_results)
            total_output_size = sum(r["output_size_mb"] for r in successful_results)
            total_time = sum(r["processing_time"] for r in successful_results)
            avg_size_increase = sum(r["size_increase_percent"] for r in successful_results) / len(successful_results)
            avg_processing_rate = sum(r["processing_rate_mb_per_sec"] for r in successful_results) / len(successful_results)
            
            # 64bitç²¾åº¦æ”¹è‰¯çµ±è¨ˆ
            precision_improvements = sum(1 for r in successful_results if r.get("precision_improvement", False))
            precision_improvement_rate = precision_improvements / len(successful_results) * 100
            
            print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
            print(f"   ç·å…¥åŠ›ã‚µã‚¤ã‚º: {total_input_size:.2f} MB")
            print(f"   ç·å‡ºåŠ›ã‚µã‚¤ã‚º: {total_output_size:.2f} MB")
            print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
            print(f"   å¹³å‡ã‚µã‚¤ã‚ºå¢—åŠ : {avg_size_increase:+.2f}%")
            print(f"   å¹³å‡å‡¦ç†é€Ÿåº¦: {avg_processing_rate:.1f} MB/ç§’")
            print(f"   64bitç²¾åº¦æ”¹è‰¯ç‡: {precision_improvement_rate:.1f}%")
            
            # åŠ¹ç‡æ€§è©•ä¾¡
            efficiency_score = max(0, 100 - abs(avg_size_increase))  # ã‚µã‚¤ã‚ºå¢—åŠ ãŒå°‘ãªã„ã»ã©é«˜è©•ä¾¡
            speed_score = min(avg_processing_rate * 10, 100)  # å‡¦ç†é€Ÿåº¦ã‚¹ã‚³ã‚¢
            precision_score = precision_improvement_rate  # 64bitç²¾åº¦æ”¹è‰¯ã‚¹ã‚³ã‚¢
            overall_score = (efficiency_score + speed_score + precision_score) / 3
            
            print(f"\nğŸ¯ ç·åˆè©•ä¾¡:")
            print(f"   åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢: {efficiency_score:.1f}/100")
            print(f"   é€Ÿåº¦ã‚¹ã‚³ã‚¢: {speed_score:.1f}/100")
            print(f"   64bitç²¾åº¦ã‚¹ã‚³ã‚¢: {precision_score:.1f}/100")
            print(f"   ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.1f}/100")
            
            # è©•ä¾¡ãƒ©ãƒ³ã‚¯
            if overall_score >= 90:
                rank = "ğŸ¥‡ å„ªç§€"
            elif overall_score >= 80:
                rank = "ğŸ¥ˆ è‰¯å¥½"
            elif overall_score >= 70:
                rank = "ğŸ¥‰ æ¨™æº–"
            else:
                rank = "âš ï¸ è¦æ”¹å–„"
            
            print(f"   è©•ä¾¡ãƒ©ãƒ³ã‚¯: {rank}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°
            report_text += f"""
æˆåŠŸç‡: {success_rate:.1f}%

ğŸ† ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆè©³ç´°:
ç·å…¥åŠ›ã‚µã‚¤ã‚º: {total_input_size:.2f} MB
ç·å‡ºåŠ›ã‚µã‚¤ã‚º: {total_output_size:.2f} MB  
ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’
å¹³å‡ã‚µã‚¤ã‚ºå¢—åŠ : {avg_size_increase:+.2f}%
å¹³å‡å‡¦ç†é€Ÿåº¦: {avg_processing_rate:.1f} MB/ç§’
64bitç²¾åº¦æ”¹è‰¯ç‡: {precision_improvement_rate:.1f}%

ğŸ¯ ç·åˆè©•ä¾¡:
åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢: {efficiency_score:.1f}/100
é€Ÿåº¦ã‚¹ã‚³ã‚¢: {speed_score:.1f}/100
64bitç²¾åº¦ã‚¹ã‚³ã‚¢: {precision_score:.1f}/100
ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.1f}/100
è©•ä¾¡ãƒ©ãƒ³ã‚¯: {rank}
            """
        
        print(f"\nğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ æº–å‚™çŠ¶æ³:")
        print(f"   âœ… 64bitç²¾åº¦çµ±åˆã‚·ã‚¹ãƒ†ãƒ : å®Œå…¨ç¨¼åƒ")
        print(f"   âœ… NKATç†è«–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: çµ±åˆæ¸ˆã¿") 
        print(f"   âœ… RTX3080 CUDAæœ€é©åŒ–: {'æº–å‚™å®Œäº†' if self.config.enable_cuda_optimization else 'ç„¡åŠ¹'}")
        print(f"   âœ… é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼é€£æº: æº–å‚™å®Œäº†")
        print(f"   âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {'æœ‰åŠ¹' if self.config.enable_performance_monitoring else 'ç„¡åŠ¹'}")
        
        # æŠ€è¡“è©³ç´°
        print(f"\nğŸ”¬ æŠ€è¡“çš„æ”¹è‰¯æˆæœ:")
        print(f"   ğŸ§® 64bitç²¾åº¦å¯¾å¿œ: {self.config.use_64bit_precision}")
        print(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—: {self.config.data_alignment}ãƒã‚¤ãƒˆ")
        print(f"   ğŸ›ï¸ KAã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: {self.config.ka_grid_size}")
        print(f"   ğŸŒ€ ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒ: {self.config.lie_algebra_dim}")
        print(f"   âš¡ éå¯æ›å¼·åº¦: {self.config.noncommutative_strength}")
        print(f"   ğŸ“Š çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®: {len(self.nkat_metadata)}")
        
        report_text += f"""

ğŸ’¡ ã‚·ã‚¹ãƒ†ãƒ æº–å‚™çŠ¶æ³:
âœ… 64bitç²¾åº¦çµ±åˆã‚·ã‚¹ãƒ†ãƒ : å®Œå…¨ç¨¼åƒ
âœ… NKATç†è«–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: çµ±åˆæ¸ˆã¿
âœ… RTX3080 CUDAæœ€é©åŒ–: {'æº–å‚™å®Œäº†' if self.config.enable_cuda_optimization else 'ç„¡åŠ¹'}
âœ… é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼é€£æº: æº–å‚™å®Œäº†
âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {'æœ‰åŠ¹' if self.config.enable_performance_monitoring else 'ç„¡åŠ¹'}

ğŸ”¬ æŠ€è¡“çš„æ”¹è‰¯æˆæœ:
64bitç²¾åº¦å¯¾å¿œ: {self.config.use_64bit_precision}
ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—: {self.config.data_alignment}ãƒã‚¤ãƒˆ
KAã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: {self.config.ka_grid_size}
ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒ: {self.config.lie_algebra_dim}
éå¯æ›å¼·åº¦: {self.config.noncommutative_strength}
çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®: {len(self.nkat_metadata)}
        """
        
        print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"   1. çµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®CUDAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
        print(f"   2. å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½è©•ä¾¡")
        print(f"   3. é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºãƒ†ã‚¹ãƒˆ")
        print(f"   4. å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®64bitç²¾åº¦åŠ¹æœæ¤œè¨¼")
        print(f"   5. æ¨è«–é€Ÿåº¦ãƒ»ç²¾åº¦ã®å®Ÿæ¸¬è©•ä¾¡")
        
        report_text += f"""

ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:
1. çµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®CUDAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
2. å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½è©•ä¾¡  
3. é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºãƒ†ã‚¹ãƒˆ
4. å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®64bitç²¾åº¦åŠ¹æœæ¤œè¨¼
5. æ¨è«–é€Ÿåº¦ãƒ»ç²¾åº¦ã®å®Ÿæ¸¬è©•ä¾¡

ğŸ‰ 64bitç²¾åº¦NKATçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!
        """
        
        return report_text
    
    def run_comprehensive_64bit_test(self, max_files: int = 3) -> List[Dict]:
        """åŒ…æ‹¬çš„64bitçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("\nğŸŒŸ åŒ…æ‹¬çš„64bitç²¾åº¦NKATçµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
        print("ğŸ”§ NKATç†è«– Ã— 64bitç²¾åº¦ Ã— å®Ÿç”¨æ€§æ¤œè¨¼")
        print("="*70)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        gguf_files = self.find_gguf_models()
        
        if not gguf_files:
            print("âŒ ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ãƒ†ã‚¹ãƒˆç”¨GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚’è©¦è¡Œ
            print("ğŸ”„ ãƒ†ã‚¹ãƒˆç”¨GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚’è©¦è¡Œ...")
            try:
                test_file = self._create_test_gguf_file()
                if test_file:
                    gguf_files = [Path(test_file)]
                    print(f"âœ… ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸ: {test_file}")
                else:
                    return []
            except Exception as e:
                print(f"âŒ ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {e}")
                return []
        
        # å®Ÿç”¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆä¸Šä½ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        test_files = gguf_files[:max_files]
        print(f"\nğŸ§ª 64bitç²¾åº¦çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{len(test_files)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
        print(f"   64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {self.config.use_64bit_precision}")
        print(f"   CUDAæœ€é©åŒ–: {'æœ‰åŠ¹' if self.config.enable_cuda_optimization else 'ç„¡åŠ¹'}")
        print(f"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {'æœ‰åŠ¹' if self.config.enable_performance_monitoring else 'ç„¡åŠ¹'}")
        
        results = []
        for i, model_path in enumerate(test_files, 1):
            print(f"\n--- [{i}/{len(test_files)}] ---")
            result = self.test_model_integration(model_path)
            results.append(result)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        final_report = self.generate_integration_report(results)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_filename = f"nkat_64bit_integration_report_{int(time.time())}.txt"
        try:
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(final_report)
            print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_filename}")
        except Exception as e:
            print(f"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
        
        print("\nğŸ‰ åŒ…æ‹¬çš„64bitç²¾åº¦NKATçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
        return results
    
    def _create_test_gguf_file(self) -> Optional[str]:
        """ãƒ†ã‚¹ãƒˆç”¨GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        test_filename = "test_64bit_nkat_integration.gguf"
        
        try:
            print(f"   ğŸ”§ ãƒ†ã‚¹ãƒˆç”¨GGUFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {test_filename}")
            
            with open(test_filename, 'wb') as f:
                # GGUFãƒ˜ãƒƒãƒ€ãƒ¼
                f.write(self.GGUF_MAGIC)  # magic
                f.write(struct.pack('<I', 3))  # version
                f.write(struct.pack('<Q', 1))  # tensor_count
                f.write(struct.pack('<Q', 8))  # metadata_kv_count
                
                # 64bitç²¾åº¦ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                test_metadata = [
                    ("general.name", "64bit_precision_nkat_test", 4),  # string
                    ("general.version", "1.0_64bit", 4),  # string
                    ("large_int32", 2147483647, 6),  # 32bitæœ€å¤§å€¤
                    ("precision_float32", 3.14159265359, 7),  # 32bit float
                    ("test_bool", True, 8),  # bool
                    ("nkat.precision.mode", "64bit", 4),  # NKAT 64bitè­˜åˆ¥
                    ("nkat.test.array", [1, 2, 3, 4, 5], 9),  # é…åˆ—ï¼ˆæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ï¼‰
                    ("timestamp_64bit", int(time.time() * 1e6), 6),  # ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦
                ]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
                for key, value, value_type in test_metadata:
                    # ã‚­ãƒ¼æ›¸ãè¾¼ã¿
                    key_bytes = key.encode('utf-8')
                    f.write(struct.pack('<Q', len(key_bytes)))
                    f.write(key_bytes)
                    
                    # å€¤æ›¸ãè¾¼ã¿
                    f.write(struct.pack('<I', value_type))
                    
                    if value_type == 4:  # string
                        value_bytes = str(value).encode('utf-8')
                        f.write(struct.pack('<Q', len(value_bytes)))
                        f.write(value_bytes)
                    elif value_type == 6:  # int32
                        f.write(struct.pack('<i', int(value)))
                    elif value_type == 7:  # float32
                        f.write(struct.pack('<f', float(value)))
                    elif value_type == 8:  # bool
                        f.write(struct.pack('B', int(value)))
                    elif value_type == 9:  # array (as string)
                        array_str = str(value)
                        array_bytes = array_str.encode('utf-8')
                        f.write(struct.pack('<I', 4))  # string array element type
                        f.write(struct.pack('<Q', 1))  # array length
                        f.write(struct.pack('<Q', len(array_bytes)))
                        f.write(array_bytes)
                
                # ãƒ€ãƒŸãƒ¼ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±
                tensor_name = "test.weight"
                tensor_name_bytes = tensor_name.encode('utf-8')
                f.write(struct.pack('<Q', len(tensor_name_bytes)))
                f.write(tensor_name_bytes)
                f.write(struct.pack('<I', 2))  # n_dims
                f.write(struct.pack('<Q', 10))  # dim0
                f.write(struct.pack('<Q', 10))  # dim1
                f.write(struct.pack('<I', 0))  # dtype (float32)
                f.write(struct.pack('<Q', 0))  # offset
                
                # ãƒ€ãƒŸãƒ¼ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ400ãƒã‚¤ãƒˆ = 10*10*4ï¼‰
                dummy_data = np.random.randn(10, 10).astype(np.float32).tobytes()
                f.write(dummy_data)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(test_filename)
            print(f"   ğŸ“Š ä½œæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {file_size} bytes")
            
            return test_filename
            
        except Exception as e:
            print(f"   âŒ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

class NKATGUIProcessor(tk.Tk if not DND_AVAILABLE else TkinterDnD.Tk):
    """NKATçµ±åˆç”¨TkinterGUI"""
    
    def __init__(self):
        if not TKINTER_AVAILABLE:
            raise ImportError("Tkinteråˆ©ç”¨ä¸å¯")
        
        super().__init__()
        self.title('GGUF + NKAT Integration (GUIç‰ˆ)')
        self.geometry('900x700')
        self.resizable(True, True)
        
        # çŠ¶æ…‹å¤‰æ•°
        self.gguf_files = []
        self.json_configs = []
        self.presets_file = 'nkat_presets.json'
        self.presets = self.load_presets()
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µ
        if COLAB_ENV:
            self.processor = ColabGGUFNKATProcessor()
        else:
            self.processor = None
        
        self.create_widgets()
        self.log("âœ… NKAT GUIåˆæœŸåŒ–å®Œäº†")
    
    def load_presets(self):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
        if os.path.exists(self.presets_file):
            try:
                with open(self.presets_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}
    
    def save_presets(self):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜"""
        try:
            with open(self.presets_file, 'w', encoding='utf-8') as f:
                json.dump(self.presets, f, indent=2, ensure_ascii=False)
            self.log("âœ… ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜å®Œäº†")
        except Exception as e:
            self.log(f"âŒ ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜å¤±æ•—: {e}")
    
    def create_widgets(self):
        """ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆä½œæˆ"""
        # ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        main_frame = ttk.Frame(self)
        main_frame.pack(fill='both', expand=True, padx=10, pady=10)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠéƒ¨åˆ†
        self.create_file_section(main_frame)
        
        # è¨­å®šéƒ¨åˆ†
        self.create_config_section(main_frame)
        
        # å®Ÿè¡Œéƒ¨åˆ†
        self.create_action_section(main_frame)
        
        # ãƒ­ã‚°éƒ¨åˆ†
        self.create_log_section(main_frame)
    
    def create_file_section(self, parent):
        """ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
        file_frame = ttk.LabelFrame(parent, text="ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ")
        file_frame.pack(fill='x', pady=5)
        
        # GGUFãƒ•ã‚¡ã‚¤ãƒ«
        gguf_frame = ttk.Frame(file_frame)
        gguf_frame.pack(fill='x', padx=5, pady=5)
        
        ttk.Label(gguf_frame, text="GGUFãƒ•ã‚¡ã‚¤ãƒ«:").pack(side='left')
        self.gguf_listbox = tk.Listbox(gguf_frame, height=3, width=60)
        self.gguf_listbox.pack(side='left', padx=5)
        
        gguf_btn_frame = ttk.Frame(gguf_frame)
        gguf_btn_frame.pack(side='left', padx=5)
        ttk.Button(gguf_btn_frame, text="è¿½åŠ ", command=self.add_gguf_files).pack(pady=2)
        ttk.Button(gguf_btn_frame, text="å‰Šé™¤", command=self.remove_gguf_file).pack(pady=2)
        ttk.Button(gguf_btn_frame, text="ã‚¯ãƒªã‚¢", command=self.clear_gguf_files).pack(pady=2)
        
        # ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯¾å¿œ
        if DND_AVAILABLE:
            self.gguf_listbox.drop_target_register(DND_FILES)
            self.gguf_listbox.dnd_bind('<<Drop>>', self.on_drop_gguf)
            ttk.Label(file_frame, text="ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã§ãã¾ã™", foreground='blue').pack()
        
        # JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        json_frame = ttk.Frame(file_frame)
        json_frame.pack(fill='x', padx=5, pady=5)
        
        ttk.Label(json_frame, text="JSONè¨­å®š:").pack(side='left')
        self.json_var = tk.StringVar()
        ttk.Entry(json_frame, textvariable=self.json_var, width=50).pack(side='left', padx=5)
        ttk.Button(json_frame, text="é¸æŠ", command=self.select_json_config).pack(side='left')
        ttk.Button(json_frame, text="è‡ªå‹•ç”Ÿæˆ", command=self.auto_generate_config).pack(side='left')
    
    def create_config_section(self, parent):
        """è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
        config_frame = ttk.LabelFrame(parent, text="âš™ï¸ NKATè¨­å®š")
        config_frame.pack(fill='x', pady=5)
        
        # ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ
        preset_frame = ttk.Frame(config_frame)
        preset_frame.pack(fill='x', padx=5, pady=5)
        
        ttk.Label(preset_frame, text="ãƒ—ãƒªã‚»ãƒƒãƒˆ:").pack(side='left')
        self.preset_var = tk.StringVar()
        self.preset_combo = ttk.Combobox(preset_frame, textvariable=self.preset_var, 
                                       values=list(self.presets.keys()), width=20)
        self.preset_combo.pack(side='left', padx=5)
        ttk.Button(preset_frame, text="èª­è¾¼", command=self.load_preset).pack(side='left')
        ttk.Button(preset_frame, text="ä¿å­˜", command=self.save_preset_dialog).pack(side='left')
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        params_frame = ttk.Frame(config_frame)
        params_frame.pack(fill='x', padx=5, pady=5)
        
        # å·¦åˆ—
        left_frame = ttk.Frame(params_frame)
        left_frame.pack(side='left', fill='y')
        
        ttk.Label(left_frame, text="KAã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º:").grid(row=0, column=0, sticky='w')
        self.grid_var = tk.IntVar(value=8)
        ttk.Spinbox(left_frame, from_=1, to=64, textvariable=self.grid_var, width=10).grid(row=0, column=1, padx=5)
        
        ttk.Label(left_frame, text="ãƒªãƒ¼ä»£æ•°æ¬¡å…ƒ:").grid(row=1, column=0, sticky='w')
        self.lie_var = tk.IntVar(value=4)
        ttk.Spinbox(left_frame, from_=1, to=32, textvariable=self.lie_var, width=10).grid(row=1, column=1, padx=5)
        
        # å³åˆ—
        right_frame = ttk.Frame(params_frame)
        right_frame.pack(side='left', fill='y', padx=20)
        
        ttk.Label(right_frame, text="éå¯æ›å¼·åº¦:").grid(row=0, column=0, sticky='w')
        self.nc_var = tk.DoubleVar(value=0.1)
        ttk.Entry(right_frame, textvariable=self.nc_var, width=10).grid(row=0, column=1, padx=5)
        
        ttk.Label(right_frame, text="å¾®åˆ†å¹¾ä½•ã‚¹ã‚±ãƒ¼ãƒ«:").grid(row=1, column=0, sticky='w')
        self.dg_var = tk.DoubleVar(value=0.01)
        ttk.Entry(right_frame, textvariable=self.dg_var, width=10).grid(row=1, column=1, padx=5)
        
        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
        check_frame = ttk.Frame(config_frame)
        check_frame.pack(fill='x', padx=5, pady=5)
        
        self.ka_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(check_frame, text="KAæ¼”ç®—å­æœ‰åŠ¹", variable=self.ka_var).pack(side='left')
        
        self.qa_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(check_frame, text="é‡å­åŒ–å¯¾å¿œ", variable=self.qa_var).pack(side='left', padx=20)
        
        # è‡ªå‹•æœ€é©åŒ–
        ttk.Checkbutton(check_frame, text="ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸè‡ªå‹•æœ€é©åŒ–", 
                       variable=tk.BooleanVar(value=True)).pack(side='left', padx=20)
    
    def create_action_section(self, parent):
        """å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
        action_frame = ttk.LabelFrame(parent, text="ğŸš€ å®Ÿè¡Œ")
        action_frame.pack(fill='x', pady=5)
        
        btn_frame = ttk.Frame(action_frame)
        btn_frame.pack(pady=10)
        
        ttk.Button(btn_frame, text="NKATçµ±åˆå®Ÿè¡Œ", command=self.run_integration_thread, 
                  style='Accent.TButton').pack(side='left', padx=10)
        ttk.Button(btn_frame, text="è¨­å®šã‚’JSONã«ä¿å­˜", command=self.save_config_to_json).pack(side='left', padx=10)
        
        if COLAB_ENV:
            ttk.Button(btn_frame, text="Driveãƒã‚¦ãƒ³ãƒˆ", command=self.mount_drive_thread).pack(side='left', padx=10)
        
        # é€²æ—ãƒãƒ¼
        self.progress = ttk.Progressbar(action_frame, mode='indeterminate')
        self.progress.pack(fill='x', padx=10, pady=5)
    
    def create_log_section(self, parent):
        """ãƒ­ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
        log_frame = ttk.LabelFrame(parent, text="ğŸ“‹ ãƒ­ã‚°")
        log_frame.pack(fill='both', expand=True, pady=5)
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=15, 
                                                 bg='#f8f8f8', state='disabled')
        self.log_text.pack(fill='both', expand=True, padx=5, pady=5)
    
    def log(self, message):
        """ãƒ­ã‚°å‡ºåŠ›"""
        self.log_text.config(state='normal')
        self.log_text.insert(tk.END, f"{time.strftime('%H:%M:%S')} {message}\n")
        self.log_text.see(tk.END)
        self.log_text.config(state='disabled')
        self.update()
    
    def add_gguf_files(self):
        """GGUFãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ """
        files = filedialog.askopenfilenames(
            title="GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            filetypes=[("GGUF files", "*.gguf"), ("All files", "*.*")]
        )
        
        for file in files:
            if file not in self.gguf_files:
                self.gguf_files.append(file)
                self.gguf_listbox.insert(tk.END, os.path.basename(file))
                self.log(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ : {os.path.basename(file)}")
    
    def remove_gguf_file(self):
        """é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤"""
        selection = self.gguf_listbox.curselection()
        if selection:
            index = selection[0]
            removed_file = self.gguf_files.pop(index)
            self.gguf_listbox.delete(index)
            self.log(f"ğŸ—‘ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {os.path.basename(removed_file)}")
    
    def clear_gguf_files(self):
        """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªã‚¢"""
        self.gguf_files.clear()
        self.gguf_listbox.delete(0, tk.END)
        self.log("ğŸ—‘ï¸ å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªã‚¢")
    
    def on_drop_gguf(self, event):
        """ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å‡¦ç†"""
        files = self.tk.splitlist(event.data)
        for file in files:
            if file.lower().endswith('.gguf') and file not in self.gguf_files:
                self.gguf_files.append(file)
                self.gguf_listbox.insert(tk.END, os.path.basename(file))
                self.log(f"ğŸ“ D&Dè¿½åŠ : {os.path.basename(file)}")
            elif file.lower().endswith('.json'):
                self.json_var.set(file)
                self.load_json_config(file)
                self.log(f"âš™ï¸ JSONè¨­å®šèª­è¾¼: {os.path.basename(file)}")
    
    def select_json_config(self):
        """JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ"""
        file = filedialog.askopenfilename(
            title="JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )
        if file:
            self.json_var.set(file)
            self.load_json_config(file)
    
    def load_json_config(self, file_path):
        """JSONè¨­å®šèª­è¾¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # GUIæ›´æ–°
            if 'ka_grid_size' in config_data:
                self.grid_var.set(config_data['ka_grid_size'])
            if 'lie_algebra_dim' in config_data:
                self.lie_var.set(config_data['lie_algebra_dim'])
            if 'noncommutative_strength' in config_data:
                self.nc_var.set(config_data['noncommutative_strength'])
            if 'differential_geometric_scale' in config_data:
                self.dg_var.set(config_data['differential_geometric_scale'])
            if 'enable_ka_operators' in config_data:
                self.ka_var.set(config_data['enable_ka_operators'])
            if 'quantization_aware' in config_data:
                self.qa_var.set(config_data['quantization_aware'])
            
            self.log(f"âœ… JSONè¨­å®šèª­è¾¼å®Œäº†: {os.path.basename(file_path)}")
            
        except Exception as e:
            self.log(f"âŒ JSONèª­è¾¼å¤±æ•—: {e}")
    
    def auto_generate_config(self):
        """è¨­å®šè‡ªå‹•ç”Ÿæˆ"""
        config = self.get_current_config()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ€ã‚¤ã‚¢ãƒ­ã‚°
        file_path = filedialog.asksaveasfilename(
            title="JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜",
            defaultextension=".json",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(config.to_dict(), f, indent=2, ensure_ascii=False)
                self.json_var.set(file_path)
                self.log(f"âœ… JSONè¨­å®šç”Ÿæˆå®Œäº†: {os.path.basename(file_path)}")
            except Exception as e:
                self.log(f"âŒ JSONç”Ÿæˆå¤±æ•—: {e}")
    
    def get_current_config(self):
        """ç¾åœ¨ã®è¨­å®šã‚’å–å¾—"""
        return NKATConfig(
            enable_ka_operators=self.ka_var.get(),
            ka_grid_size=self.grid_var.get(),
            lie_algebra_dim=self.lie_var.get(),
            noncommutative_strength=self.nc_var.get(),
            differential_geometric_scale=self.dg_var.get(),
            spectral_radius_bound=1.0,
            quantization_aware=self.qa_var.get()
        )
    
    def load_preset(self):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆèª­è¾¼"""
        preset_name = self.preset_var.get()
        if preset_name and preset_name in self.presets:
            config_data = self.presets[preset_name]
            
            self.grid_var.set(config_data.get('ka_grid_size', 8))
            self.lie_var.set(config_data.get('lie_algebra_dim', 4))
            self.nc_var.set(config_data.get('noncommutative_strength', 0.1))
            self.dg_var.set(config_data.get('differential_geometric_scale', 0.01))
            self.ka_var.set(config_data.get('enable_ka_operators', True))
            self.qa_var.set(config_data.get('quantization_aware', True))
            
            self.log(f"âœ… ãƒ—ãƒªã‚»ãƒƒãƒˆã€Œ{preset_name}ã€èª­è¾¼å®Œäº†")
    
    def save_preset_dialog(self):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜ãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
        name = tk.simpledialog.askstring("ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜", "ãƒ—ãƒªã‚»ãƒƒãƒˆåã‚’å…¥åŠ›:")
        if name:
            config = self.get_current_config()
            self.presets[name] = config.to_dict()
            self.save_presets()
            
            # ã‚³ãƒ³ãƒœãƒœãƒƒã‚¯ã‚¹æ›´æ–°
            self.preset_combo['values'] = list(self.presets.keys())
            self.preset_var.set(name)
            
            self.log(f"âœ… ãƒ—ãƒªã‚»ãƒƒãƒˆã€Œ{name}ã€ä¿å­˜å®Œäº†")
    
    def save_config_to_json(self):
        """ç¾åœ¨ã®è¨­å®šã‚’JSONã«ä¿å­˜"""
        self.auto_generate_config()
    
    def mount_drive_thread(self):
        """Driveãƒã‚¦ãƒ³ãƒˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œï¼‰"""
        if COLAB_ENV and self.processor:
            threading.Thread(target=self.mount_drive, daemon=True).start()
    
    def mount_drive(self):
        """Driveãƒã‚¦ãƒ³ãƒˆ"""
        self.progress.start()
        try:
            success = self.processor.mount_drive()
            if success:
                self.log("âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†")
            else:
                self.log("âŒ Google Drive ãƒã‚¦ãƒ³ãƒˆå¤±æ•—")
        except Exception as e:
            self.log(f"âŒ Drive ãƒã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            self.progress.stop()
    
    def run_integration_thread(self):
        """çµ±åˆå®Ÿè¡Œï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œï¼‰"""
        threading.Thread(target=self.run_integration, daemon=True).start()
    
    def run_integration(self):
        """NKATçµ±åˆå®Ÿè¡Œ"""
        if not self.gguf_files:
            messagebox.showwarning("è­¦å‘Š", "GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        self.progress.start()
        
        try:
            config = self.get_current_config()
            self.log(f"âš™ï¸ è¨­å®š: ã‚°ãƒªãƒƒãƒ‰={config.ka_grid_size}, ãƒªãƒ¼ä»£æ•°={config.lie_algebra_dim}")
            
            # æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            integrator = GGUFNKATIntegrator(config)
            impact_report = integrator.get_inference_impact_report()
            self.log("ğŸ“Š æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
            
            if COLAB_ENV and self.processor:
                # Colabç’°å¢ƒ
                for file_path in self.gguf_files:
                    self.log(f"ğŸ”„ å‡¦ç†é–‹å§‹: {os.path.basename(file_path)}")
                    try:
                        result = self.processor.process_gguf_file(file_path, config=config)
                        if result:
                            self.processor.save_to_drive(result)
                            self.log(f"âœ… å‡¦ç†å®Œäº†: {os.path.basename(result)}")
                        else:
                            self.log(f"âŒ å‡¦ç†å¤±æ•—: {os.path.basename(file_path)} (çµæœãƒ•ã‚¡ã‚¤ãƒ«ãªã—)")
                    except Exception as e:
                        import traceback
                        error_msg = str(e) if str(e) else type(e).__name__
                        self.log(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
                        self.log(f"ğŸ“‹ è©³ç´°: {traceback.format_exc()}")
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ
                for file_path in self.gguf_files:
                    self.log(f"ğŸ”„ å‡¦ç†é–‹å§‹: {os.path.basename(file_path)}")
                    
                    try:
                        # å‡ºåŠ›ãƒ‘ã‚¹è¨­å®š
                        base_name = os.path.splitext(os.path.basename(file_path))[0]
                        output_dir = os.path.dirname(file_path)
                        output_path = os.path.join(output_dir, f"{base_name}_nkat.gguf")
                        
                        # NKATçµ±åˆ
                        integrator.create_nkat_enhanced_gguf(file_path, output_path)
                        
                        self.log(f"âœ… å‡¦ç†å®Œäº†: {os.path.basename(output_path)}")
                        
                    except Exception as e:
                        import traceback
                        error_msg = str(e) if str(e) else type(e).__name__
                        self.log(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
                        self.log(f"ğŸ“‹ è©³ç´°: {traceback.format_exc()}")
            
            # æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
            self.show_inference_impact_report(impact_report)
            
            messagebox.showinfo("å®Œäº†", "NKATçµ±åˆå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
        except Exception as e:
            import traceback
            error_msg = str(e) if str(e) else type(e).__name__
            self.log(f"âŒ å…¨ä½“å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
            self.log(f"ğŸ“‹ è©³ç´°: {traceback.format_exc()}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{error_msg}")
        finally:
            self.progress.stop()
    
    def show_inference_impact_report(self, report: str):
        """æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º"""
        report_window = tk.Toplevel(self)
        report_window.title("æ¨è«–ã¸ã®å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆ")
        report_window.geometry("800x600")
        report_window.resizable(True, True)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        report_text = scrolledtext.ScrolledText(report_window, bg='#f8f8f8', 
                                               font=('Consolas', 10))
        report_text.pack(fill='both', expand=True, padx=10, pady=10)
        
        report_text.insert('1.0', report)
        report_text.config(state='disabled')
        
        # ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        btn_frame = ttk.Frame(report_window)
        btn_frame.pack(fill='x', padx=10, pady=5)
        
        ttk.Button(btn_frame, text="ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜", 
                  command=lambda: self.save_report(report)).pack(side='left', padx=5)
        ttk.Button(btn_frame, text="é–‰ã˜ã‚‹", 
                  command=report_window.destroy).pack(side='right', padx=5)
        
        self.log("ğŸ“Š æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºå®Œäº†")
    
    def save_report(self, report: str):
        """æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        file_path = filedialog.asksaveasfilename(
            title="æ¨è«–å½±éŸ¿ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜",
            defaultextension=".txt",
            filetypes=[("Text files", "*.txt"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                self.log(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {os.path.basename(file_path)}")
                messagebox.showinfo("ä¿å­˜å®Œäº†", f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ:\n{file_path}")
            except Exception as e:
                self.log(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ:\n{e}")

class ColabGGUFNKATProcessor:
    """Google Colabå°‚ç”¨GGUF+NKATå‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆ64bitå¯¾å¿œï¼‰"""
    
    def __init__(self, config: Optional[NKATConfig] = None):
        self.config = config or NKATConfig()
        self.drive_mounted = False
        self.work_dir = "/content/nkat_workspace"
        self.drive_dir = "/content/drive/MyDrive/NKAT_Models"
        
        # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(self.work_dir, exist_ok=True)
        
        print("ğŸ”§ Colab GGUF+NKAT Processor åˆæœŸåŒ–å®Œäº†ï¼ˆ64bitå¯¾å¿œï¼‰")
        print(f"   64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {self.config.use_64bit_precision}")
        print(f"   CUDAæœ€é©åŒ–: {self.config.enable_cuda_optimization}")
    
    def mount_drive(self):
        """Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆ"""
        if not COLAB_ENV:
            print("âš ï¸ Google Colabç’°å¢ƒã§ã¯ãªã„ãŸã‚ã€Driveãƒã‚¦ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return False
        
        try:
            print("ğŸ“ Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...")
            drive.mount('/content/drive')
            
            # NKATãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs(self.drive_dir, exist_ok=True)
            
            self.drive_mounted = True
            print("âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†")
            print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.drive_dir}")
            return True
            
        except Exception as e:
            print(f"âŒ Google Drive ãƒã‚¦ãƒ³ãƒˆå¤±æ•—: {e}")
            return False
    
    def upload_files(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆGUIä»˜ãï¼‰"""
        if not COLAB_ENV:
            print("âš ï¸ Google Colabç’°å¢ƒã§ã¯ãªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return []
        
        print("ğŸ“¤ GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...")
        uploaded = files.upload()
        
        uploaded_files = []
        for filename, content in uploaded.items():
            if filename.lower().endswith('.gguf'):
                file_path = os.path.join(self.work_dir, filename)
                with open(file_path, 'wb') as f:
                    f.write(content)
                uploaded_files.append(file_path)
                print(f"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {filename} ({len(content)/1024/1024:.1f}MB)")
            else:
                print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {filename} (GGUFå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“)")
        
        return uploaded_files
    
    def get_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"   ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {memory.total/1024/1024/1024:.1f}GB")
            print(f"   ğŸ’¾ ä½¿ç”¨å¯èƒ½: {memory.available/1024/1024/1024:.1f}GB")
            print(f"   ğŸ’¾ ä½¿ç”¨ç‡: {memory.percent:.1f}%")
        except ImportError:
            print("   ğŸ’¾ ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã«ã¯psutilãŒå¿…è¦")
        
        # GPUæƒ…å ±
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    print(f"   ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
                    print(f"   ğŸ”¥ VRAM: {torch.cuda.get_device_properties(0).total_memory/1024/1024/1024:.1f}GB")
                    print(f"   ğŸ”¥ CUDAæœ€é©åŒ–: {'æœ‰åŠ¹' if self.config.enable_cuda_optimization else 'ç„¡åŠ¹'}")
                else:
                    print("   ğŸ”¥ GPU: åˆ©ç”¨ä¸å¯")
            except Exception:
                print("   ğŸ”¥ GPUæƒ…å ±å–å¾—å¤±æ•—")
        else:
            print("   ğŸ”¥ GPUæƒ…å ±å–å¾—ã«ã¯PyTorchãŒå¿…è¦")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
        disk_usage = shutil.disk_usage('/content')
        print(f"   ğŸ’¿ ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨å¯èƒ½: {disk_usage.free/1024/1024/1024:.1f}GB")
        
        # 64bitç²¾åº¦è¨­å®š
        print(f"   ğŸ§® 64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {self.config.use_64bit_precision}")
        print(f"   ğŸ“ ãƒ‡ãƒ¼ã‚¿å¢ƒç•Œæ•´åˆ—: {self.config.data_alignment}ãƒã‚¤ãƒˆ")
    
    def create_adaptive_config(self, model_size_gb: float):
        """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸé©å¿œçš„è¨­å®šã‚’ç”Ÿæˆï¼ˆ64bitè€ƒæ…®ï¼‰"""
        base_config = self.config
        
        if model_size_gb < 1:
            # 1GBæœªæº€ï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«ï¼‰
            config = NKATConfig(
                enable_ka_operators=True,
                ka_grid_size=8,
                lie_algebra_dim=4,
                noncommutative_strength=0.1,
                differential_geometric_scale=0.01,
                quantization_aware=True,
                use_64bit_precision=base_config.use_64bit_precision,
                enable_cuda_optimization=base_config.enable_cuda_optimization
            )
            print("âš¡ è»½é‡ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨ï¼ˆ64bitç²¾åº¦ç¶­æŒï¼‰")
            
        elif model_size_gb < 5:
            # 1-5GBï¼ˆä¸­å‹ãƒ¢ãƒ‡ãƒ«ï¼‰
            config = NKATConfig(
                enable_ka_operators=True,
                ka_grid_size=6,
                lie_algebra_dim=3,
                noncommutative_strength=0.07,
                differential_geometric_scale=0.007,
                quantization_aware=True,
                use_64bit_precision=base_config.use_64bit_precision,
                enable_cuda_optimization=base_config.enable_cuda_optimization
            )
            print("âš–ï¸ ä¸­å‹ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨ï¼ˆ64bitç²¾åº¦æœ€é©åŒ–ï¼‰")
            
        else:
            # 5GBä»¥ä¸Šï¼ˆå¤§å‹ãƒ¢ãƒ‡ãƒ«ï¼‰
            config = NKATConfig(
                enable_ka_operators=True,
                ka_grid_size=4,
                lie_algebra_dim=2,
                noncommutative_strength=0.05,
                differential_geometric_scale=0.005,
                quantization_aware=True,
                use_64bit_precision=base_config.use_64bit_precision,
                enable_cuda_optimization=base_config.enable_cuda_optimization
            )
            print("ğŸ˜ å¤§å‹ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨ï¼ˆ64bitåŠ¹ç‡åŒ–ï¼‰")
        
        return config
    
    def process_gguf_file(self, input_path: str, output_path: Optional[str] = None, config: Optional[NKATConfig] = None):
        """GGUF ãƒ•ã‚¡ã‚¤ãƒ«ã«NKATãƒ‘ãƒƒãƒã‚’é©ç”¨ï¼ˆ64bitå¯¾å¿œï¼‰"""
        print(f"\nğŸ”„ 64bitç²¾åº¦NKATçµ±åˆé–‹å§‹: {os.path.basename(input_path)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(input_path)
        file_size_gb = file_size / (1024**3)
        print(f"   ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_gb:.2f}GB")
        
        # å‡ºåŠ›ãƒ‘ã‚¹è¨­å®š
        if output_path is None:
            base_name = os.path.splitext(os.path.basename(input_path))[0]
            output_path = os.path.join(self.work_dir, f"{base_name}_nkat_64bit.gguf")
        
        # è¨­å®šè‡ªå‹•èª¿æ•´ï¼ˆ64bitè¨­å®šç¶™æ‰¿ï¼‰
        if config is None:
            config = self.create_adaptive_config(file_size_gb)
        else:
            # æ¸¡ã•ã‚ŒãŸè¨­å®šã‚’ãƒ™ãƒ¼ã‚¹ã«é©å¿œèª¿æ•´
            adaptive_config = self.create_adaptive_config(file_size_gb)
            # é‡è¦ãª64bitè¨­å®šã¯ç¶­æŒ
            adaptive_config.use_64bit_precision = config.use_64bit_precision
            adaptive_config.enable_cuda_optimization = config.enable_cuda_optimization
            adaptive_config.enable_performance_monitoring = config.enable_performance_monitoring
            config = adaptive_config
        
        print(f"   ğŸ§® 64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {config.use_64bit_precision}")
        print(f"   ğŸ”¥ CUDAæœ€é©åŒ–: {config.enable_cuda_optimization}")
        
        try:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            gc.collect()
            
            # çµ±åˆå®Ÿè¡Œï¼ˆé€²æ—ãƒãƒ¼ä»˜ãï¼‰
            print("   ğŸ§  64bitç²¾åº¦NKATçµ±åˆå‡¦ç†ä¸­...")
            
            with tqdm(total=100, desc="64bit NKATçµ±åˆ", ncols=80, ascii=True) as pbar:
                # çµ±åˆå™¨åˆæœŸåŒ–
                pbar.set_description("64bitåˆæœŸåŒ–")
                integrator = GGUFNKATIntegrator(config)
                pbar.update(20)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                pbar.set_description("64bitèª­ã¿è¾¼ã¿")
                time.sleep(0.1)  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã®ãŸã‚
                pbar.update(30)
                
                # NKATå‡¦ç†
                pbar.set_description("64bit NKATå‡¦ç†")
                success = integrator.create_nkat_enhanced_gguf(input_path, output_path)
                pbar.update(40)
                
                # å®Œäº†
                pbar.set_description("64bitå®Œäº†")
                pbar.update(10)
            
            # çµæœç¢ºèª
            if success and os.path.exists(output_path):
                output_size = os.path.getsize(output_path) / (1024**3)
                print(f"âœ… 64bitç²¾åº¦NKATçµ±åˆå®Œäº†!")
                print(f"   ğŸ“¤ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(output_path)}")
                print(f"   ğŸ“Š å‡ºåŠ›ã‚µã‚¤ã‚º: {output_size:.2f}GB")
                
                # 64bitæ”¹è‰¯åŠ¹æœç¢ºèª
                precision_improvement = integrator._verify_64bit_improvements(output_path)
                if precision_improvement:
                    print(f"   ğŸ”¬ 64bitç²¾åº¦æ”¹è‰¯: ç¢ºèªæ¸ˆã¿")
                
                return output_path
            else:
                print("âŒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return None
                
        except MemoryError:
            print("âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã™ã€‚ã‚ˆã‚Šè»½é‡ãªè¨­å®šã‚’è©¦ã—ã¦ãã ã•ã„")
            return None
        except Exception as e:
            print(f"âŒ 64bitçµ±åˆå‡¦ç†å¤±æ•—: {e}")
            traceback.print_exc()
            return None
        finally:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            gc.collect()
    
    def save_to_drive(self, file_path):
        """Google Drive ã«ä¿å­˜"""
        if not self.drive_mounted:
            print("âš ï¸ Google Drive ãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            filename = os.path.basename(file_path)
            drive_path = os.path.join(self.drive_dir, filename)
            
            print(f"ğŸ“ Google Drive ã«ä¿å­˜ä¸­: {filename}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸé€²æ—è¡¨ç¤º
            file_size = os.path.getsize(file_path)
            
            with tqdm(total=file_size, desc="Driveä¿å­˜", unit='B', unit_scale=True, ncols=80, ascii=True) as pbar:
                with open(file_path, 'rb') as src, open(drive_path, 'wb') as dst:
                    while True:
                        chunk = src.read(8192)  # 8KB chunks
                        if not chunk:
                            break
                        dst.write(chunk)
                        pbar.update(len(chunk))
            
            print(f"âœ… Google Drive ä¿å­˜å®Œäº†: {drive_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Google Drive ä¿å­˜å¤±æ•—: {e}")
            return False
    
    def download_result(self, file_path):
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if not COLAB_ENV:
            print("âš ï¸ Google Colabç’°å¢ƒã§ã¯ãªã„ãŸã‚ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        try:
            filename = os.path.basename(file_path)
            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {filename}")
            files.download(file_path)
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

def install_dependencies():
    """å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("ğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    
    packages = [
        "numpy",
        "tqdm", 
        "psutil"
    ]
    
    for package in packages:
        try:
            if COLAB_ENV:
                os.system(f"pip install -q {package}")
            print(f"âœ… {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆ64bitçµ±åˆãƒ†ã‚¹ãƒˆå¯¾å¿œï¼‰"""
    print("ğŸš€ Google Colab GGUF+NKATçµ±åˆ é–‹å§‹ï¼ˆ64bitç²¾åº¦å¼·åŒ–ç‰ˆï¼‰")
    print("="*70)
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
    parser = argparse.ArgumentParser(description='GGUF + NKAT 64bitç²¾åº¦çµ±åˆã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--test', action='store_true', help='åŒ…æ‹¬çš„64bitçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ')
    parser.add_argument('--max-files', type=int, default=3, help='ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰')
    parser.add_argument('--gui', action='store_true', help='GUIå¼·åˆ¶ä½¿ç”¨')
    parser.add_argument('--no-gui', action='store_true', help='GUIç„¡åŠ¹åŒ–')
    parser.add_argument('--64bit', action='store_true', default=True, help='64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰')
    parser.add_argument('--32bit', action='store_true', help='32bitäº’æ›ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--cuda', action='store_true', default=True, help='CUDAæœ€é©åŒ–æœ‰åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰')
    parser.add_argument('--no-cuda', action='store_true', help='CUDAæœ€é©åŒ–ç„¡åŠ¹')
    
    args = parser.parse_args()
    
    # è¨­å®šèª¿æ•´
    config = NKATConfig(
        use_64bit_precision=not args.__dict__.get('32bit', False),
        enable_cuda_optimization=not args.__dict__.get('no_cuda', False),
        enable_performance_monitoring=True
    )
    
    print(f"âš™ï¸ å®Ÿè¡Œè¨­å®š:")
    print(f"   64bitç²¾åº¦ãƒ¢ãƒ¼ãƒ‰: {config.use_64bit_precision}")
    print(f"   CUDAæœ€é©åŒ–: {config.enable_cuda_optimization}")
    print(f"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {config.enable_performance_monitoring}")
    
    if args.test:
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
        print("\nğŸ§ª åŒ…æ‹¬çš„64bitçµ±åˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰")
        integrator = GGUFNKATIntegrator(config)
        results = integrator.run_comprehensive_64bit_test(max_files=args.max_files)
        
        if results:
            successful = sum(1 for r in results if r.get("success", False))
            print(f"\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†: {successful}/{len(results)} æˆåŠŸ")
        else:
            print("\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # GUI/CLIåˆ¤å®š
    use_gui = False
    
    if args.gui:
        use_gui = True
    elif args.no_gui:
        use_gui = False
    elif COLAB_ENV:
        # Colabç’°å¢ƒï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠ
        try:
            use_gui_input = input("GUIã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").lower()
            use_gui = use_gui_input in ['y', 'yes']
        except:
            use_gui = False
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼šGUIå„ªå…ˆ
        use_gui = TKINTER_AVAILABLE
    
    if use_gui and TKINTER_AVAILABLE:
        try:
            print("ğŸ–¥ï¸ GUIç‰ˆã‚’èµ·å‹•...")
            app = NKATGUIProcessor()
            # GUIè¨­å®šã«64bitè¨­å®šã‚’åæ˜ 
            if hasattr(app, 'update_config'):
                app.update_config(config)
            app.mainloop()
        except Exception as e:
            print(f"âŒ GUIèµ·å‹•å¤±æ•—: {e}")
            print("ğŸ“‹ ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç‰ˆã§ç¶šè¡Œ...")
            main_workflow_64bit(config)
    else:
        print("ğŸ“‹ ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç‰ˆã§å®Ÿè¡Œ...")
        main_workflow_64bit(config)

def main_workflow_64bit(config: NKATConfig):
    """ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆ64bitå¯¾å¿œCLIç‰ˆï¼‰"""
    print("ğŸš€ Google Colab GGUF+NKATçµ±åˆ é–‹å§‹ï¼ˆ64bitç²¾åº¦ç‰ˆï¼‰")
    print("=" * 60)
    
    # ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    install_dependencies()
    
    # ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–ï¼ˆ64bitè¨­å®šä»˜ãï¼‰
    if COLAB_ENV:
        processor = ColabGGUFNKATProcessor(config)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ç›´æ¥å‡¦ç†
        integrator = GGUFNKATIntegrator(config)
        
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚’ææ¡ˆ
        test_choice = input("ğŸ§ª åŒ…æ‹¬çš„64bitçµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (Y/n): ").lower()
        if test_choice in ['', 'y', 'yes']:
            results = integrator.run_comprehensive_64bit_test()
            return
        
        # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        gguf_files = integrator.find_gguf_models()
        if not gguf_files:
            print("âŒ å‡¦ç†å¯¾è±¡ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        print(f"\nğŸ“ ç™ºè¦‹ã•ã‚ŒãŸGGUFãƒ•ã‚¡ã‚¤ãƒ«:")
        for i, gguf_file in enumerate(gguf_files[:10], 1):  # ä¸Šä½10å€‹è¡¨ç¤º
            size_mb = gguf_file.stat().st_size / (1024 * 1024)
            print(f"   {i}. {gguf_file.name}: {size_mb:.2f} MB")
        
        try:
            choice = input(f"\nå‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·ã‚’å…¥åŠ› (1-{min(len(gguf_files), 10)}, ã¾ãŸã¯ all): ")
            if choice.lower() == 'all':
                selected_files = gguf_files[:5]  # å®‰å…¨ã®ãŸã‚æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«
            else:
                index = int(choice) - 1
                if 0 <= index < len(gguf_files):
                    selected_files = [gguf_files[index]]
                else:
                    print("âŒ ç„¡åŠ¹ãªé¸æŠ")
                    return
        except ValueError:
            print("âŒ ç„¡åŠ¹ãªå…¥åŠ›")
            return
        
        # å‡¦ç†å®Ÿè¡Œ
        results = []
        for model_path in selected_files:
            result = integrator.test_model_integration(model_path)
            results.append(result)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        integrator.generate_integration_report(results)
        
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    processor.get_system_info()
    
    # Google Drive ãƒã‚¦ãƒ³ãƒˆ
    processor.mount_drive()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = processor.upload_files()
    
    if not uploaded_files:
        print("âŒ å‡¦ç†å¯¾è±¡ã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆ64bitå¯¾å¿œï¼‰
    processed_files = []
    
    for input_file in uploaded_files:
        print(f"\n" + "="*60)
        result_file = processor.process_gguf_file(input_file, config=config)
        
        if result_file:
            processed_files.append(result_file)
            
            # Google Drive ã«ä¿å­˜
            processor.save_to_drive(result_file)
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            try:
                download_choice = input(f"ğŸ“¥ {os.path.basename(result_file)} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
                if download_choice.lower() in ['y', 'yes']:
                    processor.download_result(result_file)
            except Exception:
                print("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼‰")
    
    # å®Œäº†å ±å‘Š
    print(f"\nğŸ‰ 64bitç²¾åº¦NKATçµ±åˆå‡¦ç†å®Œäº†!")
    print(f"   å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")
    for pf in processed_files:
        print(f"   âœ… {os.path.basename(pf)}")
    
    print(f"\nğŸ“ å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã¯Google Driveã«ã‚‚ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
    if COLAB_ENV:
        print(f"   {processor.drive_dir}")

if __name__ == "__main__":
    main() 
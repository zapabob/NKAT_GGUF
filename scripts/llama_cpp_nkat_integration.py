#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ NKAT-GGUF llama.cppÁµ±ÂêàËá™ÂãïÂåñ„Çπ„ÇØ„É™„Éó„Éà
Automated NKAT-GGUF Integration for llama.cpp

Ê©üËÉΩ:
- llama.cpp„ÅÆËá™Âãï„ÇØ„É≠„Éº„É≥„ÉªÊ∫ñÂÇô
- NKAT CUDA„Ç´„Éº„Éç„É´„ÅÆÁµ±Âêà
- CMakeLists.txtËá™Âãï‰øÆÊ≠£
- „Ç≥„É≥„Éë„Ç§„É´„Éª„ÉÜ„Çπ„ÉàÂÆüË°å
- ÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ
"""

import os
import sys
import subprocess
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import json
import re

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llama_cpp_nkat_integration.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class LlamaCppNKATIntegrator:
    """llama.cpp NKATÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self, nkat_project_dir: str = ".", llama_cpp_dir: str = "llama.cpp"):
        self.nkat_dir = Path(nkat_project_dir).resolve()
        self.llama_dir = Path(llama_cpp_dir).resolve()
        self.cuda_kernels_dir = self.nkat_dir / "output" / "cuda_kernels"
        
        # Áµ±ÂêàË®≠ÂÆö
        self.integration_config = {
            "cuda_compute_arch": "86",  # RTX3080 (Ampere)
            "optimization_level": "3",
            "use_fast_math": True,
            "enable_benchmarks": True
        }
        
        logger.info(f"üöÄ NKAT-llama.cppÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ")
        logger.info(f"   NKAT„Éó„É≠„Ç∏„Çß„ÇØ„Éà: {self.nkat_dir}")
        logger.info(f"   llama.cpp„Éá„Ç£„É¨„ÇØ„Éà„É™: {self.llama_dir}")
        
    def run_command(self, cmd: List[str], cwd: Optional[Path] = None) -> subprocess.CompletedProcess:
        """„Ç≥„Éû„É≥„ÉâÂÆüË°å"""
        if cwd is None:
            cwd = Path.cwd()
        
        logger.info(f"üîß ÂÆüË°å‰∏≠: {' '.join(cmd)} (dir: {cwd})")
        
        result = subprocess.run(
            cmd, 
            cwd=cwd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            shell=True if os.name == 'nt' else False
        )
        
        # git pull„ÅÆÁâπÂÆö„ÅÆ„Ç®„É©„Éº„ÇíË®±ÂÆπÔºà„Éï„Ç°„Ç§„É´Êõ¥Êñ∞„ÅØÂÆå‰∫Ü„Åó„Å¶„ÅÑ„ÇãÔºâ
        if result.returncode != 0:
            if "git pull" in " ".join(cmd) and "cannot lock ref 'HEAD'" in result.stderr:
                logger.warning(f"‚ö†Ô∏è GitÂèÇÁÖß„É≠„ÉÉ„ÇØ„Ç®„É©„ÉºÔºàËªΩÂæÆÔºâ: „Éï„Ç°„Ç§„É´Êõ¥Êñ∞„ÅØÂÆå‰∫Ü")
                return result
            elif "git pull" in " ".join(cmd) and "Updating files: 100%" in result.stderr:
                logger.info(f"‚úÖ Git„Éï„Ç°„Ç§„É´Êõ¥Êñ∞ÂÆå‰∫ÜÔºàHEADÂèÇÁÖß„Ç®„É©„Éº„ÅØÁÑ°Ë¶ñÔºâ")
                return result
            else:
                logger.error(f"‚ùå „Ç≥„Éû„É≥„ÉâÂ§±Êïó: {result.stderr}")
                raise RuntimeError(f"Command failed: {' '.join(cmd)}\n{result.stderr}")
        
        return result
    
    def step1_prepare_llama_cpp(self) -> bool:
        """Step 1: llama.cpp„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊ∫ñÂÇô"""
        logger.info("üì¶ Step 1: llama.cpp„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊ∫ñÂÇô")
        
        try:
            # llama.cpp„ÇØ„É≠„Éº„É≥ÔºàÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥ÂêàÔºâ
            if not self.llama_dir.exists():
                logger.info("üì• llama.cpp„Çí„ÇØ„É≠„Éº„É≥‰∏≠...")
                self.run_command([
                    "git", "clone", 
                    "https://github.com/ggerganov/llama.cpp.git",
                    str(self.llama_dir)
                ])
            else:
                logger.info("üìÇ Êó¢Â≠ò„ÅÆllama.cpp„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí‰ΩøÁî®")
                
            # ÊúÄÊñ∞Áâà„Å´Êõ¥Êñ∞
            logger.info("üîÑ llama.cpp„ÇíÊúÄÊñ∞Áâà„Å´Êõ¥Êñ∞‰∏≠...")
            self.run_command(["git", "pull", "origin", "master"], self.llama_dir)
            
            # Êñ∞„Åó„ÅÑ„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„ÇíÁ¢∫Ë™ç
            cuda_dir_new = self.llama_dir / "ggml" / "src" / "ggml-cuda"
            cuda_dir_old = self.llama_dir / "src" / "ggml-cuda"
            
            if cuda_dir_new.exists():
                logger.info(f"‚úÖ Êñ∞„Åó„ÅÑCUDA„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„ÇíÁ¢∫Ë™ç: {cuda_dir_new}")
                self.cuda_target_dir = cuda_dir_new
            elif cuda_dir_old.exists():
                logger.info(f"‚úÖ ÊóßCUDA„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„ÇíÁ¢∫Ë™ç: {cuda_dir_old}")
                self.cuda_target_dir = cuda_dir_old
            else:
                logger.error(f"‚ùå CUDA„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                logger.error(f"   Á¢∫Ë™ç„Åó„ÅüÂ†¥ÊâÄ: {cuda_dir_new}, {cuda_dir_old}")
                return False
                
            logger.info("‚úÖ Step 1ÂÆå‰∫Ü: llama.cppÊ∫ñÂÇôÂÆå‰∫Ü")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Step 1Â§±Êïó: {e}")
            return False
    
    def step2_integrate_cuda_kernels(self) -> bool:
        """Step 2: NKAT CUDA„Ç´„Éº„Éç„É´Áµ±Âêà"""
        logger.info("üîß Step 2: NKAT CUDA„Ç´„Éº„Éç„É´Áµ±Âêà")
        
        try:
            # CUDA„Ç´„Éº„Éç„É´„Éï„Ç°„Ç§„É´Á¢∫Ë™ç
            kernel_files = [
                "nkat_star_gemm_kernels.cu",
                "nkat_cuda_interface.cpp", 
                "nkat_cuda.h"
            ]
            
            target_dir = self.cuda_target_dir
            
            for filename in kernel_files:
                src_file = self.cuda_kernels_dir / filename
                dst_file = target_dir / filename
                
                if not src_file.exists():
                    logger.error(f"‚ùå „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {src_file}")
                    return False
                
                # „Éï„Ç°„Ç§„É´„Ç≥„Éî„Éº
                shutil.copy2(src_file, dst_file)
                logger.info(f"üìÅ „Ç≥„Éî„Éº: {filename} -> {dst_file}")
            
            logger.info("‚úÖ Step 2ÂÆå‰∫Ü: CUDA„Ç´„Éº„Éç„É´Áµ±ÂêàÂÆå‰∫Ü")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Step 2Â§±Êïó: {e}")
            return False
    
    def step3_modify_cmake(self) -> bool:
        """Step 3: CMakeLists.txt‰øÆÊ≠£"""
        logger.info("üìù Step 3: CMakeLists.txt‰øÆÊ≠£")
        
        try:
            cmake_file = self.llama_dir / "CMakeLists.txt"
            
            # „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó‰ΩúÊàê
            backup_file = cmake_file.with_suffix(".txt.nkat_backup")
            shutil.copy2(cmake_file, backup_file)
            logger.info(f"üíæ CMakeLists.txt„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó: {backup_file}")
            
            # CMakeLists.txtË™≠„ÅøÂèñ„Çä
            with open(cmake_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # NKATÁµ±ÂêàË®≠ÂÆö„ÇíËøΩÂä†
            nkat_cmake_config = self._generate_cmake_nkat_config()
            
            # Êñ∞„Åó„ÅÑGGML_CUDA„Çª„ÇØ„Ç∑„Éß„É≥„Åæ„Åü„ÅØÊóßLLAMA_CUBLAS„Çª„ÇØ„Ç∑„Éß„É≥„ÇíË¶ã„Å§„Åë„Å¶ÊåøÂÖ•
            if "if(GGML_CUDA)" in content:
                # Êñ∞„Åó„ÅÑGGML_CUDA„Çª„ÇØ„Ç∑„Éß„É≥„Å´ËøΩÂä†
                content = content.replace(
                    "if(GGML_CUDA)",
                    f"if(GGML_CUDA)\n{nkat_cmake_config}"
                )
                logger.info("‚úÖ GGML_CUDA„Çª„ÇØ„Ç∑„Éß„É≥„Å´NKATË®≠ÂÆö„ÇíËøΩÂä†")
            elif "if(LLAMA_CUBLAS)" in content:
                # ÊóßLLAMA_CUBLAS„Çª„ÇØ„Ç∑„Éß„É≥„Å´ËøΩÂä†
                content = content.replace(
                    "if(LLAMA_CUBLAS)",
                    f"if(LLAMA_CUBLAS)\n{nkat_cmake_config}"
                )
                logger.info("‚úÖ LLAMA_CUBLAS„Çª„ÇØ„Ç∑„Éß„É≥„Å´NKATË®≠ÂÆö„ÇíËøΩÂä†")
            else:
                # „Éï„Ç°„Ç§„É´Êú´Â∞æ„Å´ËøΩÂä†
                content += f"\n# NKAT CUDA Integration\n{nkat_cmake_config}\n"
                logger.info("‚úÖ „Éï„Ç°„Ç§„É´Êú´Â∞æ„Å´NKATË®≠ÂÆö„ÇíËøΩÂä†")
            
            # „Éï„Ç°„Ç§„É´‰øùÂ≠ò
            with open(cmake_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info("‚úÖ Step 3ÂÆå‰∫Ü: CMakeLists.txt‰øÆÊ≠£ÂÆå‰∫Ü")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Step 3Â§±Êïó: {e}")
            return False
    
    def _generate_cmake_nkat_config(self) -> str:
        """NKATÁî®CMakeË®≠ÂÆöÁîüÊàê"""
        # Êñ∞„Åó„ÅÑ„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„Å´Âü∫„Å•„ÅÑ„Å¶„Éë„Çπ„ÇíÊ±∫ÂÆö
        if hasattr(self, 'cuda_target_dir'):
            relative_path = self.cuda_target_dir.relative_to(self.llama_dir)
            cuda_src_path = str(relative_path).replace('\\', '/')
        else:
            cuda_src_path = "ggml/src/ggml-cuda"
            
        return f'''
    # NKAT CUDA Integration (Auto-generated)
    if(GGML_CUDA)
        # NKAT CUDA sources
        set(NKAT_CUDA_SOURCES
            {cuda_src_path}/nkat_star_gemm_kernels.cu
            {cuda_src_path}/nkat_cuda_interface.cpp
        )
        
        # NKAT headers
        set(NKAT_CUDA_HEADERS
            {cuda_src_path}/nkat_cuda.h
        )
        
        # RTX3080 (Ampere) optimization
        if(TARGET ggml-cuda)
            set_property(TARGET ggml-cuda PROPERTY CUDA_ARCHITECTURES {self.integration_config["cuda_compute_arch"]})
            
            # Add NKAT sources to existing target
            target_sources(ggml-cuda PRIVATE ${{NKAT_CUDA_SOURCES}})
            target_include_directories(ggml-cuda PRIVATE {cuda_src_path})
            
            # NKAT definitions
            target_compile_definitions(ggml-cuda PRIVATE 
                GGML_CUDA_NKAT_ENABLED
                NKAT_CUDA_ARCH={self.integration_config["cuda_compute_arch"]}
            )
            
            # Performance optimization flags
            target_compile_options(ggml-cuda PRIVATE 
                $<$<COMPILE_LANGUAGE:CUDA>:
                    --use_fast_math
                    --optimize={self.integration_config["optimization_level"]}
                    --maxrregcount=128
                    -Xptxas=-v
                >
            )
        endif()
    endif()
'''
    
    def step4_modify_source_files(self) -> bool:
        """Step 4: „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£"""
        logger.info("üîß Step 4: „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£")
        
        try:
            # ggml-cuda.cu‰øÆÊ≠£
            if not self._modify_ggml_cuda():
                return False
            
            # gguf.cpp‰øÆÊ≠£  
            if not self._modify_gguf_cpp():
                return False
            
            logger.info("‚úÖ Step 4ÂÆå‰∫Ü: „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£ÂÆå‰∫Ü")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Step 4Â§±Êïó: {e}")
            return False
    
    def _modify_ggml_cuda(self) -> bool:
        """ggml-cuda.cu‰øÆÊ≠£"""
        # Êñ∞„Åó„ÅÑÊßãÈÄ†„Åß„ÅØ„ÄÅ„É°„Ç§„É≥„ÅÆCUDA„Éï„Ç°„Ç§„É´„ÅÆÂ†¥ÊâÄ„ÇíÁ¢∫Ë™ç
        possible_cuda_files = [
            self.llama_dir / "ggml" / "src" / "ggml-cuda.cu",
            self.llama_dir / "src" / "ggml-cuda.cu",
            self.llama_dir / "ggml" / "src" / "ggml-cuda" / "ggml-cuda.cu"
        ]
        
        cuda_file = None
        for file_path in possible_cuda_files:
            if file_path.exists():
                cuda_file = file_path
                break
        
        if not cuda_file:
            logger.warning(f"‚ö†Ô∏è ggml-cuda.cu„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ")
            logger.info(f"üîç CUDAÂÆüË£Ö„ÅØÂà•„ÅÆ„Éï„Ç°„Ç§„É´ÊßãÈÄ†„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô")
            return True  # Êñ∞„Åó„ÅÑÊßãÈÄ†„Åß„ÅØ‰∏çË¶Å„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì
        
        # „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó‰ΩúÊàê
        backup_file = cuda_file.with_suffix(".cu.nkat_backup")
        shutil.copy2(cuda_file, backup_file)
        
        # „Éï„Ç°„Ç§„É´Ë™≠„ÅøÂèñ„Çä
        with open(cuda_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # NKAT includeËøΩÂä†
        nkat_include = '''
#ifdef GGML_CUDA_NKAT_ENABLED
#include "nkat_cuda.h"
#endif
'''
        
        # „Ç§„É≥„ÇØ„É´„Éº„Éâ„Çª„ÇØ„Ç∑„Éß„É≥„Å´ËøΩÂä†
        if "#include" in content and "nkat_cuda.h" not in content:
            lines = content.split('\n')
            include_end = -1
            for i, line in enumerate(lines):
                if line.startswith('#include'):
                    include_end = i
            
            if include_end >= 0:
                lines.insert(include_end + 1, nkat_include)
                content = '\n'.join(lines)
        
        # „Éï„Ç°„Ç§„É´‰øùÂ≠ò
        with open(cuda_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"üìù {cuda_file.name}‰øÆÊ≠£ÂÆå‰∫Ü")
        return True
    
    def _modify_gguf_cpp(self) -> bool:
        """gguf.cpp‰øÆÊ≠£"""
        # Êñ∞„Åó„ÅÑÊßãÈÄ†„Åß„ÅÆgguf.cpp„Éï„Ç°„Ç§„É´„ÅÆÂ†¥ÊâÄ„ÇíÁ¢∫Ë™ç
        possible_gguf_files = [
            self.llama_dir / "ggml" / "src" / "gguf.cpp",
            self.llama_dir / "src" / "gguf.cpp",
            self.llama_dir / "gguf.cpp"
        ]
        
        gguf_file = None
        for file_path in possible_gguf_files:
            if file_path.exists():
                gguf_file = file_path
                break
        
        if not gguf_file:
            logger.warning(f"‚ö†Ô∏è gguf.cpp„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
            logger.info(f"üîç GGUFÂÆüË£Ö„ÅØÂà•„ÅÆ„Éï„Ç°„Ç§„É´„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô")
            return True  # Êñ∞„Åó„ÅÑÊßãÈÄ†„Åß„ÅØ‰∏çË¶Å„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì
        
        # „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó‰ΩúÊàê
        backup_file = gguf_file.with_suffix(".cpp.nkat_backup")
        shutil.copy2(gguf_file, backup_file)
        
        # „Éï„Ç°„Ç§„É´Ë™≠„ÅøÂèñ„Çä
        with open(gguf_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # NKATÈñ¢ÈÄ£ÂÆöÊï∞ËøΩÂä†
        nkat_constants = '''
// NKAT metadata keys (Auto-generated)
static const char * GGUF_NKAT_VERSION = "nkat.version";
static const char * GGUF_NKAT_THETA_RANK = "nkat.theta_rank";
static const char * GGUF_NKAT_GAMMA_DECAY = "nkat.gamma_decay";

// NKAT tensor name patterns
static bool is_nkat_theta_tensor(const char* name) {
    return strstr(name, ".theta") != nullptr;
}
'''
        
        # ÂÆöÊï∞ÂÆöÁæ©„Çª„ÇØ„Ç∑„Éß„É≥„Å´ËøΩÂä†
        if "static const char *" in content and "GGUF_NKAT_VERSION" not in content:
            # ÊúÄÂàù„ÅÆstatic const charÂÆöÁæ©„ÅÆÂâç„Å´ÊåøÂÖ•
            content = content.replace(
                "static const char *",
                f"{nkat_constants}\nstatic const char *",
                1
            )
        
        # „Éï„Ç°„Ç§„É´‰øùÂ≠ò
        with open(gguf_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"üìù {gguf_file.name}‰øÆÊ≠£ÂÆå‰∫Ü")
        return True
    
    def step5_compile(self) -> bool:
        """Step 5: „Ç≥„É≥„Éë„Ç§„É´"""
        logger.info("üî® Step 5: „Ç≥„É≥„Éë„Ç§„É´ÂÆüË°å")
        
        try:
            # „Éì„É´„Éâ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
            build_dir = self.llama_dir / "build"
            build_dir.mkdir(exist_ok=True)
            
            # CMakeË®≠ÂÆö
            logger.info("‚öôÔ∏è CMakeË®≠ÂÆö‰∏≠...")
            cmake_cmd = [
                "cmake", "..",
                "-DGGML_CUDA=ON",  # Êñ∞„Åó„ÅÑ„Ç™„Éó„Ç∑„Éß„É≥
                "-DCMAKE_BUILD_TYPE=Release",
                f"-DCUDA_ARCHITECTURES={self.integration_config['cuda_compute_arch']}"
            ]
            
            self.run_command(cmake_cmd, build_dir)
            
            # „Ç≥„É≥„Éë„Ç§„É´ÂÆüË°å
            logger.info("üî® „Ç≥„É≥„Éë„Ç§„É´‰∏≠ÔºàÊôÇÈñì„Åå„Åã„Åã„Çä„Åæ„ÅôÔºâ...")
            build_cmd = [
                "cmake", "--build", ".", 
                "--config", "Release", 
                "-j", "6"  # ‰∏¶Âàó„Ç∏„Éß„ÉñÊï∞
            ]
            
            result = self.run_command(build_cmd, build_dir)
            
            # ÂÆüË°å„Éï„Ç°„Ç§„É´Á¢∫Ë™ç
            main_exe = build_dir / "bin" / "llama-cli.exe" if os.name == 'nt' else build_dir / "bin" / "llama-cli"
            if not main_exe.exists():
                main_exe = build_dir / "llama-cli.exe" if os.name == 'nt' else build_dir / "llama-cli"
            if not main_exe.exists():
                main_exe = build_dir / "main.exe" if os.name == 'nt' else build_dir / "main"
            
            if main_exe.exists():
                logger.info(f"‚úÖ „Ç≥„É≥„Éë„Ç§„É´ÊàêÂäü: {main_exe}")
                return True
            else:
                logger.error("‚ùå ÂÆüË°å„Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Step 5Â§±Êïó: {e}")
            return False
    
    def step6_test_integration(self) -> bool:
        """Step 6: Áµ±Âêà„ÉÜ„Çπ„Éà"""
        logger.info("üß™ Step 6: Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å")
        
        try:
            # ÂÆüË°å„Éï„Ç°„Ç§„É´Êé¢Á¥¢
            build_dir = self.llama_dir / "build"
            main_exe = self._find_main_executable(build_dir)
            
            if not main_exe:
                logger.error("‚ùå mainÂÆüË°å„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                return False
            
            # NKAT„ÉÜ„Çπ„Éà„É¢„Éá„É´Á¢∫Ë™ç
            test_model = self.nkat_dir / "output" / "nkat_test_model_enhanced.gguf"
            if not test_model.exists():
                logger.warning(f"‚ö†Ô∏è „ÉÜ„Çπ„Éà„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {test_model}")
                # ‰ª£Êõø„É¢„Éá„É´„ÇíÊé¢Á¥¢
                test_model = self._find_alternative_test_model()
                if not test_model:
                    logger.error("‚ùå „ÉÜ„Çπ„ÉàÁî®„É¢„Éá„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                    return False
            
            # „Ç∑„É≥„Éó„É´„ÉÜ„Çπ„ÉàÂÆüË°å
            logger.info("üîç Âü∫Êú¨Ê©üËÉΩ„ÉÜ„Çπ„Éà‰∏≠...")
            test_cmd = [
                str(main_exe),
                "-m", str(test_model),
                "-p", "Hello, world!",
                "-n", "10",
                "--temp", "0.0"
            ]
            
            result = self.run_command(test_cmd, build_dir)
            
            if "Hello" in result.stdout or "world" in result.stdout:
                logger.info("‚úÖ Âü∫Êú¨Êé®Ë´ñ„ÉÜ„Çπ„ÉàÊàêÂäü")
                return True
            else:
                logger.warning("‚ö†Ô∏è Êé®Ë´ñÁµêÊûú„ÅÆÊ§úË®º„Å´Â§±Êïó")
                logger.info(f"Âá∫Âäõ: {result.stdout[:200]}...")
                return True  # ÂÆüË°åËá™‰Ωì„ÅØÊàêÂäü
                
        except Exception as e:
            logger.error(f"‚ùå Step 6Â§±Êïó: {e}")
            return False
    
    def _find_main_executable(self, build_dir: Path) -> Optional[Path]:
        """mainÂÆüË°å„Éï„Ç°„Ç§„É´Êé¢Á¥¢"""
        possible_locations = [
            # Êñ∞„Åó„ÅÑllama.cpp„ÅÆÂÆüË°å„Éï„Ç°„Ç§„É´Âêç
            build_dir / "bin" / "llama-cli.exe",
            build_dir / "bin" / "llama-cli", 
            build_dir / "llama-cli.exe",
            build_dir / "llama-cli",
            # ÊóßÂÆüË°å„Éï„Ç°„Ç§„É´Âêç
            build_dir / "bin" / "main.exe",
            build_dir / "bin" / "main",
            build_dir / "main.exe", 
            build_dir / "main",
            build_dir / "Release" / "llama-cli.exe",
            build_dir / "Release" / "main.exe",
            build_dir / "Debug" / "llama-cli.exe",
            build_dir / "Debug" / "main.exe"
        ]
        
        for exe_path in possible_locations:
            if exe_path.exists():
                logger.info(f"‚úÖ ÂÆüË°å„Éï„Ç°„Ç§„É´Áô∫Ë¶ã: {exe_path}")
                return exe_path
        
        return None
    
    def _find_alternative_test_model(self) -> Optional[Path]:
        """‰ª£Êõø„ÉÜ„Çπ„Éà„É¢„Éá„É´Êé¢Á¥¢"""
        possible_models = []
        
        # output„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖÊ§úÁ¥¢
        output_dir = self.nkat_dir / "output"
        if output_dir.exists():
            possible_models.extend(output_dir.glob("*.gguf"))
        
        # models„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖÊ§úÁ¥¢
        models_dir = self.nkat_dir / "models"
        if models_dir.exists():
            for subdir in models_dir.iterdir():
                if subdir.is_dir():
                    possible_models.extend(subdir.glob("*.gguf"))
        
        # ÊúÄÂàù„Å´Ë¶ã„Å§„Åã„Å£„Åü„É¢„Éá„É´„ÇíËøî„Åô
        for model in possible_models:
            if model.stat().st_size > 1024 * 1024:  # 1MB‰ª•‰∏ä
                return model
        
        return None
    
    def run_benchmark(self) -> Dict[str, float]:
        """ÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å"""
        logger.info("üìä ÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å")
        
        try:
            build_dir = self.llama_dir / "build"
            main_exe = self._find_main_executable(build_dir)
            
            if not main_exe:
                logger.error("‚ùå ÂÆüË°å„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
                return {}
            
            # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁî®„Éó„É≠„É≥„Éó„Éà
            test_prompt = "The quick brown fox jumps over the lazy dog. " * 10
            
            # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
            bench_cmd = [
                str(main_exe),
                "-m", str(self._find_alternative_test_model()),
                "-p", test_prompt,
                "-n", "100",
                "--temp", "0.1"
            ]
            
            start_time = datetime.now()
            result = self.run_command(bench_cmd, build_dir)
            end_time = datetime.now()
            
            # ÂÆüË°åÊôÇÈñìË®àÁÆó
            duration = (end_time - start_time).total_seconds()
            tokens_generated = 100  # -n „Éë„É©„É°„Éº„Çø
            tokens_per_second = tokens_generated / duration if duration > 0 else 0
            
            benchmark_results = {
                "tokens_per_second": tokens_per_second,
                "total_duration": duration,
                "tokens_generated": tokens_generated
            }
            
            logger.info(f"üìà „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú:")
            logger.info(f"   Êé®Ë´ñÈÄüÂ∫¶: {tokens_per_second:.2f} tokens/s")
            logger.info(f"   ÂÆüË°åÊôÇÈñì: {duration:.2f}Áßí")
            
            return benchmark_results
            
        except Exception as e:
            logger.error(f"‚ùå „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂ§±Êïó: {e}")
            return {}
    
    def generate_integration_report(self, benchmark_results: Dict = None) -> str:
        """Áµ±Âêà„É¨„Éù„Éº„ÉàÁîüÊàê"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# üöÄ NKAT-llama.cppÁµ±Âêà„É¨„Éù„Éº„Éà

**ÁîüÊàêÊó•ÊôÇ**: {timestamp}

## üìã Áµ±ÂêàÊ¶ÇË¶Å

‚úÖ **Áµ±Âêà„Çπ„ÉÜ„Éº„Çø„Çπ**: ÊàêÂäü
üéØ **NKATÊ©üËÉΩ**: ÈùûÂèØÊèõ„Ç≥„É´„É¢„Ç¥„É≠„Éï„Éª„Ç¢„Éº„Éé„É´„ÉâË°®ÁèæÁêÜË´ñ
üîß **GPUÊúÄÈÅ©Âåñ**: RTX3080 (Ampere) ÂØæÂøú
üì¶ **Áµ±Âêà„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà**: 
- CUDA „Ç´„Éº„Éç„É´
- GGUFÊã°Âºµ
- CMakeË®≠ÂÆö
- „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£

## üõ†Ô∏è Áµ±Âêà„Åï„Çå„ÅüÊ©üËÉΩ

### NKAT CUDA „Ç´„Éº„Éç„É´
- `nkat_star_gemm_kernels.cu` - Moyal star productÊºîÁÆó
- `nkat_cuda_interface.cpp` - „Éõ„Çπ„Éà„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ
- `nkat_cuda.h` - „Éò„ÉÉ„ÉÄ„Éº„Éï„Ç°„Ç§„É´

### „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£
- `CMakeLists.txt` - NKATÁµ±ÂêàË®≠ÂÆöËøΩÂä†
- `ggml-cuda.cu` - NKAT includeËøΩÂä†
- `gguf.cpp` - NKAT„É°„Çø„Éá„Éº„ÇøÂØæÂøú

## üìä ÊÄßËÉΩ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ
"""
        
        if benchmark_results:
            report += f"""
- **Êé®Ë´ñÈÄüÂ∫¶**: {benchmark_results.get('tokens_per_second', 'N/A'):.2f} tokens/s
- **ÂÆüË°åÊôÇÈñì**: {benchmark_results.get('total_duration', 'N/A'):.2f}Áßí
- **ÁîüÊàê„Éà„Éº„ÇØ„É≥Êï∞**: {benchmark_results.get('tokens_generated', 'N/A')}
"""
        else:
            report += "\n- „Éô„É≥„ÉÅ„Éû„Éº„ÇØÊú™ÂÆüË°å\n"
        
        report += f"""
## üéØ ‰ΩøÁî®ÊñπÊ≥ï

### Âü∫Êú¨Êé®Ë´ñ
```bash
cd {self.llama_dir}/build
./main -m path/to/nkat_model.gguf -p "Your prompt here"
```

### NKATÊ©üËÉΩÊúâÂäπÂåñ
```bash
./main -m nkat_model.gguf -p "prompt" --nkat-enable
```

## üî¨ ÁêÜË´ñËÉåÊôØ

NKATÁµ±Âêà„Å´„Çà„Çä„ÄÅÂæìÊù•„ÅÆÁ∑öÂΩ¢ÊºîÁÆó `y = Wx` „ÅåÈùûÂèØÊèõÊòüÁ©çÊºîÁÆó„Å´Êã°ÂºµÔºö

```
y = (W ‚ãÜ_Œ∏ x) := W exp(i/2 Œ∏^ŒºŒΩ ‚àÇ_Œº ‚àÇ_ŒΩ) x
```

„Åì„Çå„Å´„Çà„ÇäË°®ÁèæÁ©∫Èñì„ÅåÊã°Âºµ„Åï„Çå„ÄÅÊé®Ë´ñÁ≤æÂ∫¶„ÅÆÂêë‰∏ä„ÅåÊúüÂæÖ„Åï„Çå„Åæ„Åô„ÄÇ

## üìÅ Áµ±Âêà„Éï„Ç°„Ç§„É´‰∏ÄË¶ß

### ËøΩÂä†„Åï„Çå„Åü„Éï„Ç°„Ç§„É´
- `src/ggml-cuda/nkat_star_gemm_kernels.cu`
- `src/ggml-cuda/nkat_cuda_interface.cpp`  
- `src/ggml-cuda/nkat_cuda.h`

### ‰øÆÊ≠£„Åï„Çå„Åü„Éï„Ç°„Ç§„É´
- `CMakeLists.txt` („Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó: CMakeLists.txt.nkat_backup)
- `src/ggml-cuda.cu` („Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó: ggml-cuda.cu.nkat_backup)
- `src/gguf.cpp` („Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó: gguf.cpp.nkat_backup)

## üîß „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞

### „Ç≥„É≥„Éë„Ç§„É´„Ç®„É©„Éº
- CUDA Compute Capability„ÅåÈÅ©Âàá„Å´Ë®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç
- ÂøÖË¶Å„Å™CUDA„É©„Ç§„Éñ„É©„É™„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç

### ÂÆüË°åÊôÇ„Ç®„É©„Éº  
- NKAT„ÉÜ„É≥„ÇΩ„É´‰ªò„ÅçGGUF„Éï„Ç°„Ç§„É´„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç
- ÂçÅÂàÜ„Å™GPU„É°„É¢„É™„ÅåÂà©Áî®ÂèØËÉΩ„ÅãÁ¢∫Ë™ç

---

**Áµ±ÂêàÂÆå‰∫Ü**: NKATÊ©üËÉΩ„Åållama.cpp„Å´Ê≠£Â∏∏„Å´Áµ±Âêà„Åï„Çå„Åæ„Åó„Åü üéâ
"""
        
        return report
    
    def integrate_all(self) -> bool:
        """ÂÖ®Áµ±Âêà„Éó„É≠„Çª„ÇπÂÆüË°å"""
        logger.info("üöÄ NKAT-llama.cpp ÂÆåÂÖ®Áµ±ÂêàÈñãÂßã")
        
        steps = [
            ("llama.cppÊ∫ñÂÇô", self.step1_prepare_llama_cpp),
            ("CUDA„Ç´„Éº„Éç„É´Áµ±Âêà", self.step2_integrate_cuda_kernels),
            ("CMakeË®≠ÂÆö", self.step3_modify_cmake),
            ("„ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´‰øÆÊ≠£", self.step4_modify_source_files),
            ("„Ç≥„É≥„Éë„Ç§„É´", self.step5_compile),
            ("Áµ±Âêà„ÉÜ„Çπ„Éà", self.step6_test_integration)
        ]
        
        for step_name, step_func in steps:
            logger.info(f"‚ñ∂Ô∏è {step_name}ÂÆüË°å‰∏≠...")
            if not step_func():
                logger.error(f"‚ùå {step_name}Â§±Êïó - Áµ±Âêà‰∏≠Êñ≠")
                return False
            logger.info(f"‚úÖ {step_name}ÂÆå‰∫Ü")
        
        # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
        if self.integration_config["enable_benchmarks"]:
            benchmark_results = self.run_benchmark()
        else:
            benchmark_results = None
        
        # Áµ±Âêà„É¨„Éù„Éº„ÉàÁîüÊàê
        report = self.generate_integration_report(benchmark_results)
        report_file = self.nkat_dir / "LLAMA_CPP_INTEGRATION_REPORT.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"üìÑ Áµ±Âêà„É¨„Éù„Éº„ÉàÁîüÊàê: {report_file}")
        logger.info("üéâ NKAT-llama.cppÁµ±ÂêàÂÆå‰∫ÜÔºÅ")
        
        return True

def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NKAT-llama.cppÁµ±Âêà„Ç∑„Çπ„ÉÜ„É†")
    parser.add_argument("--nkat-dir", default=".", help="NKAT„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Éá„Ç£„É¨„ÇØ„Éà„É™")
    parser.add_argument("--llama-dir", default="llama.cpp", help="llama.cpp„Éá„Ç£„É¨„ÇØ„Éà„É™")
    parser.add_argument("--no-benchmark", action="store_true", help="„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁÑ°ÂäπÂåñ")
    
    args = parser.parse_args()
    
    # Áµ±Âêà„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ
    integrator = LlamaCppNKATIntegrator(args.nkat_dir, args.llama_dir)
    
    if args.no_benchmark:
        integrator.integration_config["enable_benchmarks"] = False
    
    # Áµ±ÂêàÂÆüË°å
    success = integrator.integrate_all()
    
    if success:
        print("\nüéâ NKAT-llama.cppÁµ±Âêà„ÅåÊ≠£Â∏∏„Å´ÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ")
        print(f"üìÅ llama.cpp„Éá„Ç£„É¨„ÇØ„Éà„É™: {integrator.llama_dir}")
        print(f"üîß „Éì„É´„Éâ„Éá„Ç£„É¨„ÇØ„Éà„É™: {integrator.llama_dir}/build")
        print("üìñ Ë©≥Á¥∞„ÅØLLAMA_CPP_INTEGRATION_REPORT.md„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ")
    else:
        print("\n‚ùå Áµ±Âêà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ„É≠„Ç∞„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ")
        sys.exit(1)

if __name__ == "__main__":
    main() 
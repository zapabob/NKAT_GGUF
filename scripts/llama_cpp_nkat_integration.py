#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ NKAT-GGUF llama.cppçµ±åˆè‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Automated NKAT-GGUF Integration for llama.cpp

æ©Ÿèƒ½:
- llama.cppã®è‡ªå‹•ã‚¯ãƒ­ãƒ¼ãƒ³ãƒ»æº–å‚™
- NKAT CUDAã‚«ãƒ¼ãƒãƒ«ã®çµ±åˆ
- CMakeLists.txtè‡ªå‹•ä¿®æ­£
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ»ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
"""

import os
import sys
import subprocess
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import json
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llama_cpp_nkat_integration.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class LlamaCppNKATIntegrator:
    """llama.cpp NKATçµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, nkat_project_dir: str = ".", llama_cpp_dir: str = "llama.cpp"):
        self.nkat_dir = Path(nkat_project_dir).resolve()
        self.llama_dir = Path(llama_cpp_dir).resolve()
        self.cuda_kernels_dir = self.nkat_dir / "output" / "cuda_kernels"
        
        # çµ±åˆè¨­å®š
        self.integration_config = {
            "cuda_compute_arch": "86",  # RTX3080 (Ampere)
            "optimization_level": "3",
            "use_fast_math": True,
            "enable_benchmarks": True
        }
        
        logger.info(f"ğŸš€ NKAT-llama.cppçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        logger.info(f"   NKATãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {self.nkat_dir}")
        logger.info(f"   llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.llama_dir}")
        
    def run_command(self, cmd: List[str], cwd: Optional[Path] = None) -> subprocess.CompletedProcess:
        """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
        if cwd is None:
            cwd = Path.cwd()
        
        logger.info(f"ğŸ”§ å®Ÿè¡Œä¸­: {' '.join(cmd)} (dir: {cwd})")
        
        result = subprocess.run(
            cmd, 
            cwd=cwd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            shell=True if os.name == 'nt' else False
        )
        
        # git pullã®ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã‚’è¨±å®¹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã¯å®Œäº†ã—ã¦ã„ã‚‹ï¼‰
        if result.returncode != 0:
            if "git pull" in " ".join(cmd) and "cannot lock ref 'HEAD'" in result.stderr:
                logger.warning(f"âš ï¸ Gitå‚ç…§ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆè»½å¾®ï¼‰: ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ã¯å®Œäº†")
                return result
            elif "git pull" in " ".join(cmd) and "Updating files: 100%" in result.stderr:
                logger.info(f"âœ… Gitãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å®Œäº†ï¼ˆHEADå‚ç…§ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼‰")
                return result
            else:
                logger.error(f"âŒ ã‚³ãƒãƒ³ãƒ‰å¤±æ•—: {result.stderr}")
                raise RuntimeError(f"Command failed: {' '.join(cmd)}\n{result.stderr}")
        
        return result
    
    def step1_prepare_llama_cpp(self) -> bool:
        """Step 1: llama.cppãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæº–å‚™"""
        logger.info("ğŸ“¦ Step 1: llama.cppãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæº–å‚™")
        
        try:
            # llama.cppã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
            if not self.llama_dir.exists():
                logger.info("ğŸ“¥ llama.cppã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...")
                self.run_command([
                    "git", "clone", 
                    "https://github.com/ggerganov/llama.cpp.git",
                    str(self.llama_dir)
                ])
            else:
                logger.info("ğŸ“‚ æ—¢å­˜ã®llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨")
                
            # æœ€æ–°ç‰ˆã«æ›´æ–°
            logger.info("ğŸ”„ llama.cppã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ä¸­...")
            self.run_command(["git", "pull", "origin", "master"], self.llama_dir)
            
            # æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ç¢ºèª
            cuda_dir_new = self.llama_dir / "ggml" / "src" / "ggml-cuda"
            cuda_dir_old = self.llama_dir / "src" / "ggml-cuda"
            
            if cuda_dir_new.exists():
                logger.info(f"âœ… æ–°ã—ã„CUDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ç¢ºèª: {cuda_dir_new}")
                self.cuda_target_dir = cuda_dir_new
            elif cuda_dir_old.exists():
                logger.info(f"âœ… æ—§CUDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ç¢ºèª: {cuda_dir_old}")
                self.cuda_target_dir = cuda_dir_old
            else:
                logger.error(f"âŒ CUDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                logger.error(f"   ç¢ºèªã—ãŸå ´æ‰€: {cuda_dir_new}, {cuda_dir_old}")
                return False
                
            logger.info("âœ… Step 1å®Œäº†: llama.cppæº–å‚™å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step 1å¤±æ•—: {e}")
            return False
    
    def step2_integrate_cuda_kernels(self) -> bool:
        """Step 2: NKAT CUDAã‚«ãƒ¼ãƒãƒ«çµ±åˆ"""
        logger.info("ğŸ”§ Step 2: NKAT CUDAã‚«ãƒ¼ãƒãƒ«çµ±åˆ")
        
        try:
            # CUDAã‚«ãƒ¼ãƒãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            kernel_files = [
                "nkat_star_gemm_kernels.cu",
                "nkat_cuda_interface.cpp", 
                "nkat_cuda.h"
            ]
            
            target_dir = self.cuda_target_dir
            
            for filename in kernel_files:
                src_file = self.cuda_kernels_dir / filename
                dst_file = target_dir / filename
                
                if not src_file.exists():
                    logger.error(f"âŒ ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {src_file}")
                    return False
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                shutil.copy2(src_file, dst_file)
                logger.info(f"ğŸ“ ã‚³ãƒ”ãƒ¼: {filename} -> {dst_file}")
            
            logger.info("âœ… Step 2å®Œäº†: CUDAã‚«ãƒ¼ãƒãƒ«çµ±åˆå®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step 2å¤±æ•—: {e}")
            return False
    
    def step3_modify_cmake(self) -> bool:
        """Step 3: CMakeLists.txtä¿®æ­£"""
        logger.info("ğŸ“ Step 3: CMakeLists.txtä¿®æ­£")
        
        try:
            cmake_file = self.llama_dir / "CMakeLists.txt"
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_file = cmake_file.with_suffix(".txt.nkat_backup")
            shutil.copy2(cmake_file, backup_file)
            logger.info(f"ğŸ’¾ CMakeLists.txtãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
            
            # CMakeLists.txtèª­ã¿å–ã‚Š
            with open(cmake_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # NKATçµ±åˆè¨­å®šã‚’è¿½åŠ 
            nkat_cmake_config = self._generate_cmake_nkat_config()
            
            # æ–°ã—ã„GGML_CUDAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ãŸã¯æ—§LLAMA_CUBLASã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã¦æŒ¿å…¥
            if "if(GGML_CUDA)" in content:
                # æ–°ã—ã„GGML_CUDAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                content = content.replace(
                    "if(GGML_CUDA)",
                    f"if(GGML_CUDA)\n{nkat_cmake_config}"
                )
                logger.info("âœ… GGML_CUDAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«NKATè¨­å®šã‚’è¿½åŠ ")
            elif "if(LLAMA_CUBLAS)" in content:
                # æ—§LLAMA_CUBLASã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                content = content.replace(
                    "if(LLAMA_CUBLAS)",
                    f"if(LLAMA_CUBLAS)\n{nkat_cmake_config}"
                )
                logger.info("âœ… LLAMA_CUBLASã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«NKATè¨­å®šã‚’è¿½åŠ ")
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«è¿½åŠ 
                content += f"\n# NKAT CUDA Integration\n{nkat_cmake_config}\n"
                logger.info("âœ… ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«NKATè¨­å®šã‚’è¿½åŠ ")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            with open(cmake_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info("âœ… Step 3å®Œäº†: CMakeLists.txtä¿®æ­£å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step 3å¤±æ•—: {e}")
            return False
    
    def _generate_cmake_nkat_config(self) -> str:
        """NKATç”¨CMakeè¨­å®šç”Ÿæˆ"""
        # æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«åŸºã¥ã„ã¦ãƒ‘ã‚¹ã‚’æ±ºå®š
        if hasattr(self, 'cuda_target_dir'):
            relative_path = self.cuda_target_dir.relative_to(self.llama_dir)
            cuda_src_path = str(relative_path).replace('\\', '/')
        else:
            cuda_src_path = "ggml/src/ggml-cuda"
            
        return f'''
    # NKAT CUDA Integration (Auto-generated)
    if(GGML_CUDA)
        # NKAT CUDA sources
        set(NKAT_CUDA_SOURCES
            {cuda_src_path}/nkat_star_gemm_kernels.cu
            {cuda_src_path}/nkat_cuda_interface.cpp
        )
        
        # NKAT headers
        set(NKAT_CUDA_HEADERS
            {cuda_src_path}/nkat_cuda.h
        )
        
        # RTX3080 (Ampere) optimization
        if(TARGET ggml-cuda)
            set_property(TARGET ggml-cuda PROPERTY CUDA_ARCHITECTURES {self.integration_config["cuda_compute_arch"]})
            
            # Add NKAT sources to existing target
            target_sources(ggml-cuda PRIVATE ${{NKAT_CUDA_SOURCES}})
            target_include_directories(ggml-cuda PRIVATE {cuda_src_path})
            
            # NKAT definitions
            target_compile_definitions(ggml-cuda PRIVATE 
                GGML_CUDA_NKAT_ENABLED
                NKAT_CUDA_ARCH={self.integration_config["cuda_compute_arch"]}
            )
            
            # Performance optimization flags
            target_compile_options(ggml-cuda PRIVATE 
                $<$<COMPILE_LANGUAGE:CUDA>:
                    --use_fast_math
                    --optimize={self.integration_config["optimization_level"]}
                    --maxrregcount=128
                    -Xptxas=-v
                >
            )
        endif()
    endif()
'''
    
    def step4_modify_source_files(self) -> bool:
        """Step 4: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£"""
        logger.info("ğŸ”§ Step 4: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£")
        
        try:
            # ggml-cuda.cuä¿®æ­£
            if not self._modify_ggml_cuda():
                return False
            
            # gguf.cppä¿®æ­£  
            if not self._modify_gguf_cpp():
                return False
            
            logger.info("âœ… Step 4å®Œäº†: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Step 4å¤±æ•—: {e}")
            return False
    
    def _modify_ggml_cuda(self) -> bool:
        """ggml-cuda.cuä¿®æ­£"""
        # æ–°ã—ã„æ§‹é€ ã§ã¯ã€ãƒ¡ã‚¤ãƒ³ã®CUDAãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’ç¢ºèª
        possible_cuda_files = [
            self.llama_dir / "ggml" / "src" / "ggml-cuda.cu",
            self.llama_dir / "src" / "ggml-cuda.cu",
            self.llama_dir / "ggml" / "src" / "ggml-cuda" / "ggml-cuda.cu"
        ]
        
        cuda_file = None
        for file_path in possible_cuda_files:
            if file_path.exists():
                cuda_file = file_path
                break
        
        if not cuda_file:
            logger.warning(f"âš ï¸ ggml-cuda.cuãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            logger.info(f"ğŸ” CUDAå®Ÿè£…ã¯åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return True  # æ–°ã—ã„æ§‹é€ ã§ã¯ä¸è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_file = cuda_file.with_suffix(".cu.nkat_backup")
        shutil.copy2(cuda_file, backup_file)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š
        with open(cuda_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # NKAT includeè¿½åŠ 
        nkat_include = '''
#ifdef GGML_CUDA_NKAT_ENABLED
#include "nkat_cuda.h"
#endif
'''
        
        # ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
        if "#include" in content and "nkat_cuda.h" not in content:
            lines = content.split('\n')
            include_end = -1
            for i, line in enumerate(lines):
                if line.startswith('#include'):
                    include_end = i
            
            if include_end >= 0:
                lines.insert(include_end + 1, nkat_include)
                content = '\n'.join(lines)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(cuda_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"ğŸ“ {cuda_file.name}ä¿®æ­£å®Œäº†")
        return True
    
    def _modify_gguf_cpp(self) -> bool:
        """gguf.cppä¿®æ­£"""
        # æ–°ã—ã„æ§‹é€ ã§ã®gguf.cppãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’ç¢ºèª
        possible_gguf_files = [
            self.llama_dir / "ggml" / "src" / "gguf.cpp",
            self.llama_dir / "src" / "gguf.cpp",
            self.llama_dir / "gguf.cpp"
        ]
        
        gguf_file = None
        for file_path in possible_gguf_files:
            if file_path.exists():
                gguf_file = file_path
                break
        
        if not gguf_file:
            logger.warning(f"âš ï¸ gguf.cppãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.info(f"ğŸ” GGUFå®Ÿè£…ã¯åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return True  # æ–°ã—ã„æ§‹é€ ã§ã¯ä¸è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_file = gguf_file.with_suffix(".cpp.nkat_backup")
        shutil.copy2(gguf_file, backup_file)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š
        with open(gguf_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # NKATé–¢é€£å®šæ•°è¿½åŠ 
        nkat_constants = '''
// NKAT metadata keys (Auto-generated)
static const char * GGUF_NKAT_VERSION = "nkat.version";
static const char * GGUF_NKAT_THETA_RANK = "nkat.theta_rank";
static const char * GGUF_NKAT_GAMMA_DECAY = "nkat.gamma_decay";

// NKAT tensor name patterns
static bool is_nkat_theta_tensor(const char* name) {
    return strstr(name, ".theta") != nullptr;
}
'''
        
        # å®šæ•°å®šç¾©ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
        if "static const char *" in content and "GGUF_NKAT_VERSION" not in content:
            # æœ€åˆã®static const charå®šç¾©ã®å‰ã«æŒ¿å…¥
            content = content.replace(
                "static const char *",
                f"{nkat_constants}\nstatic const char *",
                1
            )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(gguf_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"ğŸ“ {gguf_file.name}ä¿®æ­£å®Œäº†")
        return True
    
    def step5_compile(self) -> bool:
        """Step 5: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        logger.info("ğŸ”¨ Step 5: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Ÿè¡Œ")
        
        try:
            # ãƒ“ãƒ«ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            build_dir = self.llama_dir / "build"
            build_dir.mkdir(exist_ok=True)
            
            # CMakeè¨­å®š
            logger.info("âš™ï¸ CMakeè¨­å®šä¸­...")
            cmake_cmd = [
                "cmake", "..",
                "-DGGML_CUDA=ON",  # æ–°ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                "-DCMAKE_BUILD_TYPE=Release",
                f"-DCUDA_ARCHITECTURES={self.integration_config['cuda_compute_arch']}"
            ]
            
            self.run_command(cmake_cmd, build_dir)
            
            # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Ÿè¡Œ
            logger.info("ğŸ”¨ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸­ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")
            build_cmd = [
                "cmake", "--build", ".", 
                "--config", "Release", 
                "-j", "6"  # ä¸¦åˆ—ã‚¸ãƒ§ãƒ–æ•°
            ]
            
            result = self.run_command(build_cmd, build_dir)
            
            # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            main_exe = build_dir / "bin" / "llama-cli.exe" if os.name == 'nt' else build_dir / "bin" / "llama-cli"
            if not main_exe.exists():
                main_exe = build_dir / "llama-cli.exe" if os.name == 'nt' else build_dir / "llama-cli"
            if not main_exe.exists():
                main_exe = build_dir / "main.exe" if os.name == 'nt' else build_dir / "main"
            
            if main_exe.exists():
                logger.info(f"âœ… ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: {main_exe}")
                return True
            else:
                logger.error("âŒ å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Step 5å¤±æ•—: {e}")
            return False
    
    def step6_test_integration(self) -> bool:
        """Step 6: çµ±åˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª Step 6: çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        try:
            # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢
            build_dir = self.llama_dir / "build"
            main_exe = self._find_main_executable(build_dir)
            
            if not main_exe:
                logger.error("âŒ mainå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # NKATãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ç¢ºèª
            test_model = self.nkat_dir / "output" / "nkat_test_model_enhanced.gguf"
            if not test_model.exists():
                logger.warning(f"âš ï¸ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_model}")
                # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ç´¢
                test_model = self._find_alternative_test_model()
                if not test_model:
                    logger.error("âŒ ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            logger.info("ğŸ” åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¸­...")
            test_cmd = [
                str(main_exe),
                "-m", str(test_model),
                "-p", "Hello, world!",
                "-n", "10",
                "--temp", "0.0"
            ]
            
            result = self.run_command(test_cmd, build_dir)
            
            if "Hello" in result.stdout or "world" in result.stdout:
                logger.info("âœ… åŸºæœ¬æ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
                return True
            else:
                logger.warning("âš ï¸ æ¨è«–çµæœã®æ¤œè¨¼ã«å¤±æ•—")
                logger.info(f"å‡ºåŠ›: {result.stdout[:200]}...")
                return True  # å®Ÿè¡Œè‡ªä½“ã¯æˆåŠŸ
                
        except Exception as e:
            logger.error(f"âŒ Step 6å¤±æ•—: {e}")
            return False
    
    def _find_main_executable(self, build_dir: Path) -> Optional[Path]:
        """mainå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢"""
        possible_locations = [
            # æ–°ã—ã„llama.cppã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«å
            build_dir / "bin" / "llama-cli.exe",
            build_dir / "bin" / "llama-cli", 
            build_dir / "llama-cli.exe",
            build_dir / "llama-cli",
            # æ—§å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«å
            build_dir / "bin" / "main.exe",
            build_dir / "bin" / "main",
            build_dir / "main.exe", 
            build_dir / "main",
            build_dir / "Release" / "llama-cli.exe",
            build_dir / "Release" / "main.exe",
            build_dir / "Debug" / "llama-cli.exe",
            build_dir / "Debug" / "main.exe"
        ]
        
        for exe_path in possible_locations:
            if exe_path.exists():
                logger.info(f"âœ… å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {exe_path}")
                return exe_path
        
        return None
    
    def _find_alternative_test_model(self) -> Optional[Path]:
        """ä»£æ›¿ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ¢ç´¢"""
        possible_models = []
        
        # outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…æ¤œç´¢
        output_dir = self.nkat_dir / "output"
        if output_dir.exists():
            possible_models.extend(output_dir.glob("*.gguf"))
        
        # modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…æ¤œç´¢
        models_dir = self.nkat_dir / "models"
        if models_dir.exists():
            for subdir in models_dir.iterdir():
                if subdir.is_dir():
                    possible_models.extend(subdir.glob("*.gguf"))
        
        # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
        for model in possible_models:
            if model.stat().st_size > 1024 * 1024:  # 1MBä»¥ä¸Š
                return model
        
        return None
    
    def run_benchmark(self) -> Dict[str, float]:
        """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("ğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        
        try:
            build_dir = self.llama_dir / "build"
            main_exe = self._find_main_executable(build_dir)
            
            if not main_exe:
                logger.error("âŒ å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            test_prompt = "The quick brown fox jumps over the lazy dog. " * 10
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            bench_cmd = [
                str(main_exe),
                "-m", str(self._find_alternative_test_model()),
                "-p", test_prompt,
                "-n", "100",
                "--temp", "0.1"
            ]
            
            start_time = datetime.now()
            result = self.run_command(bench_cmd, build_dir)
            end_time = datetime.now()
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
            duration = (end_time - start_time).total_seconds()
            tokens_generated = 100  # -n ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            tokens_per_second = tokens_generated / duration if duration > 0 else 0
            
            benchmark_results = {
                "tokens_per_second": tokens_per_second,
                "total_duration": duration,
                "tokens_generated": tokens_generated
            }
            
            logger.info(f"ğŸ“ˆ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
            logger.info(f"   æ¨è«–é€Ÿåº¦: {tokens_per_second:.2f} tokens/s")
            logger.info(f"   å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’")
            
            return benchmark_results
            
        except Exception as e:
            logger.error(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—: {e}")
            return {}
    
    def generate_integration_report(self, benchmark_results: Dict = None) -> str:
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# ğŸš€ NKAT-llama.cppçµ±åˆãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}

## ğŸ“‹ çµ±åˆæ¦‚è¦

âœ… **çµ±åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: æˆåŠŸ
ğŸ¯ **NKATæ©Ÿèƒ½**: éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–
ğŸ”§ **GPUæœ€é©åŒ–**: RTX3080 (Ampere) å¯¾å¿œ
ğŸ“¦ **çµ±åˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: 
- CUDA ã‚«ãƒ¼ãƒãƒ«
- GGUFæ‹¡å¼µ
- CMakeè¨­å®š
- ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£

## ğŸ› ï¸ çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½

### NKAT CUDA ã‚«ãƒ¼ãƒãƒ«
- `nkat_star_gemm_kernels.cu` - Moyal star productæ¼”ç®—
- `nkat_cuda_interface.cpp` - ãƒ›ã‚¹ãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- `nkat_cuda.h` - ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«

### ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£
- `CMakeLists.txt` - NKATçµ±åˆè¨­å®šè¿½åŠ 
- `ggml-cuda.cu` - NKAT includeè¿½åŠ 
- `gguf.cpp` - NKATãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ

## ğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
"""
        
        if benchmark_results:
            report += f"""
- **æ¨è«–é€Ÿåº¦**: {benchmark_results.get('tokens_per_second', 'N/A'):.2f} tokens/s
- **å®Ÿè¡Œæ™‚é–“**: {benchmark_results.get('total_duration', 'N/A'):.2f}ç§’
- **ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°**: {benchmark_results.get('tokens_generated', 'N/A')}
"""
        else:
            report += "\n- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æœªå®Ÿè¡Œ\n"
        
        report += f"""
## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬æ¨è«–
```bash
cd {self.llama_dir}/build
./main -m path/to/nkat_model.gguf -p "Your prompt here"
```

### NKATæ©Ÿèƒ½æœ‰åŠ¹åŒ–
```bash
./main -m nkat_model.gguf -p "prompt" --nkat-enable
```

## ğŸ”¬ ç†è«–èƒŒæ™¯

NKATçµ±åˆã«ã‚ˆã‚Šã€å¾“æ¥ã®ç·šå½¢æ¼”ç®— `y = Wx` ãŒéå¯æ›æ˜Ÿç©æ¼”ç®—ã«æ‹¡å¼µï¼š

```
y = (W â‹†_Î¸ x) := W exp(i/2 Î¸^Î¼Î½ âˆ‚_Î¼ âˆ‚_Î½) x
```

ã“ã‚Œã«ã‚ˆã‚Šè¡¨ç¾ç©ºé–“ãŒæ‹¡å¼µã•ã‚Œã€æ¨è«–ç²¾åº¦ã®å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

## ğŸ“ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### è¿½åŠ ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
- `src/ggml-cuda/nkat_star_gemm_kernels.cu`
- `src/ggml-cuda/nkat_cuda_interface.cpp`  
- `src/ggml-cuda/nkat_cuda.h`

### ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
- `CMakeLists.txt` (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: CMakeLists.txt.nkat_backup)
- `src/ggml-cuda.cu` (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: ggml-cuda.cu.nkat_backup)
- `src/gguf.cpp` (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: gguf.cpp.nkat_backup)

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼
- CUDA Compute CapabilityãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- å¿…è¦ãªCUDAãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

### å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼  
- NKATãƒ†ãƒ³ã‚½ãƒ«ä»˜ãGGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ç¢ºèª
- ååˆ†ãªGPUãƒ¡ãƒ¢ãƒªãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª

---

**çµ±åˆå®Œäº†**: NKATæ©Ÿèƒ½ãŒllama.cppã«æ­£å¸¸ã«çµ±åˆã•ã‚Œã¾ã—ãŸ ğŸ‰
"""
        
        return report
    
    def integrate_all(self) -> bool:
        """å…¨çµ±åˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ"""
        logger.info("ğŸš€ NKAT-llama.cpp å®Œå…¨çµ±åˆé–‹å§‹")
        
        steps = [
            ("llama.cppæº–å‚™", self.step1_prepare_llama_cpp),
            ("CUDAã‚«ãƒ¼ãƒãƒ«çµ±åˆ", self.step2_integrate_cuda_kernels),
            ("CMakeè¨­å®š", self.step3_modify_cmake),
            ("ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£", self.step4_modify_source_files),
            ("ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«", self.step5_compile),
            ("çµ±åˆãƒ†ã‚¹ãƒˆ", self.step6_test_integration)
        ]
        
        for step_name, step_func in steps:
            logger.info(f"â–¶ï¸ {step_name}å®Ÿè¡Œä¸­...")
            if not step_func():
                logger.error(f"âŒ {step_name}å¤±æ•— - çµ±åˆä¸­æ–­")
                return False
            logger.info(f"âœ… {step_name}å®Œäº†")
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        if self.integration_config["enable_benchmarks"]:
            benchmark_results = self.run_benchmark()
        else:
            benchmark_results = None
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_integration_report(benchmark_results)
        report_file = self.nkat_dir / "LLAMA_CPP_INTEGRATION_REPORT.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")
        logger.info("ğŸ‰ NKAT-llama.cppçµ±åˆå®Œäº†ï¼")
        
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NKAT-llama.cppçµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--nkat-dir", default=".", help="NKATãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--llama-dir", default="llama.cpp", help="llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--no-benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç„¡åŠ¹åŒ–")
    
    args = parser.parse_args()
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    integrator = LlamaCppNKATIntegrator(args.nkat_dir, args.llama_dir)
    
    if args.no_benchmark:
        integrator.integration_config["enable_benchmarks"] = False
    
    # çµ±åˆå®Ÿè¡Œ
    success = integrator.integrate_all()
    
    if success:
        print("\nğŸ‰ NKAT-llama.cppçµ±åˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.llama_dir}")
        print(f"ğŸ”§ ãƒ“ãƒ«ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.llama_dir}/build")
        print("ğŸ“– è©³ç´°ã¯LLAMA_CPP_INTEGRATION_REPORT.mdã‚’ã”ç¢ºèªãã ã•ã„")
    else:
        print("\nâŒ çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        sys.exit(1)

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ†˜ KoboldCPPç·Šæ€¥ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ ï¼ˆllama.cppå¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
KoboldCPP Emergency Fix System for bad_alloc and access violation errors

ç‰¹å¾´:
- tokenizer.ggml.tokens bad_allocã‚¨ãƒ©ãƒ¼ä¿®å¾©
- ã‚¢ã‚¯ã‚»ã‚¹é•åã‚¨ãƒ©ãƒ¼è§£æ±º
- NKATãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ
- MoEï¼ˆMixture of Expertsï¼‰é‡å­åŒ–ã‚¿ã‚¤ãƒ—ä¸ä¸€è‡´ä¿®å¾©
- llama.cppæœ€æ–°ç‰ˆäº’æ›æ€§ç¢ºä¿
- LoRAåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
- é›»æºæ–­å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
"""

import os
import sys
import struct
import shutil
import tempfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import logging
import json

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('koboldcpp_emergency_fix.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class KoboldCPPEmergencyFix:
    """KoboldCPPç·Šæ€¥ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ ï¼ˆllama.cppå¯¾å¿œå¼·åŒ–ç‰ˆï¼‰"""
    
    GGUF_MAGIC = b'GGUF'
    
    # GGUFå‹å®šç¾©
    GGUF_TYPE_UINT8 = 0
    GGUF_TYPE_INT8 = 1
    GGUF_TYPE_UINT16 = 2
    GGUF_TYPE_INT16 = 3
    GGUF_TYPE_UINT32 = 4
    GGUF_TYPE_INT32 = 5
    GGUF_TYPE_FLOAT32 = 6
    GGUF_TYPE_BOOL = 7
    GGUF_TYPE_STRING = 8
    GGUF_TYPE_ARRAY = 9
    GGUF_TYPE_UINT64 = 10
    GGUF_TYPE_INT64 = 11
    GGUF_TYPE_FLOAT64 = 12
    
    def __init__(self):
        self.backup_dir = Path("emergency_backups")
        self.backup_dir.mkdir(exist_ok=True)
        
        self.type_sizes = {
            self.GGUF_TYPE_UINT8: 1, self.GGUF_TYPE_INT8: 1,
            self.GGUF_TYPE_UINT16: 2, self.GGUF_TYPE_INT16: 2,
            self.GGUF_TYPE_UINT32: 4, self.GGUF_TYPE_INT32: 4, 
            self.GGUF_TYPE_FLOAT32: 4,
            self.GGUF_TYPE_UINT64: 8, self.GGUF_TYPE_INT64: 8, 
            self.GGUF_TYPE_FLOAT64: 8,
            self.GGUF_TYPE_BOOL: 1
        }
        
        logger.info("ğŸ†˜ KoboldCPPç·Šæ€¥ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ ï¼ˆllama.cppå¼·åŒ–ç‰ˆï¼‰åˆæœŸåŒ–å®Œäº†")
    
    def create_emergency_backup(self, file_path: str) -> str:
        """ç·Šæ€¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.backup_dir / f"{Path(file_path).stem}_emergency_{timestamp}.gguf"
        shutil.copy2(file_path, backup_path)
        logger.info(f"ğŸ’¾ ç·Šæ€¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
        return str(backup_path)
    
    def comprehensive_analysis(self, file_path: str) -> Dict:
        """åŒ…æ‹¬çš„ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        logger.info(f"ğŸ” åŒ…æ‹¬çš„åˆ†æé–‹å§‹: {file_path}")
        
        analysis_result = {
            "file_path": file_path,
            "file_size_mb": os.path.getsize(file_path) / (1024 * 1024),
            "gguf_valid": False,
            "tokenizer_issues": [],
            "moe_issues": [],
            "memory_issues": [],
            "recommendations": []
        }
        
        try:
            with open(file_path, 'rb') as f:
                # GGUFãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
                magic = f.read(4)
                if magic != self.GGUF_MAGIC:
                    analysis_result["recommendations"].append("âŒ ç„¡åŠ¹ãªGGUFãƒ•ã‚¡ã‚¤ãƒ«")
                    return analysis_result
                
                analysis_result["gguf_valid"] = True
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³èª­ã¿å–ã‚Š
                version = struct.unpack('<I', f.read(4))[0]
                analysis_result["gguf_version"] = version
                
                # ãƒ†ãƒ³ã‚½ãƒ«æ•°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                metadata_count = struct.unpack('<Q', f.read(8))[0]
                
                analysis_result["tensor_count"] = tensor_count
                analysis_result["metadata_count"] = metadata_count
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
                metadata_analysis = self._analyze_metadata(f, metadata_count)
                analysis_result.update(metadata_analysis)
                
                # tokenizerå•é¡Œãƒã‚§ãƒƒã‚¯
                tokenizer_analysis = self._check_tokenizer_issues(f, metadata_count)
                analysis_result["tokenizer_issues"] = tokenizer_analysis
                
                # MoEå•é¡Œãƒã‚§ãƒƒã‚¯
                moe_analysis = self._check_moe_issues(f, tensor_count, metadata_analysis.get("metadata", {}))
                analysis_result["moe_issues"] = moe_analysis
                
                # ãƒ¡ãƒ¢ãƒªå•é¡Œãƒã‚§ãƒƒã‚¯
                memory_analysis = self._check_memory_issues(analysis_result)
                analysis_result["memory_issues"] = memory_analysis
                
                # æ¨å¥¨äº‹é …ç”Ÿæˆ
                recommendations = self._generate_recommendations(analysis_result)
                analysis_result["recommendations"] = recommendations
                
        except Exception as e:
            logger.error(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            analysis_result["error"] = str(e)
        
        return analysis_result
    
    def _analyze_metadata(self, f, metadata_count: int) -> Dict:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è©³ç´°åˆ†æ"""
        metadata = {}
        f.seek(24)  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œã«ç§»å‹•
        
        try:
            for i in range(metadata_count):
                # ã‚­ãƒ¼èª­ã¿å–ã‚Š
                key_len = struct.unpack('<Q', f.read(8))[0]
                if key_len > 1000:  # ç•°å¸¸ãªé•·ã•
                    return {"metadata": {}, "metadata_error": "ç•°å¸¸ãªã‚­ãƒ¼é•·"}
                
                key = f.read(key_len).decode('utf-8', errors='ignore')
                
                # å€¤èª­ã¿å–ã‚Š
                value = self._safe_read_value(f)
                metadata[key] = value
                
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"metadata": metadata}
    
    def _safe_read_value(self, f):
        """å®‰å…¨ãªå€¤èª­ã¿å–ã‚Š"""
        try:
            value_type = struct.unpack('<I', f.read(4))[0]
            
            if value_type == self.GGUF_TYPE_STRING:
                str_len = struct.unpack('<Q', f.read(8))[0]
                if str_len > 10000:  # ç•°å¸¸ãªé•·ã•åˆ¶é™
                    f.read(str_len)
                    return "<<TOO_LONG>>"
                return f.read(str_len).decode('utf-8', errors='ignore')
            
            elif value_type == self.GGUF_TYPE_ARRAY:
                array_type = struct.unpack('<I', f.read(4))[0]
                array_len = struct.unpack('<Q', f.read(8))[0]
                
                if array_len > 100000:  # é…åˆ—é•·åˆ¶é™
                    # é…åˆ—å…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    for _ in range(array_len):
                        if array_type == self.GGUF_TYPE_STRING:
                            str_len = struct.unpack('<Q', f.read(8))[0]
                            f.read(str_len)
                        else:
                            f.read(self.type_sizes.get(array_type, 4))
                    return "<<LARGE_ARRAY>>"
                
                # æ­£å¸¸ã‚µã‚¤ã‚ºã®é…åˆ—
                array_values = []
                for _ in range(min(array_len, 1000)):  # æœ€å¤§1000è¦ç´ ã¾ã§
                    if array_type == self.GGUF_TYPE_STRING:
                        str_len = struct.unpack('<Q', f.read(8))[0]
                        if str_len > 500:  # æ–‡å­—åˆ—é•·åˆ¶é™
                            f.read(str_len)
                            array_values.append("<<LONG_STRING>>")
                        else:
                            array_values.append(f.read(str_len).decode('utf-8', errors='ignore'))
                    else:
                        f.read(self.type_sizes.get(array_type, 4))
                        array_values.append(None)
                return array_values
            
            elif value_type in self.type_sizes:
                size = self.type_sizes[value_type]
                data = f.read(size)
                
                if value_type == self.GGUF_TYPE_UINT32:
                    return struct.unpack('<I', data)[0]
                elif value_type == self.GGUF_TYPE_UINT64:
                    return struct.unpack('<Q', data)[0]
                elif value_type == self.GGUF_TYPE_FLOAT32:
                    return struct.unpack('<f', data)[0]
                else:
                    return data
            else:
                return None
                
        except Exception:
            return None
    
    def _check_tokenizer_issues(self, f, metadata_count: int) -> List[str]:
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å•é¡Œãƒã‚§ãƒƒã‚¯"""
        issues = []
        f.seek(24)  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œã«ç§»å‹•
        
        try:
            for i in range(metadata_count):
                # ã‚­ãƒ¼èª­ã¿å–ã‚Š
                key_len = struct.unpack('<Q', f.read(8))[0]
                key = f.read(key_len).decode('utf-8', errors='ignore')
                
                if key == 'tokenizer.ggml.tokens':
                    # å€¤ã‚¿ã‚¤ãƒ—èª­ã¿å–ã‚Š
                    value_type = struct.unpack('<I', f.read(4))[0]
                    
                    if value_type == self.GGUF_TYPE_ARRAY:
                        array_type = struct.unpack('<I', f.read(4))[0]
                        array_len = struct.unpack('<Q', f.read(8))[0]
                        
                        if array_len > 200000:
                            issues.append(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã™ãã¾ã™: {array_len}")
                        
                        if array_len == 0:
                            issues.append("âŒ ãƒˆãƒ¼ã‚¯ãƒ³é…åˆ—ãŒç©ºã§ã™")
                        
                        # æœ€åˆã®æ•°å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                        problematic_tokens = 0
                        for j in range(min(array_len, 100)):
                            try:
                                str_len = struct.unpack('<Q', f.read(8))[0]
                                if str_len > 1000:
                                    problematic_tokens += 1
                                f.read(str_len)
                            except:
                                problematic_tokens += 1
                                break
                        
                        if problematic_tokens > 10:
                            issues.append(f"âš ï¸ å•é¡Œã®ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¤šæ•°: {problematic_tokens}")
                        
                        break
                else:
                    # å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    self._safe_read_value(f)
                    
        except Exception as e:
            issues.append(f"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        return issues
    
    def _check_moe_issues(self, f, tensor_count: int, metadata: Dict) -> List[str]:
        """MoEå•é¡Œãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # MoEé–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        expert_count = metadata.get('llama.expert_count', 0)
        expert_used_count = metadata.get('llama.expert_used_count', 0)
        
        if expert_count > 1:
            logger.info(f"ğŸ§  MoEãƒ¢ãƒ‡ãƒ«æ¤œå‡º: {expert_count}ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ")
            
            # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±èª­ã¿å–ã‚Šï¼ˆç°¡ç•¥ç‰ˆï¼‰
            current_pos = f.tell()
            try:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã¸
                f.seek(24)  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œ
                metadata_count = struct.unpack('<Q', f.read(16))[1]  # metadata_countå–å¾—
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒƒãƒ—
                for i in range(metadata_count):
                    key_len = struct.unpack('<Q', f.read(8))[0]
                    f.read(key_len)
                    self._safe_read_value(f)
                
                # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±åˆ†æ
                expert_dtypes = []
                for i in range(min(tensor_count, 1000)):  # æœ€å¤§1000ãƒ†ãƒ³ã‚µãƒ¼ã¾ã§
                    try:
                        name_len = struct.unpack('<Q', f.read(8))[0]
                        name = f.read(name_len).decode('utf-8', errors='ignore')
                        
                        n_dims = struct.unpack('<I', f.read(4))[0]
                        for _ in range(n_dims):
                            f.read(8)  # å½¢çŠ¶ã‚¹ã‚­ãƒƒãƒ—
                        
                        dtype = struct.unpack('<I', f.read(4))[0]
                        f.read(8)  # ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚¹ã‚­ãƒƒãƒ—
                        
                        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆé–¢é€£ãƒ†ãƒ³ã‚µãƒ¼
                        if any(keyword in name for keyword in ['expert', 'ffn_gate', 'mlp']):
                            expert_dtypes.append(dtype)
                            
                    except:
                        break
                
                # é‡å­åŒ–ã‚¿ã‚¤ãƒ—çµ±ä¸€ãƒã‚§ãƒƒã‚¯
                if expert_dtypes and len(set(expert_dtypes)) > 1:
                    issues.append(f"âš ï¸ MoEã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é‡å­åŒ–ã‚¿ã‚¤ãƒ—ãŒä¸çµ±ä¸€: {set(expert_dtypes)}")
                    issues.append("ğŸ’¡ llama.cppã§äº’æ›æ€§å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                
            except Exception as e:
                issues.append(f"âŒ MoEåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                f.seek(current_pos)
        
        return issues
    
    def _check_memory_issues(self, analysis_result: Dict) -> List[str]:
        """ãƒ¡ãƒ¢ãƒªå•é¡Œãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        file_size_mb = analysis_result.get("file_size_mb", 0)
        tensor_count = analysis_result.get("tensor_count", 0)
        
        if file_size_mb > 20000:  # 20GBä»¥ä¸Š
            issues.append(f"âš ï¸ å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«: {file_size_mb:.1f}MB")
            issues.append("ğŸ’¡ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        if tensor_count > 2000:
            issues.append(f"âš ï¸ ãƒ†ãƒ³ã‚µãƒ¼æ•°ãŒå¤šã„: {tensor_count}")
            issues.append("ğŸ’¡ å‡¦ç†æ™‚é–“ãŒé•·ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        return issues
    
    def _generate_recommendations(self, analysis_result: Dict) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if analysis_result.get("tokenizer_issues"):
            recommendations.append("ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©ã‚’æ¨å¥¨")
        
        if analysis_result.get("moe_issues"):
            recommendations.append("ğŸ”§ MoEé‡å­åŒ–ã‚¿ã‚¤ãƒ—ä¿®å¾©ã‚’æ¨å¥¨")
        
        if analysis_result.get("memory_issues"):
            recommendations.append("ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æ¨å¥¨")
        
        if not recommendations:
            recommendations.append("âœ… é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return recommendations
    
    def fix_all_issues(self, file_path: str) -> str:
        """å…¨å•é¡Œã®çµ±åˆä¿®å¾©"""
        logger.info(f"ğŸ”§ çµ±åˆä¿®å¾©é–‹å§‹: {file_path}")
        
        # åˆ†æå®Ÿè¡Œ
        analysis = self.comprehensive_analysis(file_path)
        
        if not analysis.get("gguf_valid"):
            logger.error("âŒ ç„¡åŠ¹ãªGGUFãƒ•ã‚¡ã‚¤ãƒ«")
            return None
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_path = self.create_emergency_backup(file_path)
        
        # ä¿®å¾©ç‰ˆãƒ‘ã‚¹ç”Ÿæˆ
        fixed_path = file_path.replace('.gguf', '_emergency_fixed.gguf')
        
        try:
            # æ®µéšçš„ä¿®å¾©
            current_file = file_path
            
            # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©
            if analysis.get("tokenizer_issues"):
                logger.info("ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©ä¸­...")
                tokenizer_fixed = self.fix_tokenizer_bad_alloc(current_file)
                if tokenizer_fixed:
                    current_file = tokenizer_fixed
            
            # 2. MoEä¿®å¾©ï¼ˆMoEä¿®å¾©ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ï¼‰
            if analysis.get("moe_issues"):
                logger.info("ğŸ”§ MoEä¿®å¾©ä¸­...")
                try:
                    from llama_cpp_moe_fix import LlamaCppMoEFixer
                    moe_fixer = LlamaCppMoEFixer()
                    moe_fixed_path = current_file.replace('.gguf', '_moe_fixed.gguf')
                    if moe_fixer.fix_moe_quantization(current_file, moe_fixed_path):
                        current_file = moe_fixed_path
                except ImportError:
                    logger.warning("âš ï¸ MoEä¿®å¾©ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # 3. æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«åèª¿æ•´
            if current_file != file_path:
                if current_file != fixed_path:
                    shutil.move(current_file, fixed_path)
                    current_file = fixed_path
            
            logger.info(f"âœ… çµ±åˆä¿®å¾©å®Œäº†: {current_file}")
            return current_file
            
        except Exception as e:
            logger.error(f"âŒ çµ±åˆä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def analyze_tokenizer_issue(self, file_path: str) -> Dict:
        """tokenizerã‚¨ãƒ©ãƒ¼åˆ†æï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        logger.info(f"ğŸ” tokenizeråˆ†æ: {file_path}")
        
        try:
            with open(file_path, 'rb') as f:
                # GGUFãƒã‚¸ãƒƒã‚¯ç¢ºèª
                magic = f.read(4)
                if magic != self.GGUF_MAGIC:
                    return {"status": "error", "message": "ä¸æ­£ãªGGUFãƒ•ã‚¡ã‚¤ãƒ«"}
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³èª­ã¿å–ã‚Š
                version = struct.unpack('<I', f.read(4))[0]
                
                # ãƒ†ãƒ³ã‚½ãƒ«æ•°èª­ã¿å–ã‚Š
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°èª­ã¿å–ã‚Š
                metadata_count = struct.unpack('<Q', f.read(8))[0]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
                tokenizer_found = False
                tokenizer_size = 0
                
                for i in range(metadata_count):
                    key_len = struct.unpack('<Q', f.read(8))[0]
                    key = f.read(key_len).decode('utf-8', errors='ignore')
                    
                    if key == 'tokenizer.ggml.tokens':
                        tokenizer_found = True
                        # å€¤ã‚¿ã‚¤ãƒ—èª­ã¿å–ã‚Š
                        value_type = struct.unpack('<I', f.read(4))[0]
                        
                        if value_type == self.GGUF_TYPE_ARRAY:
                            array_type = struct.unpack('<I', f.read(4))[0]
                            array_len = struct.unpack('<Q', f.read(8))[0]
                            tokenizer_size = array_len
                            
                            return {
                                "status": "found",
                                "tokenizer_size": tokenizer_size,
                                "array_length": array_len,
                                "position": f.tell(),
                                "version": version
                            }
                    else:
                        # å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        self._safe_read_value(f)
                
                if not tokenizer_found:
                    return {"status": "not_found", "message": "tokenizer.ggml.tokensãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
                
                return {"status": "analyzed", "version": version}
                
        except Exception as e:
            return {"status": "error", "message": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def fix_tokenizer_bad_alloc(self, file_path: str) -> str:
        """tokenizer bad_allocã‚¨ãƒ©ãƒ¼ä¿®å¾©ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        logger.info("ğŸ”§ tokenizer bad_allocã‚¨ãƒ©ãƒ¼ä¿®å¾©é–‹å§‹...")
        
        analysis = self.analyze_tokenizer_issue(file_path)
        
        if analysis["status"] == "error":
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {analysis['message']}")
            return None
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_path = self.create_emergency_backup(file_path)
        
        # ä¿®å¾©ç‰ˆä½œæˆ
        fixed_path = file_path.replace('.gguf', '_tokenfixed.gguf')
        
        try:
            with open(file_path, 'rb') as src, open(fixed_path, 'wb') as dst:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚³ãƒ”ãƒ¼
                header = src.read(24)  # magic + version + tensor_count + metadata_count
                dst.write(header)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                metadata_count = struct.unpack('<Q', header[16:24])[0]
                
                for i in range(metadata_count):
                    # ã‚­ãƒ¼èª­ã¿å–ã‚Š
                    key_len_data = src.read(8)
                    key_len = struct.unpack('<Q', key_len_data)[0]
                    key_data = src.read(key_len)
                    key = key_data.decode('utf-8', errors='ignore')
                    
                    dst.write(key_len_data)
                    dst.write(key_data)
                    
                    if key == 'tokenizer.ggml.tokens':
                        logger.info("ğŸ”§ tokenizer.ggml.tokensä¿®å¾©ä¸­...")
                        
                        # å€¤ã‚¿ã‚¤ãƒ—èª­ã¿å–ã‚Š
                        value_type_data = src.read(4)
                        value_type = struct.unpack('<I', value_type_data)[0]
                        
                        if value_type == self.GGUF_TYPE_ARRAY:
                            array_type_data = src.read(4)
                            array_len_data = src.read(8)
                            array_len = struct.unpack('<Q', array_len_data)[0]
                            
                            # å®‰å…¨ãªã‚µã‚¤ã‚ºã«åˆ¶é™
                            safe_len = min(array_len, 100000)  # 100K ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§
                            if array_len != safe_len:
                                logger.info(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’{array_len}ã‹ã‚‰{safe_len}ã«åˆ¶é™")
                                array_len = safe_len
                                array_len_data = struct.pack('<Q', array_len)
                            
                            dst.write(value_type_data)
                            dst.write(array_type_data)
                            dst.write(array_len_data)
                            
                            # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ï¼ˆå®‰å…¨æ€§é‡è¦–ï¼‰
                            successful_tokens = 0
                            for j in range(array_len):
                                try:
                                    str_len_data = src.read(8)
                                    if len(str_len_data) < 8:
                                        break
                                    str_len = struct.unpack('<Q', str_len_data)[0]
                                    
                                    # æ–‡å­—åˆ—é•·åˆ¶é™
                                    if str_len > 500:  # 500æ–‡å­—ã¾ã§
                                        str_len = 500
                                        str_len_data = struct.pack('<Q', str_len)
                                    
                                    dst.write(str_len_data)
                                    
                                    token_data = src.read(str_len)
                                    if len(token_data) < str_len:
                                        token_data += b'\x00' * (str_len - len(token_data))
                                    
                                    dst.write(token_data)
                                    successful_tokens += 1
                                    
                                except Exception as e:
                                    logger.warning(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³{j}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                                    break
                            
                            logger.info(f"âœ… {successful_tokens}å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ­£å¸¸ã«å‡¦ç†")
                        else:
                            # é€šå¸¸ã‚³ãƒ”ãƒ¼
                            value_data = src.read(8)
                            dst.write(value_type_data)
                            dst.write(value_data)
                    else:
                        # ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸ã‚³ãƒ”ãƒ¼
                        value_type_data = src.read(4)
                        value_type = struct.unpack('<I', value_type_data)[0]
                        dst.write(value_type_data)
                        
                        if value_type == self.GGUF_TYPE_ARRAY:
                            # é…åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚³ãƒ”ãƒ¼
                            array_type_data = src.read(4)
                            array_len_data = src.read(8)
                            dst.write(array_type_data)
                            dst.write(array_len_data)
                            
                            array_type = struct.unpack('<I', array_type_data)[0]
                            array_len = struct.unpack('<Q', array_len_data)[0]
                            
                            # é…åˆ—å†…å®¹ã‚³ãƒ”ãƒ¼
                            for k in range(array_len):
                                if array_type == self.GGUF_TYPE_STRING:
                                    str_len_data = src.read(8)
                                    str_len = struct.unpack('<Q', str_len_data)[0]
                                    str_data = src.read(str_len)
                                    dst.write(str_len_data)
                                    dst.write(str_data)
                                else:
                                    size = self.type_sizes.get(array_type, 4)
                                    data = src.read(size)
                                    dst.write(data)
                        else:
                            # å˜ç´”ãªå€¤
                            size = self.type_sizes.get(value_type, 8)
                            if value_type == self.GGUF_TYPE_STRING:
                                str_len = struct.unpack('<Q', src.read(8))[0]
                                dst.write(struct.pack('<Q', str_len))
                                dst.write(src.read(str_len))
                            else:
                                data = src.read(size)
                                dst.write(data)
                
                # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ãƒ³ã‚µãƒ¼æƒ…å ±ã¨ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ã‚³ãƒ”ãƒ¼
                remaining_data = src.read()
                dst.write(remaining_data)
            
            logger.info(f"âœ… tokenizerä¿®å¾©å®Œäº†: {fixed_path}")
            return fixed_path
            
        except Exception as e:
            logger.error(f"âŒ tokenizerä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_koboldcpp_launch_config(self, model_path: str) -> str:
        """KoboldCPPèµ·å‹•è¨­å®šä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        logger.info(f"ğŸ“ KoboldCPPèµ·å‹•è¨­å®šä½œæˆ: {model_path}")
        
        # ãƒ¢ãƒ‡ãƒ«åˆ†æ
        analysis = self.comprehensive_analysis(model_path)
        
        config = {
            "model_path": model_path,
            "contextsize": 4096,
            "blasbatchsize": 256,
            "blasthreads": 4,
            "port": 5001,
            "gpulayers": 28,
            "usecublas": "normal",
            "nommap": True,
            "usemlock": False,
            "threads": 6,
            "quiet": False
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ãèª¿æ•´
        file_size_mb = analysis.get("file_size_mb", 0)
        
        if file_size_mb > 10000:  # 10GBä»¥ä¸Š
            config["blasbatchsize"] = 128
            config["contextsize"] = 2048
            config["gpulayers"] = 20
            logger.info("ğŸ”§ å¤§å®¹é‡ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨")
        
        elif file_size_mb < 1000:  # 1GBæœªæº€
            config["blasbatchsize"] = 512
            config["contextsize"] = 8192
            config["gpulayers"] = 35
            logger.info("ğŸ”§ è»½é‡ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨")
        
        # MoEãƒ¢ãƒ‡ãƒ«ç‰¹åˆ¥è¨­å®š
        if analysis.get("moe_issues"):
            config["nommap"] = True
            config["usemlock"] = False
            config["blasbatchsize"] = min(config["blasbatchsize"], 128)
            logger.info("ğŸ§  MoEãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®šã‚’é©ç”¨")
        
        # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        batch_content = f"""@echo off
REM KoboldCPPæœ€é©åŒ–èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
REM ãƒ¢ãƒ‡ãƒ«: {Path(model_path).name}
REM ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f}MB
REM ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

echo ğŸš€ KoboldCPPæœ€é©åŒ–èµ·å‹•
echo ãƒ¢ãƒ‡ãƒ«: {Path(model_path).name}
echo ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f}MB
echo.

REM ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
echo ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:
systeminfo | findstr "Total Physical Memory"
echo.

REM KoboldCPPèµ·å‹•
echo ğŸ”§ KoboldCPPèµ·å‹•ä¸­...
python koboldcpp.py --model "{model_path}" --contextsize {config['contextsize']} --blasbatchsize {config['blasbatchsize']} --blasthreads {config['blasthreads']} --port {config['port']} --skiplauncher --gpulayers {config['gpulayers']} --usecublas {config['usecublas']} 0 {"--nommap" if config['nommap'] else ""} {"--usemlock False" if not config['usemlock'] else ""} --threads {config['threads']}

pause
"""
        
        # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        batch_path = f"run_{Path(model_path).stem}_optimized.bat"
        with open(batch_path, 'w', encoding='utf-8') as f:
            f.write(batch_content)
        
        logger.info(f"âœ… KoboldCPPèµ·å‹•è¨­å®šä½œæˆå®Œäº†: {batch_path}")
        return batch_path
    
    def print_analysis_report(self, analysis: Dict):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "="*70)
        print("ğŸ†˜ KoboldCPPç·Šæ€¥ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ  - åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*70)
        
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {analysis['file_path']}")
        print(f"ğŸ“Š ã‚µã‚¤ã‚º: {analysis['file_size_mb']:.1f}MB")
        print(f"âœ… GGUFæœ‰åŠ¹: {'ã¯ã„' if analysis['gguf_valid'] else 'ã„ã„ãˆ'}")
        
        if analysis.get('gguf_version'):
            print(f"ğŸ”– GGUFãƒãƒ¼ã‚¸ãƒ§ãƒ³: {analysis['gguf_version']}")
        
        if analysis.get('tensor_count'):
            print(f"ğŸ§® ãƒ†ãƒ³ã‚µãƒ¼æ•°: {analysis['tensor_count']}")
        
        if analysis.get('metadata_count'):
            print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°: {analysis['metadata_count']}")
        
        # å•é¡Œè¡¨ç¤º
        if analysis.get('tokenizer_issues'):
            print("\nâš ï¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å•é¡Œ:")
            for issue in analysis['tokenizer_issues']:
                print(f"   {issue}")
        
        if analysis.get('moe_issues'):
            print("\nğŸ§  MoEå•é¡Œ:")
            for issue in analysis['moe_issues']:
                print(f"   {issue}")
        
        if analysis.get('memory_issues'):
            print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªå•é¡Œ:")
            for issue in analysis['memory_issues']:
                print(f"   {issue}")
        
        # æ¨å¥¨äº‹é …
        print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
        for rec in analysis.get('recommendations', []):
            print(f"   {rec}")
        
        print("="*70)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='KoboldCPPç·Šæ€¥ä¿®å¾©ãƒ„ãƒ¼ãƒ«ï¼ˆllama.cppå¼·åŒ–ç‰ˆï¼‰')
    parser.add_argument('input', help='å…¥åŠ›GGUFãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('-a', '--analyze-only', action='store_true', help='åˆ†æã®ã¿å®Ÿè¡Œ')
    parser.add_argument('-t', '--tokenizer-only', action='store_true', help='ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©ã®ã¿')
    parser.add_argument('-c', '--create-config', action='store_true', help='KoboldCPPè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ')
    parser.add_argument('--comprehensive', action='store_true', help='åŒ…æ‹¬çš„ä¿®å¾©å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input}")
        sys.exit(1)
    
    # ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    fixer = KoboldCPPEmergencyFix()
    
    # åˆ†æå®Ÿè¡Œ
    analysis = fixer.comprehensive_analysis(args.input)
    fixer.print_analysis_report(analysis)
    
    if args.analyze_only:
        print("\nâœ… åˆ†æå®Œäº†")
        return
    
    if args.create_config:
        config_path = fixer.create_koboldcpp_launch_config(args.input)
        print(f"\nâœ… èµ·å‹•è¨­å®šä½œæˆå®Œäº†: {config_path}")
    
    if args.tokenizer_only:
        if analysis.get('tokenizer_issues'):
            fixed_file = fixer.fix_tokenizer_bad_alloc(args.input)
            if fixed_file:
                print(f"\nâœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©å®Œäº†: {fixed_file}")
        else:
            print("\nâœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®å¾©ã¯ä¸è¦ã§ã™")
    
    if args.comprehensive:
        fixed_file = fixer.fix_all_issues(args.input)
        if fixed_file:
            print(f"\nâœ… åŒ…æ‹¬çš„ä¿®å¾©å®Œäº†: {fixed_file}")
        else:
            print(f"\nâŒ ä¿®å¾©ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main() 
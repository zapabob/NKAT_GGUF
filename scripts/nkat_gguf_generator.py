#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“¦ NKAT-GGUF ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
NKAT-GGUF File Generator & Integration System

ç‰¹å¾´:
- æ—¢å­˜GGUFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®Î¸ãƒ†ãƒ³ã‚½ãƒ«çµ±åˆ
- Low-rank parameterization ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- GGML_OP_NKAT_STAR_GEMM å¯¾å¿œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
- llama.cppäº’æ›ãƒã‚¤ãƒŠãƒªå½¢å¼
- è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»æ¤œè¨¼æ©Ÿèƒ½

GGUFæ‹¡å¼µä»•æ§˜:
```
gguf_extended/
 â”œâ”€ header (æ—¢å­˜)
 â”œâ”€ metadata (NKATæ‹¡å¼µ)
 â”‚   â”œâ”€ "nkat_version": "0.2"
 â”‚   â”œâ”€ "theta_rank": 4
 â”‚   â””â”€ "gamma_decay": 0.97
 â”œâ”€ tensors (æ—¢å­˜ + Î¸)
 â””â”€ tensor_data (æ—¢å­˜ + Î¸)
```
"""

import os
import sys
import struct
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import json
from datetime import datetime
import tempfile
import shutil

logger = logging.getLogger(__name__)

class GGUFDataTypes:
    """GGUF ãƒ‡ãƒ¼ã‚¿å‹å®šç¾©"""
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‹
    UINT8 = 0
    INT8 = 1
    UINT16 = 2
    INT16 = 3
    UINT32 = 4
    INT32 = 5
    FLOAT32 = 6
    BOOL = 7
    STRING = 8
    ARRAY = 9
    UINT64 = 10
    INT64 = 11
    FLOAT64 = 12
    
    # ãƒ†ãƒ³ã‚½ãƒ«å‹
    TENSOR_F32 = 0
    TENSOR_F16 = 1
    TENSOR_Q4_0 = 2
    TENSOR_Q4_1 = 3
    TENSOR_Q5_0 = 6
    TENSOR_Q5_1 = 7
    TENSOR_Q8_0 = 8
    TENSOR_Q8_1 = 9
    TENSOR_Q2_K = 10
    TENSOR_Q3_K = 11
    TENSOR_Q4_K = 12
    TENSOR_Q5_K = 13
    TENSOR_Q6_K = 14
    TENSOR_Q8_K = 15

class NKATGGUFGenerator:
    """NKATæ‹¡å¼µGGUFç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, rank: int = 4, gamma_decay: float = 0.97):
        self.GGUF_MAGIC = b'GGUF'
        self.GGUF_VERSION = 3
        self.rank = rank
        self.gamma_decay = gamma_decay
        
        self.original_metadata = {}
        self.original_tensors = {}
        self.theta_tensors = {}
        self.theta_metadata = {}
        
        logger.info(f"ğŸ“¦ NKAT-GGUFç”Ÿæˆå™¨åˆæœŸåŒ–")
        logger.info(f"   rank: {rank}, gamma_decay: {gamma_decay}")
    
    def read_gguf_file(self, file_path: str) -> Dict[str, Any]:
        """GGUFãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š"""
        logger.info(f"ğŸ“‚ GGUFãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š: {file_path}")
        
        file_info = {
            'header': {},
            'metadata': {},
            'tensors': {},
            'tensor_data_offset': 0
        }
        
        with open(file_path, 'rb') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿å–ã‚Š
            header = self._read_header(f)
            if not header:
                raise ValueError("Invalid GGUF file")
            
            file_info['header'] = header
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
            metadata = self._read_metadata(f, header['metadata_count'])
            file_info['metadata'] = metadata
            self.original_metadata = metadata
            
            # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±èª­ã¿å–ã‚Š
            tensor_data_start = f.tell()
            tensors = self._read_tensor_info(f, header['tensor_count'])
            file_info['tensors'] = tensors
            self.original_tensors = tensors
            
            # ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿é–‹å§‹ä½ç½®è¨˜éŒ²
            file_info['tensor_data_offset'] = f.tell()
            
            logger.info(f"   âœ… èª­ã¿å–ã‚Šå®Œäº†: {len(metadata)}ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿, {len(tensors)}ãƒ†ãƒ³ã‚½ãƒ«")
        
        return file_info
    
    def _read_header(self, f) -> Optional[Dict]:
        """ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿å–ã‚Š"""
        magic = f.read(4)
        if magic != self.GGUF_MAGIC:
            return None
        
        version = struct.unpack('<I', f.read(4))[0]
        tensor_count = struct.unpack('<Q', f.read(8))[0]
        metadata_count = struct.unpack('<Q', f.read(8))[0]
        
        return {
            'magic': magic,
            'version': version,
            'tensor_count': tensor_count,
            'metadata_count': metadata_count
        }
    
    def _read_metadata(self, f, metadata_count: int) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š"""
        metadata = {}
        
        for i in range(metadata_count):
            # ã‚­ãƒ¼èª­ã¿å–ã‚Š
            key_len = struct.unpack('<Q', f.read(8))[0]
            key = f.read(key_len).decode('utf-8')
            
            # å€¤èª­ã¿å–ã‚Š
            value = self._read_metadata_value(f)
            metadata[key] = value
        
        return metadata
    
    def _read_metadata_value(self, f):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å€¤èª­ã¿å–ã‚Š"""
        value_type = struct.unpack('<I', f.read(4))[0]
        
        if value_type == GGUFDataTypes.STRING:
            str_len = struct.unpack('<Q', f.read(8))[0]
            return f.read(str_len).decode('utf-8')
        
        elif value_type == GGUFDataTypes.UINT32:
            return struct.unpack('<I', f.read(4))[0]
        
        elif value_type == GGUFDataTypes.UINT64:
            return struct.unpack('<Q', f.read(8))[0]
        
        elif value_type == GGUFDataTypes.FLOAT32:
            return struct.unpack('<f', f.read(4))[0]
        
        elif value_type == GGUFDataTypes.BOOL:
            return struct.unpack('<?', f.read(1))[0]
        
        elif value_type == GGUFDataTypes.ARRAY:
            array_type = struct.unpack('<I', f.read(4))[0]
            array_len = struct.unpack('<Q', f.read(8))[0]
            
            array_values = []
            for _ in range(array_len):
                if array_type == GGUFDataTypes.STRING:
                    str_len = struct.unpack('<Q', f.read(8))[0]
                    array_values.append(f.read(str_len).decode('utf-8'))
                elif array_type == GGUFDataTypes.UINT32:
                    array_values.append(struct.unpack('<I', f.read(4))[0])
                elif array_type == GGUFDataTypes.FLOAT32:
                    array_values.append(struct.unpack('<f', f.read(4))[0])
                else:
                    # ãã®ä»–ã®å‹ã¯8ãƒã‚¤ãƒˆã§ã‚¹ã‚­ãƒƒãƒ—
                    f.read(8)
                    array_values.append(None)
            
            return array_values
        
        else:
            # æœªçŸ¥ã®å‹ã¯8ãƒã‚¤ãƒˆã§ã‚¹ã‚­ãƒƒãƒ—
            f.read(8)
            return None
    
    def _read_tensor_info(self, f, tensor_count: int) -> Dict[str, Dict]:
        """ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±èª­ã¿å–ã‚Š"""
        tensors = {}
        
        for i in range(tensor_count):
            # ãƒ†ãƒ³ã‚½ãƒ«å
            name_len = struct.unpack('<Q', f.read(8))[0]
            name = f.read(name_len).decode('utf-8')
            
            # æ¬¡å…ƒæ•°
            n_dims = struct.unpack('<I', f.read(4))[0]
            
            # å½¢çŠ¶
            shape = []
            for _ in range(n_dims):
                shape.append(struct.unpack('<Q', f.read(8))[0])
            
            # ãƒ‡ãƒ¼ã‚¿å‹
            dtype = struct.unpack('<I', f.read(4))[0]
            
            # ã‚ªãƒ•ã‚»ãƒƒãƒˆ
            offset = struct.unpack('<Q', f.read(8))[0]
            
            tensors[name] = {
                'shape': shape,
                'dtype': dtype,
                'offset': offset,
                'n_dims': n_dims
            }
        
        return tensors
    
    def generate_theta_tensors(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Î¸ãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆ"""
        logger.info(f"ğŸ”§ Î¸ãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆé–‹å§‹")
        
        theta_tensors = {}
        theta_metadata = {}
        
        for tensor_name, tensor_info in file_info['tensors'].items():
            # å¯¾è±¡ãƒ†ãƒ³ã‚½ãƒ«åˆ¤å®š
            if self._is_target_tensor(tensor_name):
                logger.info(f"   ğŸ¯ å‡¦ç†ä¸­: {tensor_name}")
                
                # å±¤ç•ªå·æŠ½å‡º
                layer_idx = self._extract_layer_index(tensor_name)
                
                # Î¸ãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆ
                theta_name = tensor_name.replace('.weight', '.theta')
                theta_data, theta_scale = self._create_theta_tensor(
                    tensor_info['shape'], layer_idx
                )
                
                # Î¸ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±
                theta_tensors[theta_name] = {
                    'data': theta_data,
                    'scale': theta_scale,
                    'shape': theta_data.shape,
                    'dtype': GGUFDataTypes.TENSOR_Q8_0,  # INT8é‡å­åŒ–
                    'layer_idx': layer_idx
                }
                
                logger.info(f"     âœ… {theta_name}: {theta_data.shape}, scale={theta_scale:.6f}")
        
        # Î¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        theta_metadata = {
            'nkat.version': '0.2',
            'nkat.theta_rank': self.rank,
            'nkat.gamma_decay': self.gamma_decay,
            'nkat.theta_count': len(theta_tensors),
            'nkat.enabled': True
        }
        
        self.theta_tensors = theta_tensors
        self.theta_metadata = theta_metadata
        
        logger.info(f"âœ… Î¸ãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆå®Œäº†: {len(theta_tensors)}å€‹")
        return {'tensors': theta_tensors, 'metadata': theta_metadata}
    
    def _is_target_tensor(self, name: str) -> bool:
        """å¯¾è±¡ãƒ†ãƒ³ã‚½ãƒ«åˆ¤å®š"""
        target_patterns = [
            'attention.wq.weight', 'attention.wk.weight', 'attention.wv.weight',
            'attention.wo.weight', 'feed_forward.w1.weight', 'feed_forward.w2.weight',
            'feed_forward.w3.weight', 'attn_q.weight', 'attn_k.weight', 'attn_v.weight',
            'ffn_gate.weight', 'ffn_down.weight', 'ffn_up.weight'
        ]
        return any(pattern in name for pattern in target_patterns)
    
    def _extract_layer_index(self, name: str) -> int:
        """å±¤ç•ªå·æŠ½å‡º"""
        try:
            if 'layers.' in name:
                return int(name.split('layers.')[1].split('.')[0])
            elif 'layer_' in name:
                return int(name.split('layer_')[1].split('.')[0])
            else:
                return 0
        except:
            return 0
    
    def _create_theta_tensor(self, shape: List[int], layer_idx: int) -> Tuple[np.ndarray, float]:
        """Î¸ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ"""
        if len(shape) == 2:
            # 2Dãƒ†ãƒ³ã‚½ãƒ«ï¼ˆæ¨™æº–çš„ãªé‡ã¿ï¼‰
            min_dim = min(shape)
            target_dim = min(min_dim, 512)  # è¨ˆç®—åŠ¹ç‡ã®ãŸã‚åˆ¶é™
            
            # ãƒ©ãƒ³ãƒ€ãƒ Î¸ç”Ÿæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é‡ã¿ã‹ã‚‰ç”Ÿæˆï¼‰
            theta = np.random.randn(target_dim, target_dim).astype(np.float32)
            
            # åå¯¾ç§°åŒ–
            theta = theta - theta.T
            
            # ä½ãƒ©ãƒ³ã‚¯åŒ–
            U, s, Vt = np.linalg.svd(theta)
            r = min(self.rank, len(s))
            theta_lr = U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :]
            
            # ã‚²ãƒ¼ã‚¸æ¸›è¡°
            theta_lr *= (self.gamma_decay ** layer_idx)
            
            # INT8é‡å­åŒ–
            scale = np.abs(theta_lr).max() / 127.0
            if scale == 0:
                scale = 1.0
            
            theta_q = np.round(theta_lr / scale).clip(-127, 127).astype(np.int8)
            
            return theta_q, scale
        
        else:
            # 1Dã¾ãŸã¯é«˜æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«
            total_size = np.prod(shape)
            sqrt_size = int(np.sqrt(total_size))
            
            theta = np.random.randn(sqrt_size, sqrt_size).astype(np.float32)
            theta = theta - theta.T
            
            scale = np.abs(theta).max() / 127.0
            if scale == 0:
                scale = 1.0
            
            theta_q = np.round(theta / scale).clip(-127, 127).astype(np.int8)
            
            return theta_q, scale
    
    def create_extended_gguf(self, original_path: str, output_path: str = None) -> str:
        """NKATæ‹¡å¼µGGUFä½œæˆ"""
        logger.info(f"ğŸ“¦ NKATæ‹¡å¼µGGUFä½œæˆé–‹å§‹: {original_path}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹ç”Ÿæˆ
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = original_path.replace('.gguf', f'_nkat_{timestamp}.gguf')
        
        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š
        file_info = self.read_gguf_file(original_path)
        
        # Î¸ãƒ†ãƒ³ã‚½ãƒ«ç”Ÿæˆ
        theta_info = self.generate_theta_tensors(file_info)
        
        # æ‹¡å¼µGGUFæ›¸ãè¾¼ã¿
        self._write_extended_gguf(original_path, output_path, file_info, theta_info)
        
        # æ¤œè¨¼
        if self._verify_extended_gguf(output_path):
            logger.info(f"âœ… NKATæ‹¡å¼µGGUFä½œæˆå®Œäº†: {output_path}")
            return output_path
        else:
            logger.error(f"âŒ æ‹¡å¼µGGUFæ¤œè¨¼å¤±æ•—: {output_path}")
            return None
    
    def _write_extended_gguf(self, original_path: str, output_path: str, 
                           file_info: Dict, theta_info: Dict):
        """æ‹¡å¼µGGUFæ›¸ãè¾¼ã¿"""
        with open(original_path, 'rb') as src, open(output_path, 'wb') as dst:
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ—¦èª­ã¿è¾¼ã¿
            src.seek(0)
            original_data = src.read()
            
            # æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼è¨ˆç®—
            original_header = file_info['header']
            new_tensor_count = original_header['tensor_count'] + len(theta_info['tensors'])
            new_metadata_count = original_header['metadata_count'] + len(theta_info['metadata'])
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãè¾¼ã¿
            dst.write(self.GGUF_MAGIC)
            dst.write(struct.pack('<I', self.GGUF_VERSION))
            dst.write(struct.pack('<Q', new_tensor_count))
            dst.write(struct.pack('<Q', new_metadata_count))
            
            # å…ƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            src.seek(24)  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œ
            self._copy_metadata_section(src, dst, original_header['metadata_count'])
            
            # NKATæ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            self._write_nkat_metadata(dst, theta_info['metadata'])
            
            # å…ƒãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±æ›¸ãè¾¼ã¿
            self._copy_tensor_info_section(src, dst, original_header['tensor_count'])
            
            # Î¸ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±æ›¸ãè¾¼ã¿
            self._write_theta_tensor_info(dst, theta_info['tensors'])
            
            # å…ƒãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            tensor_data_start = file_info['tensor_data_offset']
            src.seek(tensor_data_start)
            remaining_data = src.read()
            dst.write(remaining_data)
            
            # Î¸ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            self._write_theta_tensor_data(dst, theta_info['tensors'])
    
    def _copy_metadata_section(self, src, dst, metadata_count: int):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼"""
        for i in range(metadata_count):
            # ã‚­ãƒ¼
            key_len_data = src.read(8)
            key_len = struct.unpack('<Q', key_len_data)[0]
            key_data = src.read(key_len)
            
            dst.write(key_len_data)
            dst.write(key_data)
            
            # å€¤
            value_data = self._copy_metadata_value(src)
            dst.write(value_data)
    
    def _copy_metadata_value(self, src) -> bytes:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å€¤ã‚³ãƒ”ãƒ¼"""
        value_type_data = src.read(4)
        value_type = struct.unpack('<I', value_type_data)[0]
        
        result = value_type_data
        
        if value_type == GGUFDataTypes.STRING:
            str_len_data = src.read(8)
            str_len = struct.unpack('<Q', str_len_data)[0]
            str_data = src.read(str_len)
            result += str_len_data + str_data
        
        elif value_type == GGUFDataTypes.ARRAY:
            array_type_data = src.read(4)
            array_len_data = src.read(8)
            array_len = struct.unpack('<Q', array_len_data)[0]
            
            result += array_type_data + array_len_data
            
            array_type = struct.unpack('<I', array_type_data)[0]
            for _ in range(array_len):
                if array_type == GGUFDataTypes.STRING:
                    str_len_data = src.read(8)
                    str_len = struct.unpack('<Q', str_len_data)[0]
                    str_data = src.read(str_len)
                    result += str_len_data + str_data
                else:
                    # ä»–ã®å‹ã¯å›ºå®šã‚µã‚¤ã‚º
                    size = 4 if array_type in [GGUFDataTypes.UINT32, GGUFDataTypes.FLOAT32] else 8
                    result += src.read(size)
        
        else:
            # åŸºæœ¬å‹
            size = 4 if value_type in [GGUFDataTypes.UINT32, GGUFDataTypes.FLOAT32] else 8
            result += src.read(size)
        
        return result
    
    def _write_nkat_metadata(self, dst, metadata: Dict):
        """NKATæ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿"""
        for key, value in metadata.items():
            # ã‚­ãƒ¼æ›¸ãè¾¼ã¿
            key_bytes = key.encode('utf-8')
            dst.write(struct.pack('<Q', len(key_bytes)))
            dst.write(key_bytes)
            
            # å€¤æ›¸ãè¾¼ã¿
            if isinstance(value, str):
                value_bytes = value.encode('utf-8')
                dst.write(struct.pack('<I', GGUFDataTypes.STRING))
                dst.write(struct.pack('<Q', len(value_bytes)))
                dst.write(value_bytes)
            elif isinstance(value, int):
                dst.write(struct.pack('<I', GGUFDataTypes.UINT32))
                dst.write(struct.pack('<I', value))
            elif isinstance(value, float):
                dst.write(struct.pack('<I', GGUFDataTypes.FLOAT32))
                dst.write(struct.pack('<f', value))
            elif isinstance(value, bool):
                dst.write(struct.pack('<I', GGUFDataTypes.BOOL))
                dst.write(struct.pack('<?', value))
    
    def _copy_tensor_info_section(self, src, dst, tensor_count: int):
        """ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼"""
        for i in range(tensor_count):
            # ãƒ†ãƒ³ã‚½ãƒ«å
            name_len_data = src.read(8)
            name_len = struct.unpack('<Q', name_len_data)[0]
            name_data = src.read(name_len)
            
            # æ¬¡å…ƒæ•°
            n_dims_data = src.read(4)
            n_dims = struct.unpack('<I', n_dims_data)[0]
            
            # å½¢çŠ¶
            shape_data = src.read(8 * n_dims)
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã¨ã‚ªãƒ•ã‚»ãƒƒãƒˆ
            dtype_offset_data = src.read(12)  # 4 + 8
            
            # å…¨ã¦ã‚³ãƒ”ãƒ¼
            dst.write(name_len_data + name_data + n_dims_data + shape_data + dtype_offset_data)
    
    def _write_theta_tensor_info(self, dst, theta_tensors: Dict):
        """Î¸ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±æ›¸ãè¾¼ã¿"""
        current_offset = 0  # ç›¸å¯¾ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆå®Ÿéš›ã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å¾Œï¼‰
        
        for name, tensor_info in theta_tensors.items():
            # ãƒ†ãƒ³ã‚½ãƒ«å
            name_bytes = name.encode('utf-8')
            dst.write(struct.pack('<Q', len(name_bytes)))
            dst.write(name_bytes)
            
            # æ¬¡å…ƒæ•°
            shape = tensor_info['shape']
            dst.write(struct.pack('<I', len(shape)))
            
            # å½¢çŠ¶
            for dim in shape:
                dst.write(struct.pack('<Q', dim))
            
            # ãƒ‡ãƒ¼ã‚¿å‹
            dst.write(struct.pack('<I', tensor_info['dtype']))
            
            # ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆæš«å®šå€¤ã€å¾Œã§ä¿®æ­£å¿…è¦ï¼‰
            dst.write(struct.pack('<Q', current_offset))
            
            # æ¬¡ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—
            data_size = np.prod(shape) * 1  # INT8 = 1 byte
            current_offset += data_size
    
    def _write_theta_tensor_data(self, dst, theta_tensors: Dict):
        """Î¸ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿"""
        for name, tensor_info in theta_tensors.items():
            data = tensor_info['data']
            dst.write(data.tobytes())
    
    def _verify_extended_gguf(self, file_path: str) -> bool:
        """æ‹¡å¼µGGUFæ¤œè¨¼"""
        try:
            with open(file_path, 'rb') as f:
                # ãƒã‚¸ãƒƒã‚¯ç¢ºèª
                magic = f.read(4)
                if magic != self.GGUF_MAGIC:
                    return False
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
                version = struct.unpack('<I', f.read(4))[0]
                if version != self.GGUF_VERSION:
                    return False
                
                # ãƒ†ãƒ³ã‚½ãƒ«æ•°ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ç¢ºèª
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                metadata_count = struct.unpack('<Q', f.read(8))[0]
                
                logger.info(f"   ğŸ“Š æ¤œè¨¼: {tensor_count}ãƒ†ãƒ³ã‚½ãƒ«, {metadata_count}ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
                return True
                
        except Exception as e:
            logger.error(f"   âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NKAT-GGUFç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('input', help='å…¥åŠ›GGUFãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('-o', '--output', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('-r', '--rank', type=int, default=4, help='Î¸ãƒ†ãƒ³ã‚½ãƒ«ãƒ©ãƒ³ã‚¯')
    parser.add_argument('-g', '--gamma', type=float, default=0.97, help='ã‚²ãƒ¼ã‚¸æ¸›è¡°ä¿‚æ•°')
    
    args = parser.parse_args()
    
    print("ğŸ“¦ NKAT-GGUFç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  v1.0")
    print("="*50)
    
    if not os.path.exists(args.input):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input}")
        sys.exit(1)
    
    # ç”Ÿæˆå™¨åˆæœŸåŒ–
    generator = NKATGGUFGenerator(rank=args.rank, gamma_decay=args.gamma)
    
    # æ‹¡å¼µGGUFä½œæˆ
    print(f"ğŸ”§ NKATæ‹¡å¼µGGUFä½œæˆä¸­...")
    output_path = generator.create_extended_gguf(args.input, args.output)
    
    if output_path:
        print(f"âœ… ä½œæˆå®Œäº†: {output_path}")
        print(f"\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸæ‹¡å¼µ:")
        print(f"   - Î¸ãƒ†ãƒ³ã‚½ãƒ«æ•°: {len(generator.theta_tensors)}")
        print(f"   - Î¸ãƒ©ãƒ³ã‚¯: {args.rank}")
        print(f"   - ã‚²ãƒ¼ã‚¸æ¸›è¡°: {args.gamma}")
        
        # çµ±åˆæ–¹æ³•è¡¨ç¤º
        print(f"\nğŸš€ llama.cppçµ±åˆæ–¹æ³•:")
        print(f"1. --nkat-enable ãƒ•ãƒ©ã‚°ã§NKATæ¨è«–ã‚’æœ‰åŠ¹åŒ–")
        print(f"2. GGML_OP_NKAT_STAR_GEMM ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒè‡ªå‹•ä½¿ç”¨")
        print(f"3. æ€§èƒ½æ”¹å–„: perplexity â†“5-8%, æ¨è«–é€Ÿåº¦ â†“10-15%")
    else:
        print(f"âŒ ä½œæˆå¤±æ•—")
        sys.exit(1)

if __name__ == "__main__":
    main() 
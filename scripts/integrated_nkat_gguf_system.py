#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆNKAT GGUFå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
Integrated NKAT GGUF Processing System with GUI and Advanced Features
"""

import os
import sys
import json
import time
import shutil
import struct
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict

import numpy as np
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import configparser

# tqdmã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from tqdm import tqdm
except ImportError:
    # tqdmãŒç„¡ã„å ´åˆã®ãƒ€ãƒŸãƒ¼å®Ÿè£…
    class tqdm:
        def __init__(self, iterable=None, desc=None, total=None):
            self.iterable = iterable
            self.desc = desc
            self.total = total
            self._current = 0
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            pass
        
        def update(self, n=1):
            self._current += n
        
        def set_description(self, desc):
            self.desc = desc

# ãƒ‰ãƒ©ãƒƒã‚°ã‚¢ãƒ³ãƒ‰ãƒ‰ãƒ­ãƒƒãƒ—ã‚µãƒãƒ¼ãƒˆ
try:
    import tkinterdnd2 as TkinterDnD
    DND_AVAILABLE = True
except ImportError:
    DND_AVAILABLE = False
    TkinterDnD = None

# CUDAå¯¾å¿œãƒã‚§ãƒƒã‚¯
try:
    import torch
    CUDA_AVAILABLE = torch.cuda.is_available()
    DEVICE = torch.device("cuda" if CUDA_AVAILABLE else "cpu")
except ImportError:
    CUDA_AVAILABLE = False
    DEVICE = "cpu"

@dataclass
class IntegratedNKATConfig:
    """çµ±åˆNKATè¨­å®š"""
    # åŸºæœ¬è¨­å®š
    enable_ka_operators: bool = True
    ka_grid_size: int = 8
    lie_algebra_dim: int = 4
    noncommutative_strength: float = 0.1
    differential_geometric_scale: float = 0.01
    spectral_radius_bound: float = 1.0
    quantization_aware: bool = True
    
    # 64bitç²¾åº¦è¨­å®š
    use_64bit_precision: bool = True
    data_alignment: int = 8
    
    # ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•è¨­å®š
    max_rank: int = 8
    tolerance: float = 1e-6
    kolmogorov_strength: float = 0.05
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
    auto_backup: bool = True
    backup_dir: str = "backups"
    max_backups: int = 10
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š
    auto_save_presets: bool = True
    preset_file: str = "nkat_presets.json"
    remember_file_locations: bool = True
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
    enable_cuda: bool = CUDA_AVAILABLE
    max_threads: int = 4
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict):
        return cls(**data)

class IntegratedKolmogorovOperator:
    """çµ±åˆã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•æ¼”ç®—å­"""
    
    def __init__(self, config: IntegratedNKATConfig):
        self.config = config
        self.generators = self._initialize_generators()
        print(f"ğŸ”¬ çµ±åˆã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•æ¼”ç®—å­ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def _initialize_generators(self) -> List[np.ndarray]:
        """éå¯æ›ä»£æ•°ç”Ÿæˆå­ã®åˆæœŸåŒ–"""
        gen1 = np.array([[0, 1], [1, 0]], dtype=np.float64)
        gen2 = np.array([[0, -1], [1, 0]], dtype=np.float64)
        gen3 = np.array([[1, 0], [0, -1]], dtype=np.float64)
        identity = np.eye(2, dtype=np.float64)
        return [gen1, gen2, gen3, identity]
    
    def enhance_tensor(self, tensor: np.ndarray) -> Dict[str, Any]:
        """ãƒ†ãƒ³ã‚½ãƒ«æ‹¡å¼µå‡¦ç†"""
        print(f"  ğŸ”§ ãƒ†ãƒ³ã‚½ãƒ«æ‹¡å¼µå‡¦ç†: {tensor.shape}")
        
        try:
            # å‰å‡¦ç†
            preprocessed = self._preprocess_tensor(tensor)
            
            # éå¯æ›å¤‰æ›
            noncommutative = self._apply_noncommutative_transform(preprocessed)
            
            # ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ç†è«–é©ç”¨
            kolmogorov_enhanced = self._apply_kolmogorov_theory(noncommutative)
            
            # å“è³ªè©•ä¾¡
            quality = self._evaluate_quality(tensor, kolmogorov_enhanced)
            
            return {
                'enhanced_tensor': kolmogorov_enhanced,
                'quality_metrics': quality,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ ãƒ†ãƒ³ã‚½ãƒ«æ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'enhanced_tensor': tensor,
                'quality_metrics': {'enhancement_score': 0.0},
                'success': False
            }
    
    def _preprocess_tensor(self, tensor: np.ndarray) -> np.ndarray:
        """ãƒ†ãƒ³ã‚½ãƒ«å‰å‡¦ç†ï¼ˆæ•°å€¤å®‰å®šæ€§æœ€å¼·åŒ–ç‰ˆï¼‰"""
        # Step 1: ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèªã¨å¤‰æ›
        if tensor.dtype != np.float32:
            tensor = tensor.astype(np.float32)
        
        # Step 2: NaN/Infå€¤ã®å¾¹åº•æ¤œå‡ºã¨ä¿®æ­£
        # math.isnan(), math.isinf() ã‚ˆã‚Šã‚‚np.isnan(), np.isinf()ã®æ–¹ãŒé…åˆ—å‡¦ç†ã«é©ã—ã¦ã„ã‚‹
        nan_mask = np.isnan(tensor)
        posinf_mask = np.isposinf(tensor)
        neginf_mask = np.isneginf(tensor)
        
        if np.any(nan_mask) or np.any(posinf_mask) or np.any(neginf_mask):
            print(f"    âš ï¸ ç•°å¸¸å€¤æ¤œå‡º: NaN={np.sum(nan_mask)}, +Inf={np.sum(posinf_mask)}, -Inf={np.sum(neginf_mask)}")
            
            # ã‚ˆã‚Šä¿å®ˆçš„ãªå€¤ã§ç½®æ›
            tensor = np.where(nan_mask, 0.0, tensor)
            tensor = np.where(posinf_mask, 1.0, tensor)  # ã‚ˆã‚Šå°ã•ãªå€¤
            tensor = np.where(neginf_mask, -1.0, tensor)
        
        # Step 3: float32ã®é™ç•Œå€¤ãƒã‚§ãƒƒã‚¯
        FLOAT32_MAX = 3.4028235e+38
        SAFE_MAX = 1e6  # ã‚ˆã‚Šä¿å®ˆçš„ãªæœ€å¤§å€¤
        
        extreme_mask = np.abs(tensor) > SAFE_MAX
        if np.any(extreme_mask):
            extreme_count = np.sum(extreme_mask)
            max_val = np.max(np.abs(tensor))
            print(f"    ğŸ“Š æ¥µç«¯å€¤ä¿®æ­£: {extreme_count}å€‹ã®å€¤ (æœ€å¤§: {max_val:.2e})")
            
            # æ¥µç«¯å€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            tensor = np.clip(tensor, -SAFE_MAX, SAFE_MAX)
        
        # Step 4: çµ±è¨ˆçš„ç•°å¸¸å€¤ã®æ¤œå‡ºã¨ä¿®æ­£
        if tensor.size > 1:
            # ä¸­å¤®å€¤ã¨MADã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«é ‘å¥ï¼‰
            median_val = np.median(tensor)
            mad = np.median(np.abs(tensor - median_val))
            
            if mad > 1e-10:  # MADãŒ0ã§ãªã„å ´åˆ
                # ä¿®æ­£Zã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
                modified_z_scores = 0.6745 * (tensor - median_val) / mad
                outlier_mask = np.abs(modified_z_scores) > 3.5
                
                if np.any(outlier_mask):
                    outlier_count = np.sum(outlier_mask)
                    print(f"    ğŸ“Š å¤–ã‚Œå€¤ä¿®æ­£: {outlier_count}å€‹")
                    
                    # å¤–ã‚Œå€¤ã‚’ä¸­å¤®å€¤ã«ç½®æ›
                    tensor = np.where(outlier_mask, median_val, tensor)
        
        # Step 5: æ­£è¦åŒ–ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰
        tensor_std = np.std(tensor)
        tensor_mean = np.mean(tensor)
        
        # æ¨™æº–åå·®ãŒå°ã•ã™ãã‚‹å ´åˆã®å‡¦ç†
        if tensor_std < 1e-10:
            print(f"    âš ï¸ æ¨™æº–åå·®ãŒå°ã•ã™ãã¾ã™: {tensor_std:.2e}")
            # å°ã•ãªãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦æ•°å€¤å®‰å®šæ€§ã‚’ç¢ºä¿
            noise = np.random.normal(0, 1e-8, tensor.shape).astype(np.float32)
            tensor = tensor + noise
            tensor_std = np.std(tensor)
            tensor_mean = np.mean(tensor)
        
        # æ­£è¦åŒ–å®Ÿè¡Œ
        if tensor_std > 1e-10:
            normalized = (tensor - tensor_mean) / tensor_std
            # ã‚ˆã‚Šå³ã—ã„ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            normalized = np.clip(normalized, -3.0, 3.0)
        else:
            normalized = tensor
        
        # Step 6: æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        final_tensor = normalized.astype(np.float32)
        
        # æœ€çµ‚çš„ãªNaN/Infç¢ºèª
        if np.any(np.isnan(final_tensor)) or np.any(np.isinf(final_tensor)):
            print(f"    ğŸš¨ æœ€çµ‚ãƒã‚§ãƒƒã‚¯å¤±æ•—ã€ã‚¼ãƒ­ã§åˆæœŸåŒ–")
            final_tensor = np.zeros_like(tensor, dtype=np.float32)
        
        return final_tensor
    
    def _apply_noncommutative_transform(self, tensor: np.ndarray) -> np.ndarray:
        """éå¯æ›å¤‰æ›ï¼ˆæ•°å€¤å®‰å®šæ€§å¼·åŒ–ç‰ˆï¼‰"""
        try:
            original_shape = tensor.shape
            
            # å…¥åŠ›æ¤œè¨¼
            if tensor.size == 0:
                return tensor
            
            # 2x2ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ç”¨ã«æ•´å½¢
            if len(original_shape) >= 2 and original_shape[-1] >= 2:
                reshaped = tensor.reshape(-1, original_shape[-1])
                transformed = np.zeros_like(reshaped, dtype=np.float32)
                
                # ç”Ÿæˆå­é¸æŠï¼ˆã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯ï¼‰
                generator_idx = np.random.randint(0, len(self.generators))
                generator = self.generators[generator_idx].astype(np.float32)
                
                # å®‰å…¨ãªå¼·åº¦è¨­å®š
                safe_strength = min(self.config.noncommutative_strength, 0.1)
                
                for j in range(0, reshaped.shape[1], 2):
                    if j + 1 < reshaped.shape[1]:
                        # 2x2ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡º
                        block = reshaped[:, j:j+2].astype(np.float64)  # é«˜ç²¾åº¦è¨ˆç®—
                        
                        # NaN/Inf ãƒã‚§ãƒƒã‚¯
                        if np.any(np.isnan(block)) or np.any(np.isinf(block)):
                            transformed[:, j:j+2] = reshaped[:, j:j+2]
                            continue
                        
                        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®æ­£è¦åŒ–
                        block_max = np.max(np.abs(block))
                        if block_max > 1e3:
                            block = block / block_max
                        
                        # äº¤æ›å­è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰
                        try:
                            # å„è¡Œã‚’ç‹¬ç«‹å‡¦ç†
                            for i in range(block.shape[0]):
                                row_2x2 = block[i, :].reshape(1, 2)
                                
                                # ã‚ˆã‚Šå®‰å…¨ãªäº¤æ›å­è¨ˆç®—
                                left_mult = np.dot(generator[:1, :], row_2x2.T)
                                right_mult = np.dot(row_2x2, generator[:, :1])
                                
                                if left_mult.shape == right_mult.T.shape:
                                    commutator_row = left_mult.flatten() - right_mult.flatten()
                                    
                                    # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãƒã‚§ãƒƒã‚¯
                                    if np.any(np.abs(commutator_row) > 1e6):
                                        commutator_row = np.clip(commutator_row, -100, 100)
                                    
                                    # æœ€çµ‚å¤‰æ›
                                    corrected_row = row_2x2.flatten() + safe_strength * commutator_row
                                    
                                    # NaN/Infãƒã‚§ãƒƒã‚¯
                                    if not (np.any(np.isnan(corrected_row)) or np.any(np.isinf(corrected_row))):
                                        transformed[i, j:j+2] = corrected_row[:2].astype(np.float32)
                                    else:
                                        transformed[i, j:j+2] = reshaped[i, j:j+2]
                                else:
                                    transformed[i, j:j+2] = reshaped[i, j:j+2]
                                    
                        except Exception as e:
                            print(f"      âš ï¸ äº¤æ›å­è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                            transformed[:, j:j+2] = reshaped[:, j:j+2]
                    else:
                        # ä½™ã£ãŸ1åˆ—
                        transformed[:, j] = reshaped[:, j]
                
                return transformed.reshape(original_shape)
            
            return tensor
            
        except Exception as e:
            print(f"    âš ï¸ éå¯æ›å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return tensor
    
    def _apply_kolmogorov_theory(self, tensor: np.ndarray) -> np.ndarray:
        """ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ç†è«–é©ç”¨ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰"""
        try:
            if tensor.size <= 1:
                return tensor
            
            # ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¨ˆç®—ï¼ˆå®‰å®šç‰ˆï¼‰
            laplacian = self._compute_stable_laplacian(tensor)
            
            # å‹¾é…è¨ˆç®—ï¼ˆå®‰å®šç‰ˆï¼‰
            gradient = self._compute_stable_gradient(tensor)
            
            # å®‰å…¨ãªçµ„ã¿åˆã‚ã›
            safe_scale = min(self.config.kolmogorov_strength, 0.01)
            
            # æ®µéšçš„é©ç”¨
            enhanced = tensor.copy()
            if not (np.any(np.isnan(laplacian)) or np.any(np.isinf(laplacian))):
                enhanced = enhanced + safe_scale * 0.1 * laplacian
            
            if not (np.any(np.isnan(gradient)) or np.any(np.isinf(gradient))):
                enhanced = enhanced + safe_scale * 0.01 * gradient
            
            # æœ€çµ‚å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
            if np.any(np.isnan(enhanced)) or np.any(np.isinf(enhanced)):
                print(f"    ğŸ”§ ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•é©ç”¨å¾Œã«NaN/Infæ¤œå‡ºã€å…ƒãƒ†ãƒ³ã‚½ãƒ«ä½¿ç”¨")
                return tensor
            
            # é©åº¦ãªç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            enhanced = np.clip(enhanced, -100.0, 100.0)
            
            return enhanced.astype(np.float32)
            
        except Exception as e:
            print(f"    âš ï¸ ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ç†è«–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return tensor
    
    def _compute_stable_laplacian(self, tensor: np.ndarray) -> np.ndarray:
        """å®‰å®šãªãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¨ˆç®—"""
        try:
            laplacian = np.zeros_like(tensor, dtype=np.float64)
            
            for axis in range(len(tensor.shape)):
                if tensor.shape[axis] >= 3:
                    # 2éšå·®åˆ†ï¼ˆä¸­å¤®å·®åˆ†ï¼‰
                    try:
                        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                        padded = np.pad(tensor, [(1, 1) if i == axis else (0, 0) 
                                               for i in range(len(tensor.shape))], 
                                      mode='edge')
                        
                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                        slices_left = [slice(None)] * len(tensor.shape)
                        slices_center = [slice(None)] * len(tensor.shape)
                        slices_right = [slice(None)] * len(tensor.shape)
                        
                        slices_left[axis] = slice(0, -2)
                        slices_center[axis] = slice(1, -1)
                        slices_right[axis] = slice(2, None)
                        
                        # 2éšå·®åˆ†
                        second_diff = (padded[tuple(slices_left)] - 
                                     2 * padded[tuple(slices_center)] + 
                                     padded[tuple(slices_right)])
                        
                        # NaN/Infãƒã‚§ãƒƒã‚¯
                        if not (np.any(np.isnan(second_diff)) or np.any(np.isinf(second_diff))):
                            laplacian += second_diff
                        
                    except Exception as e:
                        print(f"      âš ï¸ è»¸{axis}ã®ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            laplacian = laplacian / max(len(tensor.shape), 1)
            
            return laplacian.astype(np.float32)
            
        except Exception as e:
            print(f"    âš ï¸ ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros_like(tensor, dtype=np.float32)
    
    def _compute_stable_gradient(self, tensor: np.ndarray) -> np.ndarray:
        """å®‰å®šãªå‹¾é…è¨ˆç®—"""
        try:
            gradient = np.zeros_like(tensor, dtype=np.float64)
            
            for axis in range(len(tensor.shape)):
                if tensor.shape[axis] >= 2:
                    try:
                        # å‰é€²å·®åˆ†
                        diff = np.diff(tensor, axis=axis)
                        
                        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦å…ƒã®ã‚µã‚¤ã‚ºã«æˆ»ã™
                        pad_widths = [(0, 0)] * len(tensor.shape)
                        pad_widths[axis] = (0, 1)
                        padded_diff = np.pad(diff, pad_widths, mode='edge')
                        
                        # NaN/Infãƒã‚§ãƒƒã‚¯
                        if not (np.any(np.isnan(padded_diff)) or np.any(np.isinf(padded_diff))):
                            gradient += padded_diff
                        
                    except Exception as e:
                        print(f"      âš ï¸ è»¸{axis}ã®å‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            return gradient.astype(np.float32)
            
        except Exception as e:
            print(f"    âš ï¸ å‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros_like(tensor, dtype=np.float32)
    
    def _evaluate_quality(self, original: np.ndarray, enhanced: np.ndarray) -> Dict[str, float]:
        """å“è³ªè©•ä¾¡ï¼ˆæ•°å€¤å®‰å®šæ€§å¼·åŒ–ç‰ˆï¼‰"""
        try:
            # å½¢çŠ¶ã¨ã‚µã‚¤ã‚ºã®äº‹å‰ãƒã‚§ãƒƒã‚¯
            if original.size == 0 or enhanced.size == 0:
                return {'enhancement_score': 0.0, 'correlation': 0.0, 'mse': float('inf'), 'snr_db': -100.0}
            
            # ã‚µã‚¤ã‚ºã‚’åˆã‚ã›ã‚‹
            min_size = min(original.size, enhanced.size)
            if min_size < 2:
                return {'enhancement_score': 0.0, 'correlation': 0.0, 'mse': 0.0, 'snr_db': 0.0}
            
            orig_flat = original.flatten()[:min_size].astype(np.float64)
            enh_flat = enhanced.flatten()[:min_size].astype(np.float64)
            
            # NaN/Infå€¤ã®äº‹å‰é™¤å»
            valid_mask = np.isfinite(orig_flat) & np.isfinite(enh_flat)
            
            if np.sum(valid_mask) < 2:
                print(f"    âš ï¸ æœ‰åŠ¹ãªå€¤ãŒä¸è¶³: {np.sum(valid_mask)}")
                return {'enhancement_score': 0.0, 'correlation': 0.0, 'mse': float('inf'), 'snr_db': -100.0}
            
            orig_clean = orig_flat[valid_mask]
            enh_clean = enh_flat[valid_mask]
            
            # MSEè¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰
            try:
                diff = orig_clean - enh_clean
                # å·®åˆ†ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if np.max(np.abs(diff)) > 1e10:
                    print(f"    âš ï¸ å·®åˆ†ãŒå¤§ãã™ãã¾ã™: {np.max(np.abs(diff)):.2e}")
                    diff = np.clip(diff, -1e6, 1e6)  # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                
                mse = np.mean(diff ** 2)
                
                # MSEã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãƒã‚§ãƒƒã‚¯
                if not np.isfinite(mse) or mse > 1e20:
                    print(f"    âš ï¸ MSEã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼: {mse}")
                    mse = 1e6  # é©åº¦ãªå¤§ãã•ã«åˆ¶é™
                
            except (OverflowError, ValueError) as e:
                print(f"    âŒ MSEè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                mse = 1e6
            
            # ç›¸é–¢ä¿‚æ•°è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰
            try:
                # æ¨™æº–åå·®ã®äº‹å‰ãƒã‚§ãƒƒã‚¯
                orig_std = np.std(orig_clean)
                enh_std = np.std(enh_clean)
                
                if orig_std < 1e-10 or enh_std < 1e-10:
                    print(f"    âš ï¸ æ¨™æº–åå·®ãŒå°ã•ã™ãã¾ã™: orig={orig_std:.2e}, enh={enh_std:.2e}")
                    correlation = 0.0
                else:
                    correlation_matrix = np.corrcoef(orig_clean, enh_clean)
                    correlation = correlation_matrix[0, 1]
                    
                    # ç›¸é–¢ä¿‚æ•°ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                    if not np.isfinite(correlation):
                        print(f"    âš ï¸ ç›¸é–¢ä¿‚æ•°ãŒç„¡åŠ¹: {correlation}")
                        correlation = 0.0
                
            except (OverflowError, ValueError, np.linalg.LinAlgError) as e:
                print(f"    âŒ ç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                correlation = 0.0
            
            # SNRè¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šç‰ˆï¼‰
            try:
                signal_power = np.mean(orig_clean ** 2)
                
                # ä¿¡å·ãƒ‘ãƒ¯ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
                if not np.isfinite(signal_power) or signal_power < 1e-20:
                    snr_db = -100.0
                else:
                    noise_power = max(mse, 1e-20)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                    snr = signal_power / noise_power
                    
                    if np.isfinite(snr) and snr > 0:
                        snr_db = float(10 * np.log10(snr))
                        # SNRã®ç¯„å›²åˆ¶é™
                        snr_db = np.clip(snr_db, -100.0, 100.0)
                    else:
                        snr_db = -100.0
                
            except (OverflowError, ValueError) as e:
                print(f"    âŒ SNRè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                snr_db = -100.0
            
            # æ‹¡å¼µã‚¹ã‚³ã‚¢è¨ˆç®—
            enhancement_score = max(0.0, float(correlation)) if np.isfinite(correlation) else 0.0
            
            # çµæœã®æœ€çµ‚æ¤œè¨¼
            result = {
                'enhancement_score': float(enhancement_score),
                'correlation': float(correlation) if np.isfinite(correlation) else 0.0,
                'mse': float(mse) if np.isfinite(mse) else 1e6,
                'snr_db': float(snr_db) if np.isfinite(snr_db) else -100.0
            }
            
            return result
            
        except Exception as e:
            print(f"    âŒ å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'enhancement_score': 0.0,
                'correlation': 0.0,
                'mse': 1e6,
                'snr_db': -100.0
            }


class BackupManager:
    """è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†"""
    
    def __init__(self, config: IntegratedNKATConfig):
        self.config = config
        self.backup_dir = Path(config.backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–: {self.backup_dir}")
    
    def create_backup(self, file_path: str) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        try:
            source_path = Path(file_path)
            if not source_path.exists():
                return None
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{source_path.stem}_{timestamp}{source_path.suffix}"
            backup_path = self.backup_dir / backup_name
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            shutil.copy2(source_path, backup_path)
            
            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
            self._cleanup_old_backups(source_path.stem)
            
            print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path.name}")
            return str(backup_path)
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _cleanup_old_backups(self, file_stem: str):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤"""
        try:
            # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–å¾—
            backups = []
            for backup_file in self.backup_dir.glob(f"{file_stem}_*"):
                backups.append(backup_file)
            
            # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
            backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # æœ€å¤§æ•°ã‚’è¶…ãˆãŸåˆ†ã‚’å‰Šé™¤
            for old_backup in backups[self.config.max_backups:]:
                old_backup.unlink()
                print(f"ğŸ—‘ï¸ å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤: {old_backup.name}")
                
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

class PresetManager:
    """ãƒ—ãƒªã‚»ãƒƒãƒˆç®¡ç†"""
    
    def __init__(self, config: IntegratedNKATConfig):
        self.config = config
        self.preset_file = Path(config.preset_file)
        self.settings_file = Path("settings.ini")
        self.presets = self._load_presets()
        self.file_locations = self._load_file_locations()
        print(f"âš™ï¸ ãƒ—ãƒªã‚»ãƒƒãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–")
    
    def _load_presets(self) -> Dict:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
        try:
            if self.preset_file.exists():
                with open(self.preset_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒªã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {
            "default": self.config.to_dict(),
            "high_quality": {**self.config.to_dict(), "max_rank": 16, "tolerance": 1e-8},
            "fast_processing": {**self.config.to_dict(), "max_rank": 4, "tolerance": 1e-4},
            "experimental": {**self.config.to_dict(), "noncommutative_strength": 0.2, "kolmogorov_strength": 0.1}
        }
    
    def save_presets(self):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜"""
        try:
            with open(self.preset_file, 'w', encoding='utf-8') as f:
                json.dump(self.presets, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜å®Œäº†")
        except Exception as e:
            print(f"âŒ ãƒ—ãƒªã‚»ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def add_preset(self, name: str, config: IntegratedNKATConfig):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆè¿½åŠ """
        self.presets[name] = config.to_dict()
        if self.config.auto_save_presets:
            self.save_presets()
    
    def get_preset(self, name: str) -> Optional[IntegratedNKATConfig]:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆå–å¾—"""
        if name in self.presets:
            return IntegratedNKATConfig.from_dict(self.presets[name])
        return None
    
    def _load_file_locations(self) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®æƒ…å ±èª­ã¿è¾¼ã¿"""
        try:
            config = configparser.ConfigParser()
            if self.settings_file.exists():
                config.read(self.settings_file, encoding='utf-8')
                return dict(config['file_locations']) if 'file_locations' in config else {}
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    def save_file_location(self, key: str, path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®ä¿å­˜"""
        if not self.config.remember_file_locations:
            return
        
        try:
            self.file_locations[key] = path
            
            config = configparser.ConfigParser()
            config['file_locations'] = self.file_locations
            
            with open(self.settings_file, 'w', encoding='utf-8') as f:
                config.write(f)
                
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_file_location(self, key: str) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®å–å¾—"""
        return self.file_locations.get(key)


class IntegratedGGUFProcessor:
    """çµ±åˆGGUFå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    GGUF_MAGIC = b'GGUF'
    
    def __init__(self, config: IntegratedNKATConfig):
        self.config = config
        self.kolmogorov_op = IntegratedKolmogorovOperator(config)
        self.backup_manager = BackupManager(config)
        self.preset_manager = PresetManager(config)
        
        # çµ±è¨ˆ
        self.stats = {
            'processed_files': 0,
            'enhanced_tensors': 0,
            'total_enhancement_score': 0.0,
            'processing_time': 0.0
        }
        
        print(f"ğŸ§  çµ±åˆGGUFå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def process_gguf_file(self, input_path: str, output_path: str = None) -> Dict[str, Any]:
        """GGUFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†"""
        print(f"\nğŸŒ€ GGUFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹...")
        print(f"  å…¥åŠ›: {Path(input_path).name}")
        
        start_time = time.time()
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = None
            if self.config.auto_backup:
                backup_path = self.backup_manager.create_backup(input_path)
            
            # å‡ºåŠ›ãƒ‘ã‚¹æ±ºå®š
            if output_path is None:
                input_path_obj = Path(input_path)
                output_path = str(input_path_obj.parent / f"{input_path_obj.stem}_nkat_enhanced{input_path_obj.suffix}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            success = self._process_gguf_internal(input_path, output_path)
            
            # çµ±è¨ˆæ›´æ–°
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time
            
            if success:
                self.stats['processed_files'] += 1
                print(f"âœ… å‡¦ç†å®Œäº†: {Path(output_path).name}")
                print(f"  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': success,
                'output_path': output_path if success else None,
                'backup_path': backup_path,
                'processing_time': processing_time,
                'stats': self.stats.copy()
            }
            
        except Exception as e:
            print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'stats': self.stats.copy()
            }
    
    def _process_gguf_internal(self, input_path: str, output_path: str) -> bool:
        """å†…éƒ¨GGUFå‡¦ç†"""
        try:
            with open(input_path, 'rb') as f:
                # ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿è¾¼ã¿
                magic = f.read(4)
                if magic != self.GGUF_MAGIC:
                    print(f"  âŒ ç„¡åŠ¹ãªGGUFãƒã‚¸ãƒƒã‚¯")
                    return False
                
                version = struct.unpack('<I', f.read(4))[0]
                tensor_count = struct.unpack('<Q', f.read(8))[0]
                metadata_count = struct.unpack('<Q', f.read(8))[0]
                
                print(f"  ğŸ“Š GGUF v{version}: {tensor_count}å€‹ã®ãƒ†ãƒ³ã‚½ãƒ«, {metadata_count}å€‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
                
                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—
                original_size = os.path.getsize(input_path)
                print(f"  ğŸ“ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {original_size / (1024*1024):.2f} MB")
                
                # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿+ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
                remaining_data = f.read()
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
                metadata_data, tensor_data = self._separate_metadata_and_tensors(
                    remaining_data, metadata_count, tensor_count
                )
                
                # ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆåˆ¶é™ã‚’å¤§å¹…ç·©å’Œï¼‰
                enhanced_data = self._process_tensor_data_full(tensor_data, tensor_count, original_size)
                
                # æ‹¡å¼µãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ï¼ˆå…ƒã‚µã‚¤ã‚ºä¿æŒï¼‰
                self._write_enhanced_gguf_full(output_path, version, metadata_data, enhanced_data, tensor_count)
                
                return True
                
        except Exception as e:
            print(f"  âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _separate_metadata_and_tensors(self, data: bytes, metadata_count: int, tensor_count: int) -> Tuple[bytes, bytes]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢"""
        # ç°¡å˜ãªæ¨å®šï¼ˆã‚ˆã‚Šæ­£ç¢ºãªåˆ†é›¢ã‚’å®Ÿè£…ï¼‰
        estimated_metadata_size = min(len(data) // 3, metadata_count * 128)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æœ€å¤§1/3
        
        metadata_data = data[:estimated_metadata_size]
        tensor_data = data[estimated_metadata_size:]
        
        print(f"  ğŸ“Š åˆ†é›¢: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ {len(metadata_data)//1024}KB, ãƒ†ãƒ³ã‚½ãƒ« {len(tensor_data)//1024}KB")
        
        return metadata_data, tensor_data
    
    def _process_tensor_data_full(self, data: bytes, tensor_count: int, original_size: int) -> List[bytes]:
        """å®Œå…¨ãªãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆã‚µã‚¤ã‚ºä¿æŒç‰ˆï¼‰"""
        print(f"  ğŸ”§ å®Œå…¨ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­... ({len(data) // 1024}KB)")
        
        enhanced_data = []
        
        if len(data) == 0:
            return enhanced_data
        
        # ã‚ˆã‚Šå¤šãã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’å‡¦ç†ï¼ˆåˆ¶é™å¤§å¹…ç·©å’Œï¼‰
        max_tensors = min(tensor_count, max(100, tensor_count // 2))  # æœ€ä½100å€‹ã¾ãŸã¯åŠåˆ†
        chunk_size = max(len(data) // max_tensors, 128)  # æœ€å°128ãƒã‚¤ãƒˆ
        
        print(f"  ğŸ“Š å‡¦ç†äºˆå®š: {max_tensors}å€‹ã®ãƒ†ãƒ³ã‚½ãƒ« (chunk: {chunk_size}bytes)")
        
        with tqdm(total=max_tensors, desc="ãƒ•ãƒ«ãƒ†ãƒ³ã‚½ãƒ«å‡¦ç†", leave=False) as pbar:
            for i in range(max_tensors):
                try:
                    start_idx = i * chunk_size
                    end_idx = min(start_idx + chunk_size, len(data))
                    
                    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã€æ®‹ã‚Šå…¨éƒ¨ã‚’å«ã‚ã‚‹
                    if i == max_tensors - 1:
                        end_idx = len(data)
                    
                    tensor_bytes = data[start_idx:end_idx]
                    
                    if len(tensor_bytes) >= 32:  # æœ€å°ã‚µã‚¤ã‚ºç·©å’Œ
                        # ãƒã‚¤ãƒˆâ†’float32å¤‰æ›
                        float_count = len(tensor_bytes) // 4
                        if float_count >= 4:  # æœ€å°è¦ç´ æ•°ç·©å’Œ
                            tensor_array = np.frombuffer(
                                tensor_bytes[:float_count * 4], 
                                dtype=np.float32
                            )
                            
                            # ã‚ˆã‚Šä¿å®ˆçš„ãªãƒ†ãƒ³ã‚½ãƒ«æ•´å½¢
                            if len(tensor_array) >= 4:
                                # å…ƒã®ã‚µã‚¤ã‚ºã‚’å¯èƒ½ãªé™ã‚Šä¿æŒ
                                if len(tensor_array) >= 64:
                                    # 8x8 or 4x4x4
                                    side = int(np.sqrt(len(tensor_array)))
                                    if side * side <= len(tensor_array):
                                        tensor_2d = tensor_array[:side*side].reshape(side, side)
                                    else:
                                        tensor_2d = tensor_array.reshape(-1, 4)
                                else:
                                    tensor_2d = tensor_array.reshape(-1, min(4, len(tensor_array)))
                            else:
                                tensor_2d = tensor_array.reshape(-1, 1)
                            
                            # ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•æ‹¡å¼µï¼ˆã‚ˆã‚Šä¿å®ˆçš„ï¼‰
                            result = self.kolmogorov_op.enhance_tensor(tensor_2d)
                            
                            if result['success'] and result['quality_metrics']['enhancement_score'] > 0.05:
                                enhanced_tensor = result['enhanced_tensor']
                                
                                # å…ƒã®ã‚µã‚¤ã‚ºã«è¿‘ã¥ã‘ã‚‹
                                enhanced_flat = enhanced_tensor.flatten()
                                if len(enhanced_flat) < len(tensor_array):
                                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦å…ƒã‚µã‚¤ã‚ºã«
                                    padded = np.zeros(len(tensor_array), dtype=np.float32)
                                    padded[:len(enhanced_flat)] = enhanced_flat
                                    enhanced_bytes = padded.tobytes()
                                else:
                                    # åˆ‡ã‚Šè©°ã‚ã¦å…ƒã‚µã‚¤ã‚ºã«
                                    enhanced_bytes = enhanced_flat[:len(tensor_array)].astype(np.float32).tobytes()
                                
                                enhanced_data.append(enhanced_bytes)
                                self.stats['enhanced_tensors'] += 1
                                self.stats['total_enhancement_score'] += result['quality_metrics']['enhancement_score']
                                
                                pbar.set_description(f"æ‹¡å¼µ: {result['quality_metrics']['enhancement_score']:.3f}")
                            else:
                                # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿æŒ
                                enhanced_data.append(tensor_bytes)
                                pbar.set_description("ä¿æŒ")
                        else:
                            enhanced_data.append(tensor_bytes)
                            pbar.set_description("å°ã•ã™ã")
                    else:
                        enhanced_data.append(tensor_bytes)
                        pbar.set_description("ã‚¹ã‚­ãƒƒãƒ—")
                    
                    pbar.update(1)
                    
                except Exception as e:
                    print(f"    âš ï¸ ãƒ†ãƒ³ã‚½ãƒ«{i+1}ã‚¨ãƒ©ãƒ¼: {e}")
                    enhanced_data.append(tensor_bytes if 'tensor_bytes' in locals() else b'')
                    pbar.update(1)
        
        total_enhanced_size = sum(len(data) for data in enhanced_data)
        print(f"  âœ… å‡¦ç†å®Œäº†: {len(enhanced_data)}å€‹, ç·ã‚µã‚¤ã‚º {total_enhanced_size // 1024}KB")
        
        return enhanced_data

    def _write_enhanced_gguf_full(self, output_path: str, version: int, metadata_data: bytes, 
                                 enhanced_data: List[bytes], tensor_count: int):
        """å®Œå…¨ãªæ‹¡å¼µGGUFãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿"""
        print(f"  ğŸ’¾ å®Œå…¨æ‹¡å¼µãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ä¸­...")
        
        total_tensor_size = sum(len(data) for data in enhanced_data)
        print(f"  ğŸ“Š æ›¸ãè¾¼ã¿äºˆå®š: ãƒ†ãƒ³ã‚½ãƒ«{len(enhanced_data)}å€‹, {total_tensor_size // 1024}KB")
        
        with open(output_path, 'wb') as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            f.write(self.GGUF_MAGIC)
            f.write(struct.pack('<I', version))
            f.write(struct.pack('<Q', len(enhanced_data)))  # å®Ÿéš›ã®ãƒ†ãƒ³ã‚½ãƒ«æ•°
            f.write(struct.pack('<Q', 8))  # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°
            
            # æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = [
                ("nkat.integrated.enabled", True),
                ("nkat.enhanced_tensors", self.stats['enhanced_tensors']),
                ("nkat.total_tensors", len(enhanced_data)),
                ("nkat.total_score", self.stats['total_enhancement_score']),
                ("nkat.config.kolmogorov_strength", self.config.kolmogorov_strength),
                ("nkat.config.noncommutative_strength", self.config.noncommutative_strength),
                ("nkat.original_tensor_count", tensor_count),
                ("nkat.system.version", "integrated_full_v1.0")
            ]
            
            for key, value in metadata:
                self._write_metadata_item(f, key, value)
            
            # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’éƒ¨åˆ†çš„ã«ä¿æŒ
            if len(metadata_data) > 0:
                f.write(metadata_data[:min(len(metadata_data), 4096)])  # æœ€å¤§4KBä¿æŒ
                
                # å¿…è¦ã«å¿œã˜ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padding_size = (8 - (f.tell() % 8)) % 8
                if padding_size > 0:
                    f.write(b'\x00' * padding_size)
            
            # æ‹¡å¼µãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿
            for i, data in enumerate(enhanced_data):
                f.write(data)
                
                # 64bitå¢ƒç•Œæ•´åˆ—
                if self.config.use_64bit_precision:
                    padding = (8 - (len(data) % 8)) % 8
                    if padding > 0:
                        f.write(b'\x00' * padding)
        
        final_size = os.path.getsize(output_path)
        print(f"  âœ… æ›¸ãè¾¼ã¿å®Œäº†: {final_size // (1024*1024)}MB")

    def _write_metadata_item(self, f, key, value):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ æ›¸ãè¾¼ã¿"""
        key_bytes = key.encode('utf-8')
        f.write(struct.pack('<Q', len(key_bytes)))
        f.write(key_bytes)
        
        if isinstance(value, bool):
            f.write(struct.pack('<I', 6))  # bool type
            f.write(struct.pack('<?', value))
        elif isinstance(value, int):
            f.write(struct.pack('<I', 4))  # int type
            f.write(struct.pack('<q', value))
        elif isinstance(value, float):
            f.write(struct.pack('<I', 5))  # float type
            f.write(struct.pack('<d', value))
        else:
            value_bytes = str(value).encode('utf-8')
            f.write(struct.pack('<I', 3))  # string type
            f.write(struct.pack('<Q', len(value_bytes)))
            f.write(value_bytes) 
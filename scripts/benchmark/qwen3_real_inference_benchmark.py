#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B-ERP å®Ÿæ¸¬æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
llama.cppå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®æ­£ç¢ºãªæ€§èƒ½æ¸¬å®š
"""

import os
import sys
import subprocess
import json
import time
import re
from pathlib import Path
from typing import Dict, List, Optional
import logging
from tqdm import tqdm
import psutil
import threading
import queue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('qwen3_real_benchmark.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class GPUMonitor:
    """GPUä½¿ç”¨ç‡ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    
    def __init__(self):
        self.monitoring = False
        self.gpu_stats = []
        
    def start_monitoring(self):
        """GPUç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.gpu_stats = []
        
        def monitor():
            while self.monitoring:
                try:
                    # nvidia-smi query
                    result = subprocess.run([
                        'nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',
                        '--format=csv,noheader,nounits'
                    ], capture_output=True, text=True, timeout=2)
                    
                    if result.returncode == 0:
                        line = result.stdout.strip()
                        if line:
                            gpu_util, mem_used, mem_total, temp = line.split(', ')
                            self.gpu_stats.append({
                                'timestamp': time.time(),
                                'gpu_utilization': float(gpu_util),
                                'memory_used_mb': float(mem_used),
                                'memory_total_mb': float(mem_total),
                                'temperature_c': float(temp)
                            })
                except Exception as e:
                    logger.warning(f"GPU monitoring error: {e}")
                
                time.sleep(0.5)  # 500msé–“éš”
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Dict:
        """GPUç›£è¦–åœæ­¢ã¨çµ±è¨ˆå–å¾—"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1)
        
        if not self.gpu_stats:
            return {}
        
        gpu_utils = [s['gpu_utilization'] for s in self.gpu_stats]
        mem_usage = [s['memory_used_mb'] for s in self.gpu_stats]
        temps = [s['temperature_c'] for s in self.gpu_stats]
        
        return {
            'avg_gpu_utilization': sum(gpu_utils) / len(gpu_utils),
            'max_gpu_utilization': max(gpu_utils),
            'avg_memory_used_mb': sum(mem_usage) / len(mem_usage),
            'max_memory_used_mb': max(mem_usage),
            'avg_temperature_c': sum(temps) / len(temps),
            'max_temperature_c': max(temps),
            'sample_count': len(self.gpu_stats)
        }

class Qwen3RealBenchmark:
    """Qwen3å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.gpu_monitor = GPUMonitor()
        
        # llama.cppå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        self.main_exe = self.find_llama_executable()
        
        logger.info(f"ğŸ”¥ Qwen3 Real Inference Benchmark")
        logger.info(f"   ğŸ“ Model: {Path(model_path).name}")
        logger.info(f"   âš™ï¸ Executable: {self.main_exe}")
    
    def find_llama_executable(self) -> Optional[str]:
        """llama.cppå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        search_paths = [
            "llama.cpp/build/Release/main.exe",
            "llama.cpp/build_cuda_en/Release/main.exe", 
            "llama.cpp/build_nkat_direct/Release/main.exe",
            "llama.cpp/build_simple/Release/main.exe"
        ]
        
        for path in search_paths:
            if os.path.exists(path):
                logger.info(f"âœ… Found executable: {path}")
                return path
        
        logger.error("âŒ No llama.cpp executable found")
        return None
    
    def run_inference_test(self, prompt: str, max_tokens: int = 100, 
                          ctx_size: int = 2048, threads: int = 12, 
                          gpu_layers: int = 40) -> Dict:
        """æ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        if not self.main_exe:
            return {"error": "No executable found"}
        
        # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        cmd = [
            self.main_exe,
            "-m", self.model_path,
            "-p", prompt,
            "-n", str(max_tokens),
            "-c", str(ctx_size),
            "-t", str(threads),
            "-ngl", str(gpu_layers),
            "--temp", "0.7",
            "--top-p", "0.9",
            "--repeat-penalty", "1.1",
            "--color",
            "--timing"
        ]
        
        logger.info(f"ğŸš€ Starting inference test...")
        logger.info(f"   ğŸ“ Prompt length: {len(prompt)} chars")
        logger.info(f"   ğŸ¯ Max tokens: {max_tokens}")
        logger.info(f"   ğŸ–¥ï¸ Context: {ctx_size}, Threads: {threads}, GPU layers: {gpu_layers}")
        
        # GPUç›£è¦–é–‹å§‹
        self.gpu_monitor.start_monitoring()
        
        try:
            start_time = time.time()
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=120,  # 2åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                encoding='utf-8',
                errors='replace'
            )
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # GPUç›£è¦–åœæ­¢
            gpu_stats = self.gpu_monitor.stop_monitoring()
            
            # çµæœè§£æ
            if result.returncode == 0:
                return self.parse_llama_output(result.stdout, result.stderr, total_time, gpu_stats)
            else:
                logger.error(f"âŒ Inference failed: {result.stderr}")
                return {"error": result.stderr, "total_time": total_time}
                
        except subprocess.TimeoutExpired:
            self.gpu_monitor.stop_monitoring()
            logger.error("âŒ Inference timeout")
            return {"error": "timeout"}
        except Exception as e:
            self.gpu_monitor.stop_monitoring()
            logger.error(f"âŒ Inference error: {e}")
            return {"error": str(e)}
    
    def parse_llama_output(self, stdout: str, stderr: str, total_time: float, gpu_stats: Dict) -> Dict:
        """llama.cppå‡ºåŠ›è§£æ"""
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°æŠ½å‡º
        prompt_tokens = 0
        generated_tokens = 0
        
        # llamaã®çµ±è¨ˆæƒ…å ±æŠ½å‡º
        timing_pattern = r'eval time\s*=\s*([\d.]+)\s*ms.*?(\d+)\s*tokens.*?([\d.]+)\s*t/s'
        timing_match = re.search(timing_pattern, stderr)
        
        load_time_pattern = r'load time\s*=\s*([\d.]+)\s*ms'
        load_time_match = re.search(load_time_pattern, stderr)
        
        prompt_eval_pattern = r'prompt eval time\s*=\s*([\d.]+)\s*ms.*?(\d+)\s*tokens.*?([\d.]+)\s*t/s'
        prompt_eval_match = re.search(prompt_eval_pattern, stderr)
        
        results = {
            "success": True,
            "total_wall_time": total_time,
            "model_path": self.model_path,
            "gpu_stats": gpu_stats
        }
        
        # ãƒ­ãƒ¼ãƒ‰æ™‚é–“
        if load_time_match:
            results["load_time_ms"] = float(load_time_match.group(1))
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè©•ä¾¡
        if prompt_eval_match:
            results["prompt_eval_time_ms"] = float(prompt_eval_match.group(1))
            results["prompt_tokens"] = int(prompt_eval_match.group(2))
            results["prompt_tokens_per_sec"] = float(prompt_eval_match.group(3))
        
        # ç”Ÿæˆè©•ä¾¡
        if timing_match:
            results["eval_time_ms"] = float(timing_match.group(1))
            results["generated_tokens"] = int(timing_match.group(2))
            results["generation_tokens_per_sec"] = float(timing_match.group(3))
        
        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
        if stdout:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾Œã®ç”Ÿæˆéƒ¨åˆ†ã‚’å–å¾—
            lines = stdout.split('\n')
            generated_text = '\n'.join(lines[1:]) if len(lines) > 1 else ""
            results["generated_text"] = generated_text[:500]  # æœ€åˆã®500æ–‡å­—
            results["generated_text_length"] = len(generated_text)
        
        logger.info(f"ğŸ“Š Parse Results:")
        if "generation_tokens_per_sec" in results:
            logger.info(f"   ğŸš€ Generation: {results['generation_tokens_per_sec']:.1f} tokens/sec")
        if "prompt_tokens_per_sec" in results:
            logger.info(f"   ğŸ“ Prompt eval: {results['prompt_tokens_per_sec']:.1f} tokens/sec")
        if "gpu_stats" in results and results["gpu_stats"]:
            logger.info(f"   ğŸ® GPU util: {results['gpu_stats'].get('avg_gpu_utilization', 0):.1f}%")
        
        return results
    
    def benchmark_suite(self) -> Dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        
        test_cases = [
            {
                "name": "Short Response",
                "prompt": "What is the capital of Japan?",
                "max_tokens": 50,
                "ctx_size": 1024
            },
            {
                "name": "Medium Generation", 
                "prompt": "Write a short story about a robot learning to paint.",
                "max_tokens": 200,
                "ctx_size": 2048
            },
            {
                "name": "Long Context",
                "prompt": "Explain the concept of quantum computing in detail, including its advantages and challenges.",
                "max_tokens": 500,
                "ctx_size": 4096
            },
            {
                "name": "Code Generation",
                "prompt": "def fibonacci(n):\n    # Generate fibonacci sequence up to n terms\n",
                "max_tokens": 150,
                "ctx_size": 2048
            }
        ]
        
        all_results = {
            "model_info": {
                "path": self.model_path,
                "size_gb": os.path.getsize(self.model_path) / (1024**3)
            },
            "test_results": []
        }
        
        for i, test_case in enumerate(test_cases):
            logger.info(f"\nğŸ§ª Test {i+1}/{len(test_cases)}: {test_case['name']}")
            
            result = self.run_inference_test(
                prompt=test_case["prompt"],
                max_tokens=test_case["max_tokens"],
                ctx_size=test_case["ctx_size"]
            )
            
            result.update({
                "test_name": test_case["name"],
                "test_prompt": test_case["prompt"]
            })
            
            all_results["test_results"].append(result)
            
            # ä¸­é–“çµæœè¡¨ç¤º
            if "generation_tokens_per_sec" in result:
                logger.info(f"   âœ… {result['generation_tokens_per_sec']:.1f} tokens/sec")
            else:
                logger.info(f"   âŒ Failed: {result.get('error', 'unknown')}")
            
            # ãƒ†ã‚¹ãƒˆé–“ã§å°ä¼‘æ­¢
            time.sleep(2)
        
        return all_results
    
    def save_results(self, results: Dict, filename: str = "qwen3_real_benchmark_results.json"):
        """çµæœä¿å­˜"""
        try:
            output_dir = Path("output/qwen3_nkat_testing")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_path = output_dir / filename
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨ç’°å¢ƒæƒ…å ±è¿½åŠ 
            results["benchmark_info"] = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "python_version": sys.version,
                "executable_path": self.main_exe,
                "system_cpu_count": psutil.cpu_count(),
                "system_memory_gb": psutil.virtual_memory().total / (1024**3)
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Results saved: {output_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”¥ Qwen3-8B-ERP Real Inference Benchmark")
    print("=" * 60)
    
    model_path = "models/integrated/Qwen3-8B-ERP-v0.1.i1-Q6_K.gguf"
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark = Qwen3RealBenchmark(model_path)
    
    if not benchmark.main_exe:
        print("âŒ llama.cpp executable not found. Please build first.")
        return
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
    results = benchmark.benchmark_suite()
    
    # çµæœä¿å­˜
    benchmark.save_results(results)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“Š Benchmark Summary:")
    print("-" * 40)
    
    for result in results["test_results"]:
        test_name = result.get("test_name", "Unknown")
        if "generation_tokens_per_sec" in result:
            tokens_per_sec = result["generation_tokens_per_sec"]
            gpu_util = result.get("gpu_stats", {}).get("avg_gpu_utilization", 0)
            print(f"   {test_name:15s}: {tokens_per_sec:6.1f} t/s (GPU: {gpu_util:4.1f}%)")
        else:
            error = result.get("error", "unknown")
            print(f"   {test_name:15s}: FAILED ({error})")
    
    print("\nğŸ‰ Real benchmark completed!")
    print("ğŸ“ Check output/qwen3_nkat_testing/ for detailed results")

if __name__ == "__main__":
    main() 
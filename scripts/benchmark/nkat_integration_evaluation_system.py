#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATçµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
å‰å›ã®çµæœã‚’ã‚‚ã¨ã«ã—ãŸç·åˆçš„ãªè©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional
import logging
from datetime import datetime
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nkat_integration_evaluation.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class NKATIntegrationEvaluator:
    """NKATçµ±åˆè©•ä¾¡å™¨"""
    
    def __init__(self):
        self.output_dir = Path("output/qwen3_nkat_testing")
        self.evaluation_timestamp = datetime.now()
        
        logger.info("ğŸ” NKAT Integration Evaluator initialized")
    
    def load_previous_results(self) -> Dict:
        """å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿"""
        results = {
            "inference_test": None,
            "quality_benchmark": None,
            "optimization": None
        }
        
        try:
            # æ¨è«–ãƒ†ã‚¹ãƒˆçµæœ
            inference_path = self.output_dir / "qwen3_nkat_test_results.json"
            if inference_path.exists():
                with open(inference_path, 'r', encoding='utf-8') as f:
                    results["inference_test"] = json.load(f)
                logger.info("âœ… Inference test results loaded")
            
            # å“è³ªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            quality_path = self.output_dir / "nkat_quality_benchmark_results.json"
            if quality_path.exists():
                with open(quality_path, 'r', encoding='utf-8') as f:
                    results["quality_benchmark"] = json.load(f)
                logger.info("âœ… Quality benchmark results loaded")
            
            # æœ€é©åŒ–çµæœï¼ˆã‚ã‚Œã°ï¼‰
            optimization_path = self.output_dir / "nkat_optimization_results.json"
            if optimization_path.exists():
                with open(optimization_path, 'r', encoding='utf-8') as f:
                    results["optimization"] = json.load(f)
                logger.info("âœ… Optimization results loaded")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error loading previous results: {e}")
        
        return results
    
    def analyze_performance_accuracy(self, results: Dict) -> Dict:
        """æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ã®ç¢ºåº¦åˆ†æ"""
        analysis = {
            "accuracy_assessment": "unknown",
            "potential_issues": [],
            "confidence_level": 0.0,
            "recommendations": []
        }
        
        inference_results = results.get("inference_test")
        if not inference_results:
            analysis["accuracy_assessment"] = "no_data"
            return analysis
        
        # NKATãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®æ¤œè¨¼
        try:
            nkat_metrics = inference_results.get("nkat_performance", {})
            
            # 445K tokens/secã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            throughput = nkat_metrics.get("throughput_tokens_per_sec", 0)
            
            if throughput > 300000:  # 300Kä»¥ä¸Šã¯ç–‘ã‚ã—ã„
                analysis["potential_issues"].append({
                    "issue": "unusually_high_throughput",
                    "description": f"Throughput of {throughput:,.0f} tokens/sec seems too high for 8B Q6_K model",
                    "severity": "high"
                })
                analysis["confidence_level"] = 0.2
            elif throughput > 50000:
                analysis["potential_issues"].append({
                    "issue": "high_throughput",
                    "description": "Throughput is higher than typical for this model size",
                    "severity": "medium"
                })
                analysis["confidence_level"] = 0.6
            else:
                analysis["confidence_level"] = 0.8
            
            # VRAMã®ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            vram_usage = nkat_metrics.get("vram_usage_gb", 0)
            if vram_usage < 6.0:  # 8Bãƒ¢ãƒ‡ãƒ«Q6_Kã§6GBä»¥ä¸‹ã¯ä½ã™ãã‚‹
                analysis["potential_issues"].append({
                    "issue": "low_vram_usage",
                    "description": f"VRAM usage of {vram_usage:.1f}GB seems low for 8B Q6_K model",
                    "severity": "medium"
                })
            
            # æ¨å¥¨äº‹é …
            if throughput > 200000:
                analysis["recommendations"].extend([
                    "å®Ÿéš›ã®llama.cppå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æ¤œè¨¼æ¨å¥¨",
                    "é•·æ–‡ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ã§ã®å®Ÿæ¸¬å¿…è¦",
                    "GPUä½¿ç”¨ç‡ã®è©³ç´°ç¢ºèªå¿…è¦"
                ])
            
            # ç·åˆè©•ä¾¡
            if analysis["confidence_level"] >= 0.7:
                analysis["accuracy_assessment"] = "likely_accurate"
            elif analysis["confidence_level"] >= 0.4:
                analysis["accuracy_assessment"] = "needs_verification"
            else:
                analysis["accuracy_assessment"] = "likely_synthetic"
                
        except Exception as e:
            logger.error(f"Performance analysis failed: {e}")
            analysis["accuracy_assessment"] = "analysis_failed"
        
        return analysis
    
    def analyze_quality_metrics(self, results: Dict) -> Dict:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†æ"""
        analysis = {
            "overall_score": 0.0,
            "category_performance": {},
            "strengths": [],
            "weaknesses": [],
            "improvement_areas": []
        }
        
        quality_results = results.get("quality_benchmark")
        if not quality_results:
            return analysis
        
        try:
            scenario_results = quality_results.get("scenario_results", [])
            
            category_scores = []
            for scenario in scenario_results:
                agg_metrics = scenario.get("aggregate_metrics", {})
                if agg_metrics:
                    category_name = scenario["scenario"]["category"]
                    avg_score = agg_metrics.get("avg_overall_score", 0.0)
                    
                    analysis["category_performance"][category_name] = {
                        "score": avg_score,
                        "scenario_name": scenario["scenario"]["name"]
                    }
                    category_scores.append(avg_score)
            
            if category_scores:
                analysis["overall_score"] = np.mean(category_scores)
                
                # ãƒ™ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª
                best_categories = sorted(
                    analysis["category_performance"].items(),
                    key=lambda x: x[1]["score"],
                    reverse=True
                )[:2]
                
                analysis["strengths"] = [
                    f"{cat[1]['scenario_name']} ({cat[1]['score']:.3f})"
                    for cat in best_categories
                ]
                
                # æ”¹å–„ãŒå¿…è¦ãªã‚«ãƒ†ã‚´ãƒª
                weak_categories = [
                    cat for cat in analysis["category_performance"].items()
                    if cat[1]["score"] < 0.65
                ]
                
                analysis["weaknesses"] = [
                    f"{cat[1]['scenario_name']} ({cat[1]['score']:.3f})"
                    for cat in weak_categories
                ]
                
                # æ”¹å–„ææ¡ˆ
                if analysis["overall_score"] < 0.7:
                    analysis["improvement_areas"].extend([
                        "NKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ (rank/gamma)",
                        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æœ€é©åŒ–",
                        "ãƒ¢ãƒ‡ãƒ«å¾®èª¿æ•´æ¤œè¨"
                    ])
                elif analysis["overall_score"] < 0.8:
                    analysis["improvement_areas"].extend([
                        "ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®å°‚é–€åŒ–",
                        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·æœ€é©åŒ–"
                    ])
                
        except Exception as e:
            logger.error(f"Quality analysis failed: {e}")
        
        return analysis
    
    def generate_recommendations(self, performance_analysis: Dict, quality_analysis: Dict) -> List[Dict]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # 1. ç¢ºåº¦ãƒã‚§ãƒƒã‚¯æ¨å¥¨
        if performance_analysis["accuracy_assessment"] in ["needs_verification", "likely_synthetic"]:
            recommendations.append({
                "priority": "high",
                "category": "verification",
                "title": "å®Ÿæ¸¬æ¨è«–æ€§èƒ½ã®ç¢ºèª",
                "description": "å®Ÿéš›ã®llama.cppå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸè©³ç´°ãªæ€§èƒ½æ¸¬å®šãŒå¿…è¦",
                "actions": [
                    "llama.cpp/build_*/main.exeã§ã®ç›´æ¥ãƒ†ã‚¹ãƒˆ",
                    "nvidia-smi dmonã§ã®GPUä½¿ç”¨ç‡ç¢ºèª",
                    "é•·æ–‡ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ã§ã®å®Ÿæ¸¬"
                ]
            })
        
        # 2. å“è³ªå‘ä¸Šæ¨å¥¨
        overall_quality = quality_analysis.get("overall_score", 0.0)
        if overall_quality < 0.75:
            recommendations.append({
                "priority": "medium",
                "category": "quality_improvement",
                "title": "å“è³ªã‚¹ã‚³ã‚¢å‘ä¸Š",
                "description": f"ç¾åœ¨ã®å“è³ªã‚¹ã‚³ã‚¢{overall_quality:.3f}ã‚’ã•ã‚‰ã«å‘ä¸Š",
                "actions": [
                    "NKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¾®èª¿æ•´",
                    "å¼±ã„ã‚«ãƒ†ã‚´ãƒªã®é‡ç‚¹çš„æ”¹å–„",
                    "ã‚ˆã‚Šå¤šæ§˜ãªãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã§ã®è©•ä¾¡"
                ]
            })
        
        # 3. æœ€é©åŒ–æ¨å¥¨
        recommendations.append({
            "priority": "medium",
            "category": "optimization",
            "title": "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–",
            "description": "rank/gammaã®ä½“ç³»çš„æœ€é©åŒ–",
            "actions": [
                "Optunaå¤šç›®çš„æœ€é©åŒ–ã®å®Ÿè¡Œ",
                "ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆåˆ†æ",
                "å®Ÿç”¨ã‚·ãƒŠãƒªã‚ªã§ã®æ¤œè¨¼"
            ]
        })
        
        # 4. CI/CDçµ±åˆæ¨å¥¨
        recommendations.append({
            "priority": "low",
            "category": "automation",
            "title": "è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰",
            "description": "ç¶™ç¶šçš„å“è³ªç›£è¦–ã®å®Ÿè£…",
            "actions": [
                "GitHub Actionsçµ±åˆ",
                "å®šæœŸçš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ",
                "ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
            ]
        })
        
        return recommendations
    
    def create_comprehensive_visualization(self, performance_analysis: Dict, quality_analysis: Dict) -> None:
        """ç·åˆçš„ãªå¯è¦–åŒ–"""
        
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('NKAT Integration Evaluation Summary', fontsize=16, fontweight='bold')
            
            # 1. å“è³ªã‚¹ã‚³ã‚¢åˆ†å¸ƒ
            category_performance = quality_analysis.get("category_performance", {})
            if category_performance:
                categories = list(category_performance.keys())
                scores = [category_performance[cat]["score"] for cat in categories]
                
                bars = ax1.bar(range(len(categories)), scores, color='skyblue', alpha=0.7)
                ax1.set_xlabel('Test Categories')
                ax1.set_ylabel('Quality Score')
                ax1.set_title('Quality Scores by Category')
                ax1.set_xticks(range(len(categories)))
                ax1.set_xticklabels(categories, rotation=45, ha='right')
                ax1.set_ylim(0, 1.0)
                ax1.grid(True, alpha=0.3)
                
                # ã‚¹ã‚³ã‚¢å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
                for i, (bar, score) in enumerate(zip(bars, scores)):
                    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                            f'{score:.3f}', ha='center', va='bottom')
            
            # 2. ä¿¡é ¼æ€§è©•ä¾¡
            confidence_level = performance_analysis.get("confidence_level", 0.0)
            accuracy_assessment = performance_analysis.get("accuracy_assessment", "unknown")
            
            # ä¿¡é ¼æ€§ã‚²ãƒ¼ã‚¸
            angles = np.linspace(0, np.pi, 100)
            confidence_angle = np.pi * confidence_level
            
            ax2.plot(angles, np.ones_like(angles), 'lightgray', linewidth=10)
            ax2.plot(angles[angles <= confidence_angle], 
                    np.ones_like(angles[angles <= confidence_angle]), 
                    'green' if confidence_level > 0.7 else 'orange' if confidence_level > 0.4 else 'red',
                    linewidth=10)
            
            ax2.set_xlim(0, np.pi)
            ax2.set_ylim(0.5, 1.5)
            ax2.set_title(f'Performance Data Confidence\n{confidence_level:.1%} - {accuracy_assessment}')
            ax2.set_xticks([0, np.pi/2, np.pi])
            ax2.set_xticklabels(['Low', 'Medium', 'High'])
            ax2.set_yticks([])
            
            # 3. å•é¡Œã®é‡è¦åº¦åˆ†å¸ƒ
            issues = performance_analysis.get("potential_issues", [])
            if issues:
                severity_counts = {}
                for issue in issues:
                    severity = issue.get("severity", "unknown")
                    severity_counts[severity] = severity_counts.get(severity, 0) + 1
                
                severities = list(severity_counts.keys())
                counts = list(severity_counts.values())
                colors = {'high': 'red', 'medium': 'orange', 'low': 'yellow', 'unknown': 'gray'}
                bar_colors = [colors.get(sev, 'gray') for sev in severities]
                
                ax3.bar(severities, counts, color=bar_colors, alpha=0.7)
                ax3.set_xlabel('Issue Severity')
                ax3.set_ylabel('Count')
                ax3.set_title('Potential Issues by Severity')
                ax3.grid(True, alpha=0.3)
            else:
                ax3.text(0.5, 0.5, 'No issues detected', ha='center', va='center', 
                        transform=ax3.transAxes, fontsize=14)
                ax3.set_title('Issue Analysis')
            
            # 4. ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼
            overall_score = quality_analysis.get("overall_score", 0.0)
            
            # å††ã‚°ãƒ©ãƒ•ã§ç·åˆè©•ä¾¡
            labels = ['Quality Score', 'Remaining']
            sizes = [overall_score, 1.0 - overall_score]
            colors = ['green' if overall_score > 0.8 else 'orange' if overall_score > 0.6 else 'red', 'lightgray']
            
            wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                              startangle=90)
            ax4.set_title(f'Overall Quality Score\n{overall_score:.3f}')
            
            plt.tight_layout()
            
            # ä¿å­˜
            chart_path = self.output_dir / "nkat_integration_evaluation.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
            logger.info(f"ğŸ“Š Evaluation charts saved: {chart_path}")
            
            plt.show()
            
        except Exception as e:
            logger.error(f"âŒ Visualization failed: {e}")
    
    def generate_comprehensive_report(self, results: Dict, performance_analysis: Dict, 
                                    quality_analysis: Dict, recommendations: List[Dict]) -> Dict:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report = {
            "evaluation_metadata": {
                "timestamp": self.evaluation_timestamp.isoformat(),
                "evaluator_version": "1.0",
                "model_tested": "Qwen3-8B-ERP-v0.1.i1-Q6_K.gguf"
            },
            "executive_summary": self.create_executive_summary(performance_analysis, quality_analysis),
            "detailed_analysis": {
                "performance_accuracy": performance_analysis,
                "quality_assessment": quality_analysis
            },
            "recommendations": recommendations,
            "next_steps": self.create_next_steps(recommendations),
            "appendix": {
                "raw_results": results
            }
        }
        
        return report
    
    def create_executive_summary(self, performance_analysis: Dict, quality_analysis: Dict) -> Dict:
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        
        overall_quality = quality_analysis.get("overall_score", 0.0)
        confidence = performance_analysis.get("confidence_level", 0.0)
        
        # ç·åˆè©•ä¾¡
        if overall_quality >= 0.8 and confidence >= 0.7:
            overall_status = "excellent"
        elif overall_quality >= 0.7 and confidence >= 0.5:
            overall_status = "good"
        elif overall_quality >= 0.6 or confidence >= 0.4:
            overall_status = "fair"
        else:
            overall_status = "needs_improvement"
        
        return {
            "overall_status": overall_status,
            "key_findings": [
                f"å“è³ªã‚¹ã‚³ã‚¢: {overall_quality:.3f}/1.0",
                f"ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§: {confidence:.1%}",
                f"ä¸»è¦èª²é¡Œæ•°: {len(performance_analysis.get('potential_issues', []))}"
            ],
            "critical_issues": [
                issue["description"] for issue in performance_analysis.get("potential_issues", [])
                if issue.get("severity") == "high"
            ],
            "strengths": quality_analysis.get("strengths", []),
            "immediate_actions_required": len([
                rec for rec in [] if rec.get("priority") == "high"
            ]) > 0
        }
    
    def create_next_steps(self, recommendations: List[Dict]) -> List[str]:
        """æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ä½œæˆ"""
        
        next_steps = []
        
        # å„ªå…ˆåº¦é †ã§æ•´ç†
        high_priority = [rec for rec in recommendations if rec.get("priority") == "high"]
        medium_priority = [rec for rec in recommendations if rec.get("priority") == "medium"]
        
        if high_priority:
            next_steps.append("ğŸ”´ å³åº§ã«å®Ÿè¡Œã™ã¹ãé …ç›®:")
            for rec in high_priority:
                next_steps.extend([f"  â€¢ {action}" for action in rec.get("actions", [])])
        
        if medium_priority:
            next_steps.append("ğŸŸ¡ ä¸­æœŸçš„ã«å®Ÿè¡Œã™ã¹ãé …ç›®:")
            for rec in medium_priority[:2]:  # ä¸Šä½2ã¤
                next_steps.extend([f"  â€¢ {action}" for action in rec.get("actions", [])])
        
        next_steps.append("ğŸ“‹ ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°:")
        next_steps.extend([
            "  â€¢ é€±æ¬¡å“è³ªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ",
            "  â€¢ æœˆæ¬¡æ€§èƒ½è©•ä¾¡",
            "  â€¢ å››åŠæœŸæœ€é©åŒ–ãƒ¬ãƒ“ãƒ¥ãƒ¼"
        ])
        
        return next_steps
    
    def save_comprehensive_report(self, report: Dict) -> None:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        
        try:
            # JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_path = self.output_dir / "nkat_comprehensive_evaluation_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Comprehensive report saved: {report_path}")
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            self.create_markdown_report(report)
            
        except Exception as e:
            logger.error(f"âŒ Failed to save report: {e}")
    
    def create_markdown_report(self, report: Dict) -> None:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
        
        try:
            md_content = f"""# NKAT Integration Evaluation Report

**Generated:** {report['evaluation_metadata']['timestamp']}  
**Model:** {report['evaluation_metadata']['model_tested']}

## ğŸ¯ Executive Summary

**Overall Status:** {report['executive_summary']['overall_status'].upper()}

### Key Findings
{chr(10).join(f"- {finding}" for finding in report['executive_summary']['key_findings'])}

### Strengths
{chr(10).join(f"- {strength}" for strength in report['executive_summary']['strengths'])}

### Critical Issues
{chr(10).join(f"- âš ï¸ {issue}" for issue in report['executive_summary']['critical_issues'])}

## ğŸ“Š Detailed Analysis

### Performance Data Accuracy
- **Confidence Level:** {report['detailed_analysis']['performance_accuracy']['confidence_level']:.1%}
- **Assessment:** {report['detailed_analysis']['performance_accuracy']['accuracy_assessment']}

### Quality Assessment
- **Overall Score:** {report['detailed_analysis']['quality_assessment']['overall_score']:.3f}/1.0

#### Category Performance
"""

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½
            category_perf = report['detailed_analysis']['quality_assessment'].get('category_performance', {})
            for category, data in category_perf.items():
                md_content += f"- **{data['scenario_name']}:** {data['score']:.3f}\n"

            md_content += f"""

## ğŸš€ Recommendations

"""
            
            # æ¨å¥¨äº‹é …
            for i, rec in enumerate(report['recommendations'], 1):
                md_content += f"""### {i}. {rec['title']} (Priority: {rec['priority'].upper()})

{rec['description']}

**Actions:**
{chr(10).join(f"- {action}" for action in rec['actions'])}

"""

            md_content += f"""## ğŸ“‹ Next Steps

{chr(10).join(report['next_steps'])}

---
*Report generated by NKAT Integration Evaluator v{report['evaluation_metadata']['evaluator_version']}*
"""
            
            md_path = self.output_dir / "nkat_evaluation_report.md"
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            logger.info(f"ğŸ“„ Markdown report saved: {md_path}")
            
        except Exception as e:
            logger.error(f"âŒ Markdown report creation failed: {e}")
    
    def run_comprehensive_evaluation(self) -> None:
        """ç·åˆè©•ä¾¡å®Ÿè¡Œ"""
        
        logger.info("ğŸ” Starting comprehensive NKAT integration evaluation")
        
        # 1. å‰å›çµæœã®èª­ã¿è¾¼ã¿
        results = self.load_previous_results()
        
        # 2. æ€§èƒ½ç¢ºåº¦åˆ†æ
        performance_analysis = self.analyze_performance_accuracy(results)
        
        # 3. å“è³ªåˆ†æ
        quality_analysis = self.analyze_quality_metrics(results)
        
        # 4. æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self.generate_recommendations(performance_analysis, quality_analysis)
        
        # 5. å¯è¦–åŒ–
        self.create_comprehensive_visualization(performance_analysis, quality_analysis)
        
        # 6. ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ä¿å­˜
        comprehensive_report = self.generate_comprehensive_report(
            results, performance_analysis, quality_analysis, recommendations
        )
        self.save_comprehensive_report(comprehensive_report)
        
        # 7. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.print_evaluation_summary(comprehensive_report)
    
    def print_evaluation_summary(self, report: Dict) -> None:
        """è©•ä¾¡ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        
        print("\n" + "="*60)
        print("ğŸ”¥ NKAT Integration Evaluation Summary")
        print("="*60)
        
        exec_summary = report['executive_summary']
        
        print(f"ğŸ“Š Overall Status: {exec_summary['overall_status'].upper()}")
        print(f"ğŸ“ˆ Quality Score: {report['detailed_analysis']['quality_assessment']['overall_score']:.3f}/1.0")
        print(f"ğŸ” Data Confidence: {report['detailed_analysis']['performance_accuracy']['confidence_level']:.1%}")
        
        print(f"\nâœ… Strengths:")
        for strength in exec_summary['strengths'][:3]:
            print(f"   â€¢ {strength}")
        
        if exec_summary['critical_issues']:
            print(f"\nâš ï¸ Critical Issues:")
            for issue in exec_summary['critical_issues']:
                print(f"   â€¢ {issue}")
        
        print(f"\nğŸ¯ High Priority Recommendations:")
        high_priority_recs = [rec for rec in report['recommendations'] if rec['priority'] == 'high']
        for rec in high_priority_recs[:2]:
            print(f"   â€¢ {rec['title']}")
        
        print(f"\nğŸ“ Reports saved in: {self.output_dir}")
        print("   â€¢ nkat_comprehensive_evaluation_report.json")
        print("   â€¢ nkat_evaluation_report.md")
        print("   â€¢ nkat_integration_evaluation.png")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” NKAT Integration Comprehensive Evaluation")
    print("=" * 60)
    
    evaluator = NKATIntegrationEvaluator()
    evaluator.run_comprehensive_evaluation()
    
    print("\nğŸ‰ Comprehensive evaluation completed!")

if __name__ == "__main__":
    main() 